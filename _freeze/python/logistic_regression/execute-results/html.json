{
  "hash": "488cd1342703e5ecf8e4b5dfa931e938",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Logistic Regression\"\noutput: html_document\n---\n\n\n# Imports\n\n::: {#41ac130f .cell execution_count=1}\n``` {.python .cell-code}\n#data manipulation\nimport pandas as pd\nimport numpy as np\n\n#modelling\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\n```\n:::\n\n\n# Background\n\nIn binary logistic regression, there is a single binary dependent variable, coded by an indicator variable. For example, if we respresent a response as 1 and non-response as 0, then the corresponding probability of response, can be between 0 (certainly not a response) and 1 (certainly a response) - hence the labeling !\n\nThe logistic model models the log-odds of an event as a linear combination of one or more independent variables (explanatory variables). If we observed $(y_i, x_i),$ where $y_i$ is a Bernoulli variable and $x_i$ a vector of explanatory variables, the model for $\\pi_i = P(y_i=1)$ is\n\n$$\n\\text{logit}(\\pi_i)= \\log\\left\\{ \\frac{\\pi_i}{1-\\pi_i}\\right\\} = \\beta_0 + \\beta x_i, i = 1,\\ldots,n \n$$\n\nThe model is especially useful in case-control studies and leads to the effect of risk factors by odds ratios.\n\n# Example : Lung cancer data\n\n*Data source: Loprinzi CL. Laurie JA. Wieand HS. Krook JE. Novotny PJ. Kugler JW. Bartel J. Law M. Bateman M. Klatt NE. et al. Prospective evaluation of prognostic variables from patient-completed questionnaires. North Central Cancer Treatment Group. Journal of Clinical Oncology. 12(3):601-7, 1994.*\n\nThese data were sourced from the R package {survival} and have been downloaded and stored in the `data` folder.\n\n::: {#fb61788b .cell execution_count=2}\n``` {.python .cell-code}\n# importing and prepare\nlung2 = pd.read_csv(\"../data/lung_cancer.csv\")\n\n#create weight loss factor while respecting missing values\n# 1: patients with a weight loss of more than zero\n# 0: patients a weight loss of zero or less\nlung2[\"wt_grp\"] = np.where(lung2[\"wt.loss\"].isnull(), np.nan, (lung2[\"wt.loss\"] > 0).astype(int))\n```\n:::\n\n\n# Logistic Regression Modelling\n\nLet's further prepare our data for modelling by selecting the explanatory variables and the dependent variable. The Python packages that we are are aware of require complete (i.e. no missing values) data so for convenience of demonstrating these methods we will drop rows with missing values.\n\n::: {#2dcd95bb .cell execution_count=3}\n``` {.python .cell-code}\nx_vars = [\"age\", \"sex\", \"ph.ecog\", \"meal.cal\"]\ny_var = \"wt_grp\"\n\n# drop rows with missing values \nlung2_complete = lung2.dropna(axis=0)\n\n#select variables\nx = lung2_complete[x_vars]\ny = lung2_complete[y_var]\n```\n:::\n\n\n## Statsmodels package\n\nWe will use the `sm.Logit()` method to fit our logistic regression model.\n\n::: {#317aeae9 .cell execution_count=4}\n``` {.python .cell-code}\n#intercept column\nx_sm = sm.add_constant(x)\n\n#fit model\nlr_sm = sm.Logit(y, x_sm).fit() \nprint(lr_sm.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.568825\n         Iterations 5\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 wt_grp   No. Observations:                  167\nModel:                          Logit   Df Residuals:                      162\nMethod:                           MLE   Df Model:                            4\nDate:                Thu, 13 Mar 2025   Pseudo R-squ.:                 0.05169\nTime:                        16:21:34   Log-Likelihood:                -94.994\nconverged:                       True   LL-Null:                       -100.17\nCovariance Type:            nonrobust   LLR p-value:                   0.03484\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          3.3576      1.654      2.029      0.042       0.115       6.600\nage           -0.0126      0.021     -0.598      0.550      -0.054       0.029\nsex           -0.8645      0.371     -2.328      0.020      -1.592      -0.137\nph.ecog        0.4182      0.263      1.592      0.111      -0.097       0.933\nmeal.cal      -0.0009      0.000     -1.932      0.053      -0.002    1.27e-05\n==============================================================================\n```\n:::\n:::\n\n\n### Model fitting\n\nIn addition to the information contained in the summary, we can display the model coefficients as odds ratios:\n\n::: {#ab8dc416 .cell execution_count=5}\n``` {.python .cell-code}\nprint(\"Odds ratios for statsmodels logistic regression:\")\nprint(np.exp(lr_sm.params))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOdds ratios for statsmodels logistic regression:\nconst       28.719651\nage          0.987467\nsex          0.421266\nph.ecog      1.519198\nmeal.cal     0.999140\ndtype: float64\n```\n:::\n:::\n\n\nWe can also provide the 5% confidence intervals for the odds ratios:\n\n::: {#10c00933 .cell execution_count=6}\n``` {.python .cell-code}\nprint(\"CI at 5% for statsmodels logistic regression:\")\nprint(np.exp(lr_sm.conf_int(alpha = 0.05)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCI at 5% for statsmodels logistic regression:\n                 0           1\nconst     1.121742  735.301118\nage       0.947449    1.029175\nsex       0.203432    0.872354\nph.ecog   0.907984    2.541852\nmeal.cal  0.998269    1.000013\n```\n:::\n:::\n\n\n### Prediction\n\nLet's use our trained model to make a weight loss prediction about a new patient.\n\n::: {#a8d52762 .cell execution_count=7}\n``` {.python .cell-code}\n# new female, symptomatic but completely ambulatory patient consuming 2500 calories\nnew_pt = pd.DataFrame({\n    \"age\": [56],\n    \"sex\": [2],\n    \"ph.ecog\": [1.00], \n    \"meal.cal\": [2500]\n})\n\n# Add intercept term to the new data; for a single row this should be \n# forced using the `add_constant` command\nnew_pt_sm = sm.add_constant(new_pt, has_constant=\"add\")\nprint(\"Probability of weight loss using the statsmodels package:\")\nprint(lr_sm.predict(new_pt_sm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProbability of weight loss using the statsmodels package:\n0    0.308057\ndtype: float64\n```\n:::\n:::\n\n\n## Scikit-learn Package\n\nThe `scikit-learn` package is a popular package for machine learning and predictive modelling.\n\n::: callout-warning\nIt's important to note that l2 regularisation is applied by default in the `scikit-learn` implementation of logistic regression. More recent releases of this package include an option to have no regularisation penalty.\n:::\n\n::: {#d0d17230 .cell execution_count=8}\n``` {.python .cell-code}\nlr_sk = LogisticRegression(penalty=None).fit(x, y)\n```\n:::\n\n\nUnlike the `statsmodels` approach `scikit-learn` doesn't have a summary method for the model but you can extract some of the model parameters as follows:\n\n::: {#6fa062ac .cell execution_count=9}\n``` {.python .cell-code}\nprint(\"Intercept for scikit learn logistic regression:\")\nprint(lr_sk.intercept_)\nprint(\"Odds ratios for scikit learn logistic regression:\")\nprint(np.exp(lr_sk.coef_))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept for scikit learn logistic regression:\n[3.35756405]\nOdds ratios for scikit learn logistic regression:\n[[0.98746739 0.42126736 1.51919379 0.99914048]]\n```\n:::\n:::\n\n\nHowever, obtaining the confidence intervals and other metrics is not directly supported in `scikit-learn`.\n\n### Prediction\n\nUsing the same new patient example we can use our logistic regression model to make a prediction. The `predict_proba` method is used to return the probability for each class. If you are interested in viewing the prediction for `y = 1`, i.e. the probability of weight loss then you can select the second probability as shown:\n\n::: {#0cd100f7 .cell execution_count=10}\n``` {.python .cell-code}\nprint(\"Probability of weight loss using the scikit-learn package:\")\nprint(lr_sk.predict_proba(new_pt)[:,1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProbability of weight loss using the scikit-learn package:\n[0.30805813]\n```\n:::\n:::\n\n\n## Conclusions\n\nThere are two main ways to fit a logistic regression using python. Each of these packages have their advantages with `statsmodel` geared more towards model and coefficient interpretation in low dimensional data settings and in contrast the `scikit-learn` implementation more appropriate for use cases focused on prediction with more complex, higher dimensional data.\n\n",
    "supporting": [
      "logistic_regression_files"
    ],
    "filters": [],
    "includes": {}
  }
}