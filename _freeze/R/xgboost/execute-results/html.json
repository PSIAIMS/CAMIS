{
  "hash": "88bb0a4346d5631950a8e9dac4c5a91e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"XGBoost\"\n---\n\n\n\n# XGBoost\n\nXGBoost which stands for eXtreme Gradient Boosting is an efficent implementation of gradient boosting. Gradient boosting is an ensemble technique in machine learning. Unlike traditional models that learn from the data independently, boosting combines the predictions of multiple weak learners to create a single, more accurate strong learner.\n\nAn XGBoost model is based on trees, so we don’t need to do much preprocessing for our data; we don’t need to worry about the factors or centering or scaling our data.\n\n## Available R packages\n\nThere are multiple packages that can be used to to implement xgboost in R.\n\n-   [{tidymodels}](https://www.tidymodels.org/)\n-   [{xgboost}](https://cran.r-project.org/web/packages/xgboost/index.html)\n-   [{caret}](https://cran.r-project.org/web/packages/caret/index.html)\n\n{tidymodels} and {caret} easy ways to access xgboost easily. This example will use {tidymodels} because of the functionality included in {tidymodels} and is being heavily supported by Posit. {caret} was the precursor to {tidymodels} and it is recommended that you use {tidymodels} over {caret} as no new features are being added. \n\n## Data used\n\nData used for this example is `birthwt` which is part of the {MASS} package. This data-set considers a number of risk factors associated with birth weight in infants.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(MASS)\nlibrary(tidymodels)\nlibrary(xgboost)\n\nhead(birthwt)\n```\n:::\n\n\n\nOur modeling goal using the `birthwt` dataset is to predict whether the birth weight is low or not low based on factors such as mother's age, smoking status, and history of hypertension.\n\n## Example Code\n\nUse {tidymodels} metadata package to split the data into training and testing data. For classification, we need to change the Low variable into a factor, since currently coded as an integer (0,1).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbirthwt <- \n  birthwt %>% \n  mutate(\n    low_f = lvls_revalue(factor(low), c(\"Not Low\", \"Low\")),\n    smoke_f = lvls_revalue(factor(smoke), c(\"Non-smoker\", \"Smoker\"))\n  )\n\n\nbrthwt_split <- initial_split(birthwt, strata = low)\nbrthwt_train <- training(brthwt_split)\nbrthwt_test <- testing(brthwt_split)\n```\n:::\n\n\n\n### Classification\n\nAfter creating the data split, we setup the params of the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgboost_spec <- \n  boost_tree(trees = 15) %>% \n  # This model can be used for classification or regression, so set mode\n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\")\n\nxgboost_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = 15\n\nComputational engine: xgboost \n```\n\n\n:::\n\n```{.r .cell-code}\nxgboost_cls_fit <- xgboost_spec %>% fit(low_f ~ ., data = brthwt_train)\nxgboost_cls_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n##### xgb.Booster\nraw: 15.2 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 1, min_child_weight = 1, \n    subsample = 1), data = x$data, nrounds = 15, watchlist = x$watchlist, \n    verbose = 0, nthread = 1, objective = \"binary:logistic\")\nparams (as set within xgb.train):\n  eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"1\", subsample = \"1\", nthread = \"1\", objective = \"binary:logistic\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 12 \nniter: 15\nnfeatures : 12 \nevaluation_log:\n  iter training_logloss\n <num>            <num>\n     1       0.44894346\n     2       0.31033704\n   ---              ---\n    14       0.01571681\n    15       0.01396153\n```\n\n\n:::\n\n```{.r .cell-code}\nbind_cols(\n  predict(xgboost_cls_fit, brthwt_test),\n  predict(xgboost_cls_fit, brthwt_test, type = \"prob\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 48 × 3\n   .pred_class `.pred_Not Low` .pred_Low\n   <fct>                 <dbl>     <dbl>\n 1 Not Low               0.985    0.0151\n 2 Not Low               0.985    0.0151\n 3 Not Low               0.985    0.0151\n 4 Not Low               0.985    0.0151\n 5 Not Low               0.985    0.0151\n 6 Not Low               0.985    0.0151\n 7 Not Low               0.985    0.0151\n 8 Not Low               0.985    0.0151\n 9 Not Low               0.988    0.0116\n10 Not Low               0.988    0.0116\n# ℹ 38 more rows\n```\n\n\n:::\n:::\n\n\n\n### Regression\n\nTo perform xgboost with regression, when setting up the parameter of the model, set the mode of xgboost to regression. After that switch and then changing the variable of interest back to an integer, the rest of the code is the same.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgboost_reg_spec <- \n  boost_tree(trees = 15) %>% \n  # This model can be used for classification or regression, so set mode\n  set_mode(\"regression\") %>% \n  set_engine(\"xgboost\")\n\nxgboost_reg_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 15\n\nComputational engine: xgboost \n```\n\n\n:::\n\n```{.r .cell-code}\n# For a regression model, the outcome should be `numeric`, not a `factor`.\nxgboost_reg_fit <- xgboost_reg_spec %>% fit(low~ ., data = brthwt_train)\nxgboost_reg_fit \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n##### xgb.Booster\nraw: 15.2 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 1, min_child_weight = 1, \n    subsample = 1), data = x$data, nrounds = 15, watchlist = x$watchlist, \n    verbose = 0, nthread = 1, objective = \"reg:squarederror\")\nparams (as set within xgb.train):\n  eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"1\", subsample = \"1\", nthread = \"1\", objective = \"reg:squarederror\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 13 \nniter: 15\nnfeatures : 13 \nevaluation_log:\n  iter training_rmse\n <num>         <num>\n     1   0.352094163\n     2   0.247943366\n   ---           ---\n    14   0.003690328\n    15   0.002599106\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(xgboost_reg_fit , brthwt_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 48 × 1\n     .pred\n     <dbl>\n 1 0.00253\n 2 0.00253\n 3 0.00253\n 4 0.00253\n 5 0.00253\n 6 0.00253\n 7 0.00253\n 8 0.00253\n 9 0.00253\n10 0.00253\n# ℹ 38 more rows\n```\n\n\n:::\n:::\n\n\n\n## Reference\n\n-   [XGBoost with tidymodels by Julia Silge](https://juliasilge.com/blog/xgboost-tune-volleyball/)\n\n::: {.callout-note collapse=\"true\" title=\"Session Info\"}\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31 ucrt)\n os       Windows 11 x64 (build 26100)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2025-08-18\n pandoc   3.4 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version    date (UTC) lib source\n P backports      1.5.0      2024-05-23 [?] RSPM (R 4.4.0)\n P broom        * 1.0.7      2024-09-26 [?] RSPM (R 4.4.0)\n   class          7.3-22     2023-05-03 [2] CRAN (R 4.4.2)\n P cli            3.6.3      2024-06-21 [?] RSPM (R 4.4.0)\n   codetools      0.2-20     2024-03-31 [2] CRAN (R 4.4.2)\n P colorspace     2.1-1      2024-07-26 [?] RSPM (R 4.4.0)\n P data.table     1.16.0     2024-08-27 [?] RSPM (R 4.4.0)\n P dials        * 1.3.0      2024-07-30 [?] RSPM (R 4.4.0)\n P DiceDesign     1.10       2023-12-07 [?] RSPM (R 4.4.0)\n P digest         0.6.37     2024-08-19 [?] RSPM (R 4.4.0)\n P dplyr        * 1.1.4      2023-11-17 [?] RSPM (R 4.4.0)\n P evaluate       1.0.0      2024-09-17 [?] RSPM (R 4.4.0)\n P fansi          1.0.6      2023-12-08 [?] RSPM (R 4.4.0)\n P fastmap        1.2.0      2024-05-15 [?] RSPM (R 4.4.0)\n P forcats      * 1.0.0      2023-01-29 [?] RSPM (R 4.4.0)\n P foreach        1.5.2      2022-02-02 [?] RSPM (R 4.4.0)\n P furrr          0.3.1      2022-08-15 [?] RSPM (R 4.4.0)\n P future         1.34.0     2024-07-29 [?] RSPM (R 4.4.0)\n P future.apply   1.11.2     2024-03-28 [?] RSPM (R 4.4.0)\n P generics       0.1.3      2022-07-05 [?] RSPM (R 4.4.0)\n P ggplot2      * 3.5.1      2024-04-23 [?] RSPM (R 4.4.0)\n P globals        0.16.3     2024-03-08 [?] RSPM (R 4.4.0)\n   glue           1.8.0      2024-09-30 [2] CRAN (R 4.4.3)\n P gower          1.0.1      2022-12-22 [?] RSPM (R 4.4.0)\n P GPfit          1.0-8      2019-02-08 [?] RSPM (R 4.4.0)\n P gtable         0.3.5      2024-04-22 [?] RSPM (R 4.4.0)\n P hardhat        1.4.0      2024-06-02 [?] RSPM (R 4.4.0)\n P hms            1.1.3      2023-03-21 [?] RSPM (R 4.4.0)\n P htmltools      0.5.8.1    2024-04-04 [?] RSPM (R 4.4.0)\n P htmlwidgets    1.6.4      2023-12-06 [?] RSPM (R 4.4.0)\n P infer        * 1.0.7      2024-03-25 [?] RSPM (R 4.4.0)\n P ipred          0.9-15     2024-07-18 [?] RSPM (R 4.4.0)\n P iterators      1.0.14     2022-02-05 [?] RSPM (R 4.4.0)\n P jsonlite       1.8.9      2024-09-20 [?] RSPM (R 4.4.0)\n P knitr          1.50       2025-03-16 [?] RSPM (R 4.4.0)\n   lattice        0.22-6     2024-03-20 [2] CRAN (R 4.4.2)\n P lava           1.8.0      2024-03-05 [?] RSPM (R 4.4.0)\n P lhs            1.2.0      2024-06-30 [?] RSPM (R 4.4.0)\n   lifecycle      1.0.4      2023-11-07 [2] CRAN (R 4.4.3)\n P listenv        0.9.1      2024-01-29 [?] RSPM (R 4.4.0)\n P lubridate    * 1.9.3      2023-09-27 [?] RSPM (R 4.4.0)\n   magrittr       2.0.3      2022-03-30 [2] CRAN (R 4.4.3)\n   MASS         * 7.3-61     2024-06-13 [2] CRAN (R 4.4.2)\n   Matrix         1.7-1      2024-10-18 [2] CRAN (R 4.4.2)\n P modeldata    * 1.4.0      2024-06-19 [?] RSPM (R 4.4.0)\n P munsell        0.5.1      2024-04-01 [?] RSPM (R 4.4.0)\n   nnet           7.3-19     2023-05-03 [2] CRAN (R 4.4.2)\n P parallelly     1.38.0     2024-07-27 [?] RSPM (R 4.4.0)\n P parsnip      * 1.2.1      2024-03-22 [?] RSPM (R 4.4.0)\n P pillar         1.9.0      2023-03-22 [?] RSPM (R 4.4.0)\n P pkgconfig      2.0.3      2019-09-22 [?] RSPM (R 4.4.0)\n P prodlim        2024.06.25 2024-06-24 [?] RSPM (R 4.4.0)\n P purrr        * 1.0.2      2023-08-10 [?] RSPM (R 4.4.0)\n P R6             2.5.1      2021-08-19 [?] RSPM (R 4.4.0)\n P Rcpp           1.0.13     2024-07-17 [?] RSPM (R 4.4.0)\n P readr        * 2.1.5      2024-01-10 [?] RSPM (R 4.4.0)\n P recipes      * 1.1.0      2024-07-04 [?] RSPM (R 4.4.0)\n   renv           1.0.10     2024-10-05 [1] RSPM (R 4.4.2)\n P rlang          1.1.4      2024-06-04 [?] RSPM (R 4.4.0)\n P rmarkdown      2.28       2024-08-17 [?] RSPM (R 4.4.0)\n   rpart          4.1.23     2023-12-05 [2] CRAN (R 4.4.2)\n P rsample      * 1.2.1      2024-03-25 [?] RSPM (R 4.4.0)\n P rstudioapi     0.16.0     2024-03-24 [?] RSPM (R 4.4.0)\n P scales       * 1.3.0      2023-11-28 [?] RSPM (R 4.4.0)\n P sessioninfo    1.2.2      2021-12-06 [?] RSPM (R 4.4.0)\n P stringi        1.8.4      2024-05-06 [?] RSPM (R 4.4.0)\n P stringr      * 1.5.1      2023-11-14 [?] RSPM (R 4.4.0)\n   survival       3.7-0      2024-06-05 [2] CRAN (R 4.4.2)\n P tibble       * 3.2.1      2023-03-20 [?] RSPM (R 4.4.0)\n P tidymodels   * 1.2.0      2024-03-25 [?] RSPM (R 4.4.0)\n P tidyr        * 1.3.1      2024-01-24 [?] RSPM (R 4.4.0)\n P tidyselect     1.2.1      2024-03-11 [?] RSPM (R 4.4.0)\n P tidyverse    * 2.0.0      2023-02-22 [?] RSPM (R 4.4.0)\n P timechange     0.3.0      2024-01-18 [?] RSPM (R 4.4.0)\n P timeDate       4041.110   2024-09-22 [?] RSPM (R 4.4.0)\n P tune         * 1.2.1      2024-04-18 [?] RSPM (R 4.4.0)\n P tzdb           0.4.0      2023-05-12 [?] RSPM (R 4.4.0)\n P utf8           1.2.4      2023-10-22 [?] RSPM (R 4.4.0)\n   vctrs          0.6.5      2023-12-01 [2] CRAN (R 4.4.3)\n P withr          3.0.1      2024-07-31 [?] RSPM (R 4.4.0)\n P workflows    * 1.1.4      2024-02-19 [?] RSPM (R 4.4.0)\n P workflowsets * 1.1.0      2024-03-21 [?] RSPM (R 4.4.0)\n P xfun           0.52       2025-04-02 [?] RSPM (R 4.4.0)\n P xgboost      * 1.7.8.1    2024-07-24 [?] RSPM (R 4.4.0)\n   yaml           2.3.10     2024-07-26 [2] CRAN (R 4.4.3)\n P yardstick    * 1.3.1      2024-03-21 [?] RSPM (R 4.4.0)\n\n [1] C:/Users/ljohn71/Desktop/CAMIS/renv/library/windows/R-4.4/x86_64-w64-mingw32\n [2] C:/Users/ljohn71/AppData/Local/Programs/R/R-4.4.2/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n\n:::\n",
    "supporting": [
      "xgboost_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}