[
  {
    "objectID": "R/cmh.html",
    "href": "R/cmh.html",
    "title": "CMH Test",
    "section": "",
    "text": "The CMH procedure tests for conditional independence in partial contingency tables for a 2 x 2 x K design. However, it can be generalized to tables of X x Y x K dimensions.\n\n\nWe did not find any R package that delivers all the same measures as SAS at once. Therefore, we tried out multiple packages:\n\n\n\n\n\n\n\n\n\nPackage\nGeneral Association\nRow Means Differ\nNonzero Correlation\nM-H Odds Ratio\nHomogeneity Test\nNote\n\n\n\n\nstats::mantelhaen.test()\n✅\n❌\n❌\n✅\n❌\nWorks well for 2x2xK\n\n\nvcdExtra::CMHtest()\n✅\n✅\n✅\n❌\n❌\nProblems with sparsity, potential bug\n\n\nepiDisplay::mhor()\n❌\n❌\n❌\n✅\n✅\nOR are limited to 2x2xK design\n\n\n\n\n\n\n\n\n\n\n\nWe will use the CDISC Pilot data set, which is publicly available on the PHUSE Test Data Factory repository. We applied very basic filtering conditions upfront (see below) and this data set served as the basis of the examples to follow.\n\n\n# A tibble: 231 × 36\n   STUDYID SITEID SITEGR1 USUBJID TRTSDT     TRTEDT     TRTP  TRTPN   AGE AGEGR1\n   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;date&gt;     &lt;date&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n 1 CDISCP… 701    701     01-701… 2014-01-02 2014-07-02 Plac…     0    63 &lt;65   \n 2 CDISCP… 701    701     01-701… 2012-08-05 2012-09-01 Plac…     0    64 &lt;65   \n 3 CDISCP… 701    701     01-701… 2013-07-19 2014-01-14 Xano…    81    71 65-80 \n 4 CDISCP… 701    701     01-701… 2014-03-18 2014-03-31 Xano…    54    74 65-80 \n 5 CDISCP… 701    701     01-701… 2014-07-01 2014-12-30 Xano…    81    77 65-80 \n 6 CDISCP… 701    701     01-701… 2013-02-12 2013-03-09 Plac…     0    85 &gt;80   \n 7 CDISCP… 701    701     01-701… 2014-01-01 2014-07-09 Xano…    54    68 65-80 \n 8 CDISCP… 701    701     01-701… 2012-09-07 2012-09-16 Xano…    54    81 &gt;80   \n 9 CDISCP… 701    701     01-701… 2012-11-30 2013-01-23 Xano…    54    84 &gt;80   \n10 CDISCP… 701    701     01-701… 2014-03-12 2014-09-09 Plac…     0    52 &lt;65   \n# ℹ 221 more rows\n# ℹ 26 more variables: AGEGR1N &lt;dbl&gt;, RACE &lt;chr&gt;, RACEN &lt;dbl&gt;, SEX &lt;chr&gt;,\n#   ITTFL &lt;chr&gt;, EFFFL &lt;chr&gt;, COMP24FL &lt;chr&gt;, AVISIT &lt;chr&gt;, AVISITN &lt;dbl&gt;,\n#   VISIT &lt;chr&gt;, VISITNUM &lt;dbl&gt;, ADY &lt;dbl&gt;, ADT &lt;date&gt;, PARAMCD &lt;chr&gt;,\n#   PARAM &lt;chr&gt;, PARAMN &lt;dbl&gt;, AVAL &lt;dbl&gt;, ANL01FL &lt;chr&gt;, DTYPE &lt;chr&gt;,\n#   AWRANGE &lt;chr&gt;, AWTARGET &lt;dbl&gt;, AWTDIFF &lt;dbl&gt;, AWLO &lt;dbl&gt;, AWHI &lt;dbl&gt;,\n#   AWU &lt;chr&gt;, QSSEQ &lt;dbl&gt;\n\n\n\n\n\n\n\nThis is included in a base installation of R, as part of the stats package. Requires inputting data as a table or as vectors.\n\nmantelhaen.test(x = data$TRTP, y = data$SEX, z = data$AGEGR1)\n\n\n    Cochran-Mantel-Haenszel test\n\ndata:  data$TRTP and data$SEX and data$AGEGR1\nCochran-Mantel-Haenszel M^2 = 2.482, df = 2, p-value = 0.2891\n\n\n\n\n\nThe vcdExtra package provides results for the generalized CMH test, for each of the three model it outputs the Chi-square value and the respective p-values. Flexible data input methods available: table or formula (aggregated level data in a data frame).\n\nlibrary(vcdExtra)\n\nLoading required package: vcd\n\n\nLoading required package: grid\n\n\nLoading required package: gnm\n\n\n\nAttaching package: 'vcdExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    summarise\n\n# Formula: Freq ~ X + Y | K\nCMHtest(Freq ~ TRTP + SEX | AGEGR1 , data=data, overall=TRUE) \n\n$`AGEGR1:&lt;65`\nCochran-Mantel-Haenszel Statistics for TRTP by SEX \n    in stratum AGEGR1:&lt;65 \n\n                 AltHypothesis   Chisq Df    Prob\ncor        Nonzero correlation 0.33168  1 0.56467\nrmeans  Row mean scores differ 1.52821  2 0.46575\ncmeans  Col mean scores differ 0.33168  1 0.56467\ngeneral    General association 1.52821  2 0.46575\n\n\n$`AGEGR1:&gt;80`\nCochran-Mantel-Haenszel Statistics for TRTP by SEX \n    in stratum AGEGR1:&gt;80 \n\n                 AltHypothesis   Chisq Df    Prob\ncor        Nonzero correlation 0.39433  1 0.53003\nrmeans  Row mean scores differ 3.80104  2 0.14949\ncmeans  Col mean scores differ 0.39433  1 0.53003\ngeneral    General association 3.80104  2 0.14949\n\n\n$`AGEGR1:65-80`\nCochran-Mantel-Haenszel Statistics for TRTP by SEX \n    in stratum AGEGR1:65-80 \n\n                 AltHypothesis   Chisq Df    Prob\ncor        Nonzero correlation 0.52744  1 0.46768\nrmeans  Row mean scores differ 0.62921  2 0.73008\ncmeans  Col mean scores differ 0.52744  1 0.46768\ngeneral    General association 0.62921  2 0.73008\n\n\n$ALL\nCochran-Mantel-Haenszel Statistics for TRTP by SEX \n    Overall tests, controlling for all strata \n\n                 AltHypothesis      Chisq Df    Prob\ncor        Nonzero correlation 0.00086897  1 0.97648\nrmeans  Row mean scores differ      2.482  2 0.28909\ncmeans  Col mean scores differ 0.00086897  1 0.97648\ngeneral    General association      2.482  2 0.28909\n\n\n\n\n\nTo get the M-H common odds ratio and the homogeneity test, the epiDisplay package can be used.\n\nlibrary(epiDisplay) \nmhor(x,y,k, graph = FALSE)\n\n\n\nTo tackle the issue with sparse data it is recommended that a use of solve() is replaced with MASS::ginv. This was implemented in the forked version of vcdExtra which can be installed from here:\n\ndevtools::install_github(\"mstackhouse/vcdExtra\")\n\nHowever, also the forked version for the vcdExtra package works only until a certain level of sparsity. In case of our data, it still works if the data are stratified by the pooled Site ID (SITEGR1 - 11 unique values) whereas using the unpooled Site ID (SITEID - 17 unique values) also throws an error. Note: this version is not up to date and sometimes calculates degrees of freedom incorrectly."
  },
  {
    "objectID": "R/cmh.html#available-r-packages",
    "href": "R/cmh.html#available-r-packages",
    "title": "CMH Test",
    "section": "",
    "text": "We did not find any R package that delivers all the same measures as SAS at once. Therefore, we tried out multiple packages:\n\n\n\n\n\n\n\n\n\nPackage\nGeneral Association\nRow Means Differ\nNonzero Correlation\nM-H Odds Ratio\nHomogeneity Test\nNote\n\n\n\n\nstats::mantelhaen.test()\n✅\n❌\n❌\n✅\n❌\nWorks well for 2x2xK\n\n\nvcdExtra::CMHtest()\n✅\n✅\n✅\n❌\n❌\nProblems with sparsity, potential bug\n\n\nepiDisplay::mhor()\n❌\n❌\n❌\n✅\n✅\nOR are limited to 2x2xK design"
  },
  {
    "objectID": "R/cmh.html#data-used",
    "href": "R/cmh.html#data-used",
    "title": "CMH Test",
    "section": "",
    "text": "We will use the CDISC Pilot data set, which is publicly available on the PHUSE Test Data Factory repository. We applied very basic filtering conditions upfront (see below) and this data set served as the basis of the examples to follow.\n\n\n# A tibble: 231 × 36\n   STUDYID SITEID SITEGR1 USUBJID TRTSDT     TRTEDT     TRTP  TRTPN   AGE AGEGR1\n   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;date&gt;     &lt;date&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n 1 CDISCP… 701    701     01-701… 2014-01-02 2014-07-02 Plac…     0    63 &lt;65   \n 2 CDISCP… 701    701     01-701… 2012-08-05 2012-09-01 Plac…     0    64 &lt;65   \n 3 CDISCP… 701    701     01-701… 2013-07-19 2014-01-14 Xano…    81    71 65-80 \n 4 CDISCP… 701    701     01-701… 2014-03-18 2014-03-31 Xano…    54    74 65-80 \n 5 CDISCP… 701    701     01-701… 2014-07-01 2014-12-30 Xano…    81    77 65-80 \n 6 CDISCP… 701    701     01-701… 2013-02-12 2013-03-09 Plac…     0    85 &gt;80   \n 7 CDISCP… 701    701     01-701… 2014-01-01 2014-07-09 Xano…    54    68 65-80 \n 8 CDISCP… 701    701     01-701… 2012-09-07 2012-09-16 Xano…    54    81 &gt;80   \n 9 CDISCP… 701    701     01-701… 2012-11-30 2013-01-23 Xano…    54    84 &gt;80   \n10 CDISCP… 701    701     01-701… 2014-03-12 2014-09-09 Plac…     0    52 &lt;65   \n# ℹ 221 more rows\n# ℹ 26 more variables: AGEGR1N &lt;dbl&gt;, RACE &lt;chr&gt;, RACEN &lt;dbl&gt;, SEX &lt;chr&gt;,\n#   ITTFL &lt;chr&gt;, EFFFL &lt;chr&gt;, COMP24FL &lt;chr&gt;, AVISIT &lt;chr&gt;, AVISITN &lt;dbl&gt;,\n#   VISIT &lt;chr&gt;, VISITNUM &lt;dbl&gt;, ADY &lt;dbl&gt;, ADT &lt;date&gt;, PARAMCD &lt;chr&gt;,\n#   PARAM &lt;chr&gt;, PARAMN &lt;dbl&gt;, AVAL &lt;dbl&gt;, ANL01FL &lt;chr&gt;, DTYPE &lt;chr&gt;,\n#   AWRANGE &lt;chr&gt;, AWTARGET &lt;dbl&gt;, AWTDIFF &lt;dbl&gt;, AWLO &lt;dbl&gt;, AWHI &lt;dbl&gt;,\n#   AWU &lt;chr&gt;, QSSEQ &lt;dbl&gt;"
  },
  {
    "objectID": "R/cmh.html#example-code",
    "href": "R/cmh.html#example-code",
    "title": "CMH Test",
    "section": "",
    "text": "This is included in a base installation of R, as part of the stats package. Requires inputting data as a table or as vectors.\n\nmantelhaen.test(x = data$TRTP, y = data$SEX, z = data$AGEGR1)\n\n\n    Cochran-Mantel-Haenszel test\n\ndata:  data$TRTP and data$SEX and data$AGEGR1\nCochran-Mantel-Haenszel M^2 = 2.482, df = 2, p-value = 0.2891\n\n\n\n\n\nThe vcdExtra package provides results for the generalized CMH test, for each of the three model it outputs the Chi-square value and the respective p-values. Flexible data input methods available: table or formula (aggregated level data in a data frame).\n\nlibrary(vcdExtra)\n\nLoading required package: vcd\n\n\nLoading required package: grid\n\n\nLoading required package: gnm\n\n\n\nAttaching package: 'vcdExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    summarise\n\n# Formula: Freq ~ X + Y | K\nCMHtest(Freq ~ TRTP + SEX | AGEGR1 , data=data, overall=TRUE) \n\n$`AGEGR1:&lt;65`\nCochran-Mantel-Haenszel Statistics for TRTP by SEX \n    in stratum AGEGR1:&lt;65 \n\n                 AltHypothesis   Chisq Df    Prob\ncor        Nonzero correlation 0.33168  1 0.56467\nrmeans  Row mean scores differ 1.52821  2 0.46575\ncmeans  Col mean scores differ 0.33168  1 0.56467\ngeneral    General association 1.52821  2 0.46575\n\n\n$`AGEGR1:&gt;80`\nCochran-Mantel-Haenszel Statistics for TRTP by SEX \n    in stratum AGEGR1:&gt;80 \n\n                 AltHypothesis   Chisq Df    Prob\ncor        Nonzero correlation 0.39433  1 0.53003\nrmeans  Row mean scores differ 3.80104  2 0.14949\ncmeans  Col mean scores differ 0.39433  1 0.53003\ngeneral    General association 3.80104  2 0.14949\n\n\n$`AGEGR1:65-80`\nCochran-Mantel-Haenszel Statistics for TRTP by SEX \n    in stratum AGEGR1:65-80 \n\n                 AltHypothesis   Chisq Df    Prob\ncor        Nonzero correlation 0.52744  1 0.46768\nrmeans  Row mean scores differ 0.62921  2 0.73008\ncmeans  Col mean scores differ 0.52744  1 0.46768\ngeneral    General association 0.62921  2 0.73008\n\n\n$ALL\nCochran-Mantel-Haenszel Statistics for TRTP by SEX \n    Overall tests, controlling for all strata \n\n                 AltHypothesis      Chisq Df    Prob\ncor        Nonzero correlation 0.00086897  1 0.97648\nrmeans  Row mean scores differ      2.482  2 0.28909\ncmeans  Col mean scores differ 0.00086897  1 0.97648\ngeneral    General association      2.482  2 0.28909\n\n\n\n\n\nTo get the M-H common odds ratio and the homogeneity test, the epiDisplay package can be used.\n\nlibrary(epiDisplay) \nmhor(x,y,k, graph = FALSE)\n\n\n\nTo tackle the issue with sparse data it is recommended that a use of solve() is replaced with MASS::ginv. This was implemented in the forked version of vcdExtra which can be installed from here:\n\ndevtools::install_github(\"mstackhouse/vcdExtra\")\n\nHowever, also the forked version for the vcdExtra package works only until a certain level of sparsity. In case of our data, it still works if the data are stratified by the pooled Site ID (SITEGR1 - 11 unique values) whereas using the unpooled Site ID (SITEID - 17 unique values) also throws an error. Note: this version is not up to date and sometimes calculates degrees of freedom incorrectly."
  },
  {
    "objectID": "R/count_data_regression.html",
    "href": "R/count_data_regression.html",
    "title": "Regression for Count Data",
    "section": "",
    "text": "The most commonly used models for count data in clinical trials include:\n\nPoisson regression: assumes the response variable \\(Y\\) has a Poisson distribution, which is linked using the logarithm with explanatory variables \\(\\bf{x}\\).\n\n\\[\n\\text{log}(E(Y|x))= \\beta_0 + \\beta' x, \\; i = 1,\\ldots,n\n\\]\n\nQuasi-Poisson regression: Poisson model that allows overdispersion, i.e. dispersion parameter is not fixed at one.\nNegative-Binomial regression: popular generalization which loosens the assumption that the variance is equal to the mean made by the Poisson model.\n\nOther models include hurdle or zero-inflated models, if data have more zero observations than expected.\n\nExample: Familial Andenomatous Polyposis Data\nData source: F. M. Giardiello, S. R. Hamilton, A. J. Krush, S. Piantadosi, L. M. Hylind, P. Celano, S. V. Booker, C. R. Robinson and G. J. A. Offerhaus (1993), Treatment of colonic and rectal adenomas with sulindac in familial adenomatous polyposis. New England Journal of Medicine, 328(18), 1313–1316.\nData from a placebo-controlled trial of a non-steroidal anti-inflammatory drug in the treatment of familial andenomatous polyposis (FAP). (see ?polyps for details).\n\npolyps &lt;- HSAUR2::polyps\nglimpse(polyps)\n\nRows: 20\nColumns: 3\n$ number &lt;dbl&gt; 63, 2, 28, 17, 61, 1, 7, 15, 44, 25, 3, 28, 10, 40, 33, 46, 50,…\n$ treat  &lt;fct&gt; placebo, drug, placebo, drug, placebo, drug, placebo, placebo, …\n$ age    &lt;dbl&gt; 20, 16, 18, 22, 13, 23, 34, 50, 19, 17, 23, 22, 30, 27, 23, 22,…\n\n\nWe analyze the number of colonic polyps at 12 months in dependency of treatment and age of the patient.\n\npolyps %&gt;% \n  ggplot(aes(y = number, x = age, color = treat)) + \n  geom_point() + theme_minimal()\n\n\n\n\n\n\n\n\n\n\nModel Fit\nWe fit a generalized linear model for number using the Poisson distribution with default log link.\n\n# Poisson\nm1 &lt;- glm(number ~ treat + age, data = polyps, family = poisson)\nsummary(m1)\n\n\nCall:\nglm(formula = number ~ treat + age, family = poisson, data = polyps)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  4.529024   0.146872   30.84  &lt; 2e-16 ***\ntreatdrug   -1.359083   0.117643  -11.55  &lt; 2e-16 ***\nage         -0.038830   0.005955   -6.52 7.02e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 378.66  on 19  degrees of freedom\nResidual deviance: 179.54  on 17  degrees of freedom\nAIC: 273.88\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe parameter estimates are on log-scale. For better interpretation, we can exponentiate these estimates, to obtain estimates and provide \\(95\\)% confidence intervals:\n\n# OR and CI\nexp(coef(m1))\n\n(Intercept)   treatdrug         age \n 92.6681047   0.2568961   0.9619140 \n\nexp(confint(m1))\n\nWaiting for profiling to be done...\n\n\n                 2.5 %      97.5 %\n(Intercept) 69.5361752 123.6802476\ntreatdrug    0.2028078   0.3218208\nage          0.9505226   0.9729788\n\n\nPredictions for number of colonic polyps given a new 25-year-old patient on either treatment using predict():\n\n# new 25 year old patient\nnew_pt &lt;- data.frame(treat = c(\"drug\",\"placebo\"), age=25)\npredict(m1, new_pt, type = \"response\")\n\n        1         2 \n 9.017654 35.102332 \n\n\n\n\nModelling Overdispersion\nPoisson model assumes that mean and variance are equal, which can be a very restrictive assumption. One option to relax the assumption is adding a overdispersion constant to the relationship, i.e. \\(\\text{Var}(\\text{response}) = \\phi\\cdot \\mu\\), which results in a quasipoisson model:\n\n# Quasi poisson\nm2 &lt;- glm(number ~ treat + age, data = polyps, family = quasipoisson)\nsummary(m2)\n\n\nCall:\nglm(formula = number ~ treat + age, family = quasipoisson, data = polyps)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.52902    0.48106   9.415 3.72e-08 ***\ntreatdrug   -1.35908    0.38533  -3.527  0.00259 ** \nage         -0.03883    0.01951  -1.991  0.06284 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 10.72805)\n\n    Null deviance: 378.66  on 19  degrees of freedom\nResidual deviance: 179.54  on 17  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nAlternatively, we can explicitly model the count data with overdispersion using the negative Binomial model. In this case, the overdispersion is a function of both \\(\\mu\\) and \\(\\mu^2\\):\n\\[\n\\text{Var}(\\text{response}) = \\mu + \\kappa\\,\\mu^2.\n\\]\n\n# Negative Binomial\nm3 &lt;- MASS::glm.nb(number ~ treat + age, data = polyps)\nsummary(m3)\n\n\nCall:\nMASS::glm.nb(formula = number ~ treat + age, data = polyps, init.theta = 1.719491, \n    link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  4.52603    0.59466   7.611 2.72e-14 ***\ntreatdrug   -1.36812    0.36903  -3.707 0.000209 ***\nage         -0.03856    0.02095  -1.840 0.065751 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.7195) family taken to be 1)\n\n    Null deviance: 36.734  on 19  degrees of freedom\nResidual deviance: 22.002  on 17  degrees of freedom\nAIC: 164.88\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.719 \n          Std. Err.:  0.607 \n\n 2 x log-likelihood:  -156.880 \n\n\nBoth model result very similar parameter estimates, but vary in estimates for their respective standard deviation."
  },
  {
    "objectID": "R/mmrm.html",
    "href": "R/mmrm.html",
    "title": "MMRM in R",
    "section": "",
    "text": "Mixed models for repeated measures (MMRM) are a popular choice for analyzing longitudinal continuous outcomes in randomized clinical trials and beyond; see Cnaan, Laird and Slasor (1997) for a tutorial and Mallinckrodt, Lane and Schnell (2008) for a review.\nThis vignette shows examples from the mmrm package.\nThe mmrm package implements MMRM based on the marginal linear model without random effects using Template Model Builder (TMB) which enables fast and robust model fitting. Users can specify a variety of covariance matrices, weight observations, fit models with restricted or standard maximum likelihood inference, perform hypothesis testing with Satterthwaite or Kenward-Roger adjustment, and extract least square means estimates by using emmeans.\n\n\n\nFlexible covariance specification:\n\nStructures: unstructured, Toeplitz, AR1, compound symmetry, ante-dependence, and spatial exponential.\nGroups: shared covariance structure for all subjects or group-specific covariance estimates.\nVariances: homogeneous or heterogeneous across time points.\n\nInference:\n\nSupports REML and ML.\nSupports weights.\n\nHypothesis testing:\n\nLeast square means: can be obtained with the emmeans package\nOne- and multi-dimensional linear contrasts of model parameters can be tested.\nSatterthwaite adjusted degrees of freedom.\nKenward-Roger adjusted degrees of freedom and coefficients covariance matrix.\nCoefficient Covariance\n\nC++ backend:\n\nFast implementation using C++ and automatic differentiation to obtain precise gradient information for model fitting.\nModel fitting algorithm details used in mmrm.\n\nPackage ecosystems integration:\n\nIntegration with tidymodels package ecosystem\n\nDedicated parsnip engine for linear regression\nIntegration with recipes\n\nIntegration with tern package ecosystems\n\nThe tern.mmrm package can be used to run the mmrm fit and generate tabulation and plots of least square means per visit and treatment arm, tabulation of model diagnostics, diagnostic graphs, and other standard model outputs.\n\n\n\n\n\n\nSee also the introductory vignette\nThe code below implements an MMRM fit in R with the mmrm::mmrm function.\n\nlibrary(mmrm)\nfit &lt;- mmrm(\n  formula = FEV1 ~ RACE + SEX + ARMCD * AVISIT + us(AVISIT | USUBJID),\n  data = fev_data\n)\n\nThe code specifies an MMRM with the given covariates and an unstructured covariance matrix for the timepoints (also called visits in the clinical trial context, here given by AVISIT) within the subjects (here USUBJID). While by default this uses restricted maximum likelihood (REML), it is also possible to use ML, see ?mmrm.\nPrinting the object will show you output which should be familiar to anyone who has used any popular modeling functions such as stats::lm(), stats::glm(), glmmTMB::glmmTMB(), and lme4::nlmer(). From this print out we see the function call, the data used, the covariance structure with number of variance parameters, as well as the likelihood method, and model deviance achieved. Additionally the user is provided a printout of the estimated coefficients and the model convergence information:\n\nfit\n\nmmrm fit\n\nFormula:     FEV1 ~ RACE + SEX + ARMCD * AVISIT + us(AVISIT | USUBJID)\nData:        fev_data (used 537 observations from 197 subjects with maximum 4 \ntimepoints)\nCovariance:  unstructured (10 variance parameters)\nInference:   REML\nDeviance:    3386.45\n\nCoefficients: \n                  (Intercept) RACEBlack or African American \n                   30.7774065                     1.5305950 \n                    RACEWhite                     SEXFemale \n                    5.6435679                     0.3260274 \n                     ARMCDTRT                    AVISITVIS2 \n                    3.7744139                     4.8396039 \n                   AVISITVIS3                    AVISITVIS4 \n                   10.3421671                    15.0537863 \n          ARMCDTRT:AVISITVIS2           ARMCDTRT:AVISITVIS3 \n                   -0.0420899                    -0.6938068 \n          ARMCDTRT:AVISITVIS4 \n                    0.6241229 \n\nModel Inference Optimization:\nConverged with code 0 and message: \n\n\nThe summary() method then provides the coefficients table with Satterthwaite degrees of freedom as well as the covariance matrix estimate:\n\nfit |&gt;\n  summary()\n\nmmrm fit\n\nFormula:     FEV1 ~ RACE + SEX + ARMCD * AVISIT + us(AVISIT | USUBJID)\nData:        fev_data (used 537 observations from 197 subjects with maximum 4 \ntimepoints)\nCovariance:  unstructured (10 variance parameters)\nMethod:      Satterthwaite\nVcov Method: Asymptotic\nInference:   REML\n\nModel selection criteria:\n     AIC      BIC   logLik deviance \n  3406.4   3439.3  -1693.2   3386.4 \n\nCoefficients: \n                               Estimate Std. Error        df t value Pr(&gt;|t|)\n(Intercept)                    30.77741    0.88657 218.79000  34.715  &lt; 2e-16\nRACEBlack or African American   1.53059    0.62446 168.67000   2.451 0.015263\nRACEWhite                       5.64357    0.66559 157.14000   8.479 1.56e-14\nSEXFemale                       0.32603    0.53194 166.14000   0.613 0.540776\nARMCDTRT                        3.77441    1.07416 145.55000   3.514 0.000589\nAVISITVIS2                      4.83960    0.80173 143.87000   6.036 1.27e-08\nAVISITVIS3                     10.34217    0.82269 155.56000  12.571  &lt; 2e-16\nAVISITVIS4                     15.05379    1.31288 138.46000  11.466  &lt; 2e-16\nARMCDTRT:AVISITVIS2            -0.04209    1.12933 138.56000  -0.037 0.970324\nARMCDTRT:AVISITVIS3            -0.69381    1.18764 158.17000  -0.584 0.559924\nARMCDTRT:AVISITVIS4             0.62412    1.85096 129.71000   0.337 0.736520\n                                 \n(Intercept)                   ***\nRACEBlack or African American *  \nRACEWhite                     ***\nSEXFemale                        \nARMCDTRT                      ***\nAVISITVIS2                    ***\nAVISITVIS3                    ***\nAVISITVIS4                    ***\nARMCDTRT:AVISITVIS2              \nARMCDTRT:AVISITVIS3              \nARMCDTRT:AVISITVIS4              \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCovariance estimate:\n        VIS1    VIS2    VIS3    VIS4\nVIS1 40.5544 14.3960  4.9760 13.3779\nVIS2 14.3960 26.5714  2.7836  7.4773\nVIS3  4.9760  2.7836 14.8980  0.9036\nVIS4 13.3779  7.4773  0.9036 95.5565\n\n\n\n\n\nIn order to extract relevant marginal means (LSmeans) and contrasts we can use the emmeans package. This package includes methods that allow mmrm objects to be used with the emmeans package. emmeans computes estimated marginal means (also called least-square means) for the coefficients of the MMRM.\n\nif (require(emmeans)) {\n  emmeans(fit, ~ ARMCD | AVISIT)\n}\n\nLoading required package: emmeans\n\n\nmmrm() registered as emmeans extension\n\n\nAVISIT = VIS1:\n ARMCD emmean    SE  df lower.CL upper.CL\n PBO     33.3 0.755 148     31.8     34.8\n TRT     37.1 0.763 143     35.6     38.6\n\nAVISIT = VIS2:\n ARMCD emmean    SE  df lower.CL upper.CL\n PBO     38.2 0.612 147     37.0     39.4\n TRT     41.9 0.602 143     40.7     43.1\n\nAVISIT = VIS3:\n ARMCD emmean    SE  df lower.CL upper.CL\n PBO     43.7 0.462 130     42.8     44.6\n TRT     46.8 0.509 130     45.7     47.8\n\nAVISIT = VIS4:\n ARMCD emmean    SE  df lower.CL upper.CL\n PBO     48.4 1.189 134     46.0     50.7\n TRT     52.8 1.188 133     50.4     55.1\n\nResults are averaged over the levels of: RACE, SEX \nConfidence level used: 0.95 \n\n\nNote that the degrees of freedom choice is inherited here from the initial mmrm fit."
  },
  {
    "objectID": "R/mmrm.html#fitting-the-mmrm-in-r",
    "href": "R/mmrm.html#fitting-the-mmrm-in-r",
    "title": "MMRM in R",
    "section": "",
    "text": "Mixed models for repeated measures (MMRM) are a popular choice for analyzing longitudinal continuous outcomes in randomized clinical trials and beyond; see Cnaan, Laird and Slasor (1997) for a tutorial and Mallinckrodt, Lane and Schnell (2008) for a review.\nThis vignette shows examples from the mmrm package.\nThe mmrm package implements MMRM based on the marginal linear model without random effects using Template Model Builder (TMB) which enables fast and robust model fitting. Users can specify a variety of covariance matrices, weight observations, fit models with restricted or standard maximum likelihood inference, perform hypothesis testing with Satterthwaite or Kenward-Roger adjustment, and extract least square means estimates by using emmeans.\n\n\n\nFlexible covariance specification:\n\nStructures: unstructured, Toeplitz, AR1, compound symmetry, ante-dependence, and spatial exponential.\nGroups: shared covariance structure for all subjects or group-specific covariance estimates.\nVariances: homogeneous or heterogeneous across time points.\n\nInference:\n\nSupports REML and ML.\nSupports weights.\n\nHypothesis testing:\n\nLeast square means: can be obtained with the emmeans package\nOne- and multi-dimensional linear contrasts of model parameters can be tested.\nSatterthwaite adjusted degrees of freedom.\nKenward-Roger adjusted degrees of freedom and coefficients covariance matrix.\nCoefficient Covariance\n\nC++ backend:\n\nFast implementation using C++ and automatic differentiation to obtain precise gradient information for model fitting.\nModel fitting algorithm details used in mmrm.\n\nPackage ecosystems integration:\n\nIntegration with tidymodels package ecosystem\n\nDedicated parsnip engine for linear regression\nIntegration with recipes\n\nIntegration with tern package ecosystems\n\nThe tern.mmrm package can be used to run the mmrm fit and generate tabulation and plots of least square means per visit and treatment arm, tabulation of model diagnostics, diagnostic graphs, and other standard model outputs.\n\n\n\n\n\n\nSee also the introductory vignette\nThe code below implements an MMRM fit in R with the mmrm::mmrm function.\n\nlibrary(mmrm)\nfit &lt;- mmrm(\n  formula = FEV1 ~ RACE + SEX + ARMCD * AVISIT + us(AVISIT | USUBJID),\n  data = fev_data\n)\n\nThe code specifies an MMRM with the given covariates and an unstructured covariance matrix for the timepoints (also called visits in the clinical trial context, here given by AVISIT) within the subjects (here USUBJID). While by default this uses restricted maximum likelihood (REML), it is also possible to use ML, see ?mmrm.\nPrinting the object will show you output which should be familiar to anyone who has used any popular modeling functions such as stats::lm(), stats::glm(), glmmTMB::glmmTMB(), and lme4::nlmer(). From this print out we see the function call, the data used, the covariance structure with number of variance parameters, as well as the likelihood method, and model deviance achieved. Additionally the user is provided a printout of the estimated coefficients and the model convergence information:\n\nfit\n\nmmrm fit\n\nFormula:     FEV1 ~ RACE + SEX + ARMCD * AVISIT + us(AVISIT | USUBJID)\nData:        fev_data (used 537 observations from 197 subjects with maximum 4 \ntimepoints)\nCovariance:  unstructured (10 variance parameters)\nInference:   REML\nDeviance:    3386.45\n\nCoefficients: \n                  (Intercept) RACEBlack or African American \n                   30.7774065                     1.5305950 \n                    RACEWhite                     SEXFemale \n                    5.6435679                     0.3260274 \n                     ARMCDTRT                    AVISITVIS2 \n                    3.7744139                     4.8396039 \n                   AVISITVIS3                    AVISITVIS4 \n                   10.3421671                    15.0537863 \n          ARMCDTRT:AVISITVIS2           ARMCDTRT:AVISITVIS3 \n                   -0.0420899                    -0.6938068 \n          ARMCDTRT:AVISITVIS4 \n                    0.6241229 \n\nModel Inference Optimization:\nConverged with code 0 and message: \n\n\nThe summary() method then provides the coefficients table with Satterthwaite degrees of freedom as well as the covariance matrix estimate:\n\nfit |&gt;\n  summary()\n\nmmrm fit\n\nFormula:     FEV1 ~ RACE + SEX + ARMCD * AVISIT + us(AVISIT | USUBJID)\nData:        fev_data (used 537 observations from 197 subjects with maximum 4 \ntimepoints)\nCovariance:  unstructured (10 variance parameters)\nMethod:      Satterthwaite\nVcov Method: Asymptotic\nInference:   REML\n\nModel selection criteria:\n     AIC      BIC   logLik deviance \n  3406.4   3439.3  -1693.2   3386.4 \n\nCoefficients: \n                               Estimate Std. Error        df t value Pr(&gt;|t|)\n(Intercept)                    30.77741    0.88657 218.79000  34.715  &lt; 2e-16\nRACEBlack or African American   1.53059    0.62446 168.67000   2.451 0.015263\nRACEWhite                       5.64357    0.66559 157.14000   8.479 1.56e-14\nSEXFemale                       0.32603    0.53194 166.14000   0.613 0.540776\nARMCDTRT                        3.77441    1.07416 145.55000   3.514 0.000589\nAVISITVIS2                      4.83960    0.80173 143.87000   6.036 1.27e-08\nAVISITVIS3                     10.34217    0.82269 155.56000  12.571  &lt; 2e-16\nAVISITVIS4                     15.05379    1.31288 138.46000  11.466  &lt; 2e-16\nARMCDTRT:AVISITVIS2            -0.04209    1.12933 138.56000  -0.037 0.970324\nARMCDTRT:AVISITVIS3            -0.69381    1.18764 158.17000  -0.584 0.559924\nARMCDTRT:AVISITVIS4             0.62412    1.85096 129.71000   0.337 0.736520\n                                 \n(Intercept)                   ***\nRACEBlack or African American *  \nRACEWhite                     ***\nSEXFemale                        \nARMCDTRT                      ***\nAVISITVIS2                    ***\nAVISITVIS3                    ***\nAVISITVIS4                    ***\nARMCDTRT:AVISITVIS2              \nARMCDTRT:AVISITVIS3              \nARMCDTRT:AVISITVIS4              \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCovariance estimate:\n        VIS1    VIS2    VIS3    VIS4\nVIS1 40.5544 14.3960  4.9760 13.3779\nVIS2 14.3960 26.5714  2.7836  7.4773\nVIS3  4.9760  2.7836 14.8980  0.9036\nVIS4 13.3779  7.4773  0.9036 95.5565\n\n\n\n\n\nIn order to extract relevant marginal means (LSmeans) and contrasts we can use the emmeans package. This package includes methods that allow mmrm objects to be used with the emmeans package. emmeans computes estimated marginal means (also called least-square means) for the coefficients of the MMRM.\n\nif (require(emmeans)) {\n  emmeans(fit, ~ ARMCD | AVISIT)\n}\n\nLoading required package: emmeans\n\n\nmmrm() registered as emmeans extension\n\n\nAVISIT = VIS1:\n ARMCD emmean    SE  df lower.CL upper.CL\n PBO     33.3 0.755 148     31.8     34.8\n TRT     37.1 0.763 143     35.6     38.6\n\nAVISIT = VIS2:\n ARMCD emmean    SE  df lower.CL upper.CL\n PBO     38.2 0.612 147     37.0     39.4\n TRT     41.9 0.602 143     40.7     43.1\n\nAVISIT = VIS3:\n ARMCD emmean    SE  df lower.CL upper.CL\n PBO     43.7 0.462 130     42.8     44.6\n TRT     46.8 0.509 130     45.7     47.8\n\nAVISIT = VIS4:\n ARMCD emmean    SE  df lower.CL upper.CL\n PBO     48.4 1.189 134     46.0     50.7\n TRT     52.8 1.188 133     50.4     55.1\n\nResults are averaged over the levels of: RACE, SEX \nConfidence level used: 0.95 \n\n\nNote that the degrees of freedom choice is inherited here from the initial mmrm fit."
  },
  {
    "objectID": "R/linear-regression.html",
    "href": "R/linear-regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "To demonstrate the use of linear regression we examine a dataset that illustrates the relationship between Height and Weight in a group of 237 teen-aged boys and girls. The dataset is available here and is imported to the workspace.\n\nDescriptive Statistics\nThe first step is to obtain the simple descriptive statistics for the numeric variables of htwt data, and one-way frequencies for categorical variables. This is accomplished by employing summary function. There are 237 participants who are from 13.9 to 25 years old. It is a cross-sectional study, with each participant having one observation. We can use this data set to examine the relationship of participants’ height to their age and sex.\n\nknitr::opts_chunk$set(echo = TRUE)\nhtwt&lt;-read.csv(\"../data/htwt.csv\")\nsummary(htwt)\n\n      ROW          SEX                 AGE            HEIGHT     \n Min.   :  1   Length:237         Min.   :13.90   Min.   :50.50  \n 1st Qu.: 60   Class :character   1st Qu.:14.80   1st Qu.:58.80  \n Median :119   Mode  :character   Median :16.30   Median :61.50  \n Mean   :119                      Mean   :16.44   Mean   :61.36  \n 3rd Qu.:178                      3rd Qu.:17.80   3rd Qu.:64.30  \n Max.   :237                      Max.   :25.00   Max.   :72.00  \n     WEIGHT     \n Min.   : 50.5  \n 1st Qu.: 85.0  \n Median :101.0  \n Mean   :101.3  \n 3rd Qu.:112.0  \n Max.   :171.5  \n\n\nIn order to create a regression model to demonstrate the relationship between age and height for females, we first need to create a flag variable identifying females and an interaction variable between age and female gender flag.\n\nhtwt$female &lt;- ifelse(htwt$SEX=='f',1,0)\nhtwt$fem_age &lt;- htwt$AGE * htwt$female\nhead(htwt)\n\n  ROW SEX  AGE HEIGHT WEIGHT female fem_age\n1   1   f 14.3   56.3   85.0      1    14.3\n2   2   f 15.5   62.3  105.0      1    15.5\n3   3   f 15.3   63.3  108.0      1    15.3\n4   4   f 16.1   59.0   92.0      1    16.1\n5   5   f 19.1   62.5  112.5      1    19.1\n6   6   f 17.1   62.5  112.0      1    17.1\n\n\n\n\nRegression Analysis\nNext, we fit a regression model, representing the relationships between gender, age, height and the interaction variable created in the datastep above. We again use a where statement to restrict the analysis to those who are less than or equal to 19 years old. We use the clb option to get a 95% confidence interval for each of the parameters in the model. The model that we are fitting is height = b0 + b1 x female + b2 x age + b3 x fem_age + e\n\nregression&lt;-lm(HEIGHT~female+AGE+fem_age, data=htwt, AGE&lt;=19)\nsummary(regression)\n\n\nCall:\nlm(formula = HEIGHT ~ female + AGE + fem_age, data = htwt, subset = AGE &lt;= \n    19)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.2429 -1.7351  0.0383  1.6518  7.9289 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.8828     2.8734  10.052  &lt; 2e-16 ***\nfemale       13.6123     4.0192   3.387 0.000841 ***\nAGE           2.0313     0.1776  11.435  &lt; 2e-16 ***\nfem_age      -0.9294     0.2478  -3.750 0.000227 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.799 on 215 degrees of freedom\nMultiple R-squared:  0.4595,    Adjusted R-squared:  0.452 \nF-statistic: 60.93 on 3 and 215 DF,  p-value: &lt; 2.2e-16\n\n\nFrom the coefficients table b0,b1,b2,b3 are estimated as b0=28.88 b1=13.61 b2=2.03 b3=-0.92942\nThe resulting regression model for height, age and gender based on the available data is height=28.8828 + 13.6123 x female + 2.0313 x age -0.9294 x fem_age"
  },
  {
    "objectID": "R/survival.html",
    "href": "R/survival.html",
    "title": "Survival Analysis Using R",
    "section": "",
    "text": "The most commonly used survival analysis methods in clinical trials include:\nAdditionally, other methods for analyzing time-to-event data are available, such as:\nWhile these models may be explored in a separate document, this particular document focuses solely on the three most prevalent methods: KM estimators, log-rank test and Cox PH model."
  },
  {
    "objectID": "R/survival.html#example-data",
    "href": "R/survival.html#example-data",
    "title": "Survival Analysis Using R",
    "section": "Example Data",
    "text": "Example Data\nData source: https://stats.idre.ucla.edu/sas/seminars/sas-survival/\nThe data include 500 subjects from the Worcester Heart Attack Study. This study examined several factors, such as age, gender and BMI, that may influence survival time after heart attack. Follow up time for all participants begins at the time of hospital admission after heart attack and ends with death or loss to follow up (censoring). The variables used here are:\n\nlenfol: length of followup, terminated either by death or censoring - time variable\nfstat: loss to followup = 0, death = 1 - censoring variable\nafb: atrial fibrillation, no = 0, 1 = yes - explanatory variable\ngender: males = 0, females = 1 - stratification factor\n\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(survival)\nlibrary(survminer)\nlibrary(broom)\nlibrary(knitr)\nknitr::opts_chunk$set(echo = TRUE)\n\ndat &lt;- read_sas(file.path(\"../data/whas500.sas7bdat\")) %&gt;%\n  mutate(LENFOLY = round(LENFOL/365.25, 2), ## change follow-up days to years for better visualization\n         AFB = factor(AFB, levels = c(1, 0))) ## change AFB order to use \"Yes\" as the reference group to be consistent with SAS"
  },
  {
    "objectID": "R/survival.html#the-non-stratified-model",
    "href": "R/survival.html#the-non-stratified-model",
    "title": "Survival Analysis Using R",
    "section": "The Non-stratified Model",
    "text": "The Non-stratified Model\nFirst we try a non-stratified analysis following the mock-up above to describe the association between survival time and afb (atrial fibrillation).\nThe KM estimators are from survival::survfit function, the log-rank test uses survminer::surv_pvalue, and Cox PH model is conducted using survival::coxph function. Numerous R packages and functions are available for performing survival analysis. The author has selected survival and survminer for use in this context, but alternative options can also be employed for survival analysis.\n\nKM estimators\n\nfit.km &lt;- survfit(Surv(LENFOLY, FSTAT) ~ AFB, data = dat)\n\n## quantile estimates\nquantile(fit.km, probs = c(0.25, 0.5, 0.75)) \n\n$quantile\n        25   50   75\nAFB=1 0.26 2.37 6.43\nAFB=0 0.94 5.91 6.44\n\n$lower\n        25   50   75\nAFB=1 0.05 1.27 4.24\nAFB=0 0.55 4.32 6.44\n\n$upper\n        25   50 75\nAFB=1 1.11 4.24 NA\nAFB=0 1.47   NA NA\n\n## landmark estimates at 1, 3, 5-year\nsummary(fit.km, times = c(1, 3, 5)) \n\nCall: survfit(formula = Surv(LENFOLY, FSTAT) ~ AFB, data = dat)\n\n                AFB=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1     50      28    0.641  0.0543        0.543        0.757\n    3     27      12    0.455  0.0599        0.351        0.589\n    5     11       6    0.315  0.0643        0.211        0.470\n\n                AFB=0 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    312     110    0.739  0.0214        0.699        0.782\n    3    199      33    0.642  0.0245        0.595        0.691\n    5     77      20    0.530  0.0311        0.472        0.595\n\n\n\n\nLog-rank test\n\nsurvminer::surv_pvalue(fit.km, data = dat)\n\n  variable         pval   method    pval.txt\n1      AFB 0.0009646027 Log-rank p = 0.00096\n\n\n\n\nCox PH model\n\nfit.cox &lt;- coxph(Surv(LENFOLY, FSTAT) ~ AFB, data = dat)\nfit.cox %&gt;% \n  tidy(exponentiate = TRUE, conf.int = TRUE, conf.level = 0.95) %&gt;%\n  select(term, estimate, conf.low, conf.high)\n\n# A tibble: 1 × 4\n  term  estimate conf.low conf.high\n  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 AFB0     0.583    0.421     0.806"
  },
  {
    "objectID": "R/survival.html#the-stratified-model",
    "href": "R/survival.html#the-stratified-model",
    "title": "Survival Analysis Using R",
    "section": "The Stratified Model",
    "text": "The Stratified Model\nIn a stratified model, the Kaplan-Meier estimators remain the same as those in the non-stratified model. To implement stratified log-rank tests and Cox proportional hazards models, simply include the strata() function within the model formula.\n\nStratified Log-rank test\n\nfit.km.str &lt;- survfit(Surv(LENFOLY, FSTAT) ~ AFB + strata(GENDER), data = dat)\n\nsurvminer::surv_pvalue(fit.km.str, data = dat)\n\n            variable        pval   method   pval.txt\n1 AFB+strata(GENDER) 0.001506607 Log-rank p = 0.0015\n\n\n\n\nStratified Cox PH model\n\nfit.cox.str &lt;- coxph(Surv(LENFOLY, FSTAT) ~ AFB + strata(GENDER), data = dat)\nfit.cox.str %&gt;% \n  tidy(exponentiate = TRUE, conf.int = TRUE, conf.level = 0.95) %&gt;%\n  select(term, estimate, conf.low, conf.high)\n\n# A tibble: 1 × 4\n  term  estimate conf.low conf.high\n  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 AFB0     0.594    0.430     0.823"
  },
  {
    "objectID": "R/ttest_2Sample.html",
    "href": "R/ttest_2Sample.html",
    "title": "Two Sample t-test",
    "section": "",
    "text": "The Two Sample t-test is used to compare two independent samples against each other. In the Two Sample t-test, the mean of the first sample is compared against the mean of the second sample. In R, a Two Sample t-test can be performed using the Base R t.test() function from the stats package or the proc_ttest() function from the procs package.\n\n\nThe following data was used in this example.\n\n# Create sample data\nd1 &lt;- tibble::tribble(\n  ~trt_grp, ~WtGain,\n  \"placebo\",    94, \"placebo\",  12, \"placebo\",  26, \"placebo\",  89,\n  \"placebo\",    88, \"placebo\",  96, \"placebo\",  85, \"placebo\",  130,\n  \"placebo\",    75, \"placebo\",  54, \"placebo\",  112, \"placebo\", 69,\n  \"placebo\",    104, \"placebo\", 95, \"placebo\",  53, \"placebo\",  21,\n  \"treatment\",  45, \"treatment\",    62, \"treatment\",    96, \"treatment\",    128,\n  \"treatment\",  120, \"treatment\",   99, \"treatment\",    28, \"treatment\",    50,\n  \"treatment\",  109, \"treatment\",   115, \"treatment\",   39, \"treatment\",    96,\n  \"treatment\",  87, \"treatment\",    100, \"treatment\",   76, \"treatment\",    80\n)\n\n\n\n\nIf we have normalized data, we can use the classic Student’s t-test. For a Two sample test where the variances are not equal, we should use the Welch’s t-test. Both of those options are available with the Base R t.test() function.\n\n\n\n\nThe following code was used to test the comparison in Base R. By default, the R two sample t-test function assumes the variances in the data are unequal, and uses a Welch’s t-test. Therefore, to use a classic Student’s t-test with normalized data, we must specify var.equal = TRUE. Also note that we must separate the single variable into two variables to satisfy the t.test() syntax and set paired = FALSE.\n\n  d1p &lt;- dplyr::filter(d1, trt_grp == 'placebo')\n  d1t &lt;- dplyr::filter(d1, trt_grp == 'treatment')\n\n  # Perform t-test\n  t.test(d1p$WtGain, d1t$WtGain, \n       var.equal = TRUE, paired = FALSE)\n\n\n    Two Sample t-test\n\ndata:  d1p$WtGain and d1t$WtGain\nt = -0.6969, df = 30, p-value = 0.4912\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -31.19842  15.32342\nsample estimates:\nmean of x mean of y \n  75.1875   83.1250 \n\n\n\n\n\n\n\n\nThe following code was used to test the comparison in Base R using Welch’s t-test. Observe that in this case, the var.equal parameter is set to FALSE.\n\n  d1p &lt;- dplyr::filter(d1, trt_grp == 'placebo')\n  d1t &lt;- dplyr::filter(d1, trt_grp == 'treatment')\n\n  # Perform t-test\n  t.test(d1p$WtGain, d1t$WtGain, \n       var.equal = FALSE, paired = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  d1p$WtGain and d1t$WtGain\nt = -0.6969, df = 29.694, p-value = 0.4913\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -31.20849  15.33349\nsample estimates:\nmean of x mean of y \n  75.1875   83.1250 \n\n\n\n\n\n\n\n\n\n\n\nThe following code from the procs package was used to perform a two sample t-test. Note that the proc_ttest() function performs both the Student’s t-test and Welch’s (Satterthwaite) t-test in the same call. The results are displayed on separate rows. This output is similar to SAS.\n\n  library(procs)\n\n  # Perform t-test\n  proc_ttest(d1, var = WtGain,\n             class = trt_grp)\n\n$Statistics\n     VAR      CLASS        METHOD  N    MEAN      STD    STDERR MIN MAX\n1 WtGain    placebo          &lt;NA&gt; 16 75.1875 33.81167  8.452918  12 130\n2 WtGain  treatment          &lt;NA&gt; 16 83.1250 30.53495  7.633738  28 128\n3 WtGain Diff (1-2)        Pooled NA -7.9375       NA 11.389723  NA  NA\n4 WtGain Diff (1-2) Satterthwaite NA -7.9375       NA 11.389723  NA  NA\n\n$ConfLimits\n     VAR      CLASS        METHOD    MEAN      LCLM     UCLM      STD  LCLMSTD\n1 WtGain    placebo          &lt;NA&gt; 75.1875  57.17053 93.20447 33.81167 24.97685\n2 WtGain  treatment          &lt;NA&gt; 83.1250  66.85407 99.39593 30.53495 22.55632\n3 WtGain Diff (1-2)        Pooled -7.9375 -31.19842 15.32342       NA       NA\n4 WtGain Diff (1-2) Satterthwaite -7.9375 -31.20849 15.33349       NA       NA\n   UCLMSTD\n1 52.33003\n2 47.25868\n3       NA\n4       NA\n\n$TTests\n     VAR        METHOD VARIANCES       DF          T     PROBT\n1 WtGain        Pooled     Equal 30.00000 -0.6969002 0.4912306\n2 WtGain Satterthwaite   Unequal 29.69359 -0.6969002 0.4912856\n\n$Equality\n     VAR   METHOD NDF DDF     FVAL     PROBF\n1 WtGain Folded F  15  15 1.226136 0.6980614\n\n\nViewer Output:"
  },
  {
    "objectID": "R/ttest_2Sample.html#base-r",
    "href": "R/ttest_2Sample.html#base-r",
    "title": "Two Sample t-test",
    "section": "",
    "text": "If we have normalized data, we can use the classic Student’s t-test. For a Two sample test where the variances are not equal, we should use the Welch’s t-test. Both of those options are available with the Base R t.test() function.\n\n\n\n\nThe following code was used to test the comparison in Base R. By default, the R two sample t-test function assumes the variances in the data are unequal, and uses a Welch’s t-test. Therefore, to use a classic Student’s t-test with normalized data, we must specify var.equal = TRUE. Also note that we must separate the single variable into two variables to satisfy the t.test() syntax and set paired = FALSE.\n\n  d1p &lt;- dplyr::filter(d1, trt_grp == 'placebo')\n  d1t &lt;- dplyr::filter(d1, trt_grp == 'treatment')\n\n  # Perform t-test\n  t.test(d1p$WtGain, d1t$WtGain, \n       var.equal = TRUE, paired = FALSE)\n\n\n    Two Sample t-test\n\ndata:  d1p$WtGain and d1t$WtGain\nt = -0.6969, df = 30, p-value = 0.4912\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -31.19842  15.32342\nsample estimates:\nmean of x mean of y \n  75.1875   83.1250 \n\n\n\n\n\n\n\n\nThe following code was used to test the comparison in Base R using Welch’s t-test. Observe that in this case, the var.equal parameter is set to FALSE.\n\n  d1p &lt;- dplyr::filter(d1, trt_grp == 'placebo')\n  d1t &lt;- dplyr::filter(d1, trt_grp == 'treatment')\n\n  # Perform t-test\n  t.test(d1p$WtGain, d1t$WtGain, \n       var.equal = FALSE, paired = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  d1p$WtGain and d1t$WtGain\nt = -0.6969, df = 29.694, p-value = 0.4913\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -31.20849  15.33349\nsample estimates:\nmean of x mean of y \n  75.1875   83.1250"
  },
  {
    "objectID": "R/ttest_2Sample.html#procs-package",
    "href": "R/ttest_2Sample.html#procs-package",
    "title": "Two Sample t-test",
    "section": "",
    "text": "The following code from the procs package was used to perform a two sample t-test. Note that the proc_ttest() function performs both the Student’s t-test and Welch’s (Satterthwaite) t-test in the same call. The results are displayed on separate rows. This output is similar to SAS.\n\n  library(procs)\n\n  # Perform t-test\n  proc_ttest(d1, var = WtGain,\n             class = trt_grp)\n\n$Statistics\n     VAR      CLASS        METHOD  N    MEAN      STD    STDERR MIN MAX\n1 WtGain    placebo          &lt;NA&gt; 16 75.1875 33.81167  8.452918  12 130\n2 WtGain  treatment          &lt;NA&gt; 16 83.1250 30.53495  7.633738  28 128\n3 WtGain Diff (1-2)        Pooled NA -7.9375       NA 11.389723  NA  NA\n4 WtGain Diff (1-2) Satterthwaite NA -7.9375       NA 11.389723  NA  NA\n\n$ConfLimits\n     VAR      CLASS        METHOD    MEAN      LCLM     UCLM      STD  LCLMSTD\n1 WtGain    placebo          &lt;NA&gt; 75.1875  57.17053 93.20447 33.81167 24.97685\n2 WtGain  treatment          &lt;NA&gt; 83.1250  66.85407 99.39593 30.53495 22.55632\n3 WtGain Diff (1-2)        Pooled -7.9375 -31.19842 15.32342       NA       NA\n4 WtGain Diff (1-2) Satterthwaite -7.9375 -31.20849 15.33349       NA       NA\n   UCLMSTD\n1 52.33003\n2 47.25868\n3       NA\n4       NA\n\n$TTests\n     VAR        METHOD VARIANCES       DF          T     PROBT\n1 WtGain        Pooled     Equal 30.00000 -0.6969002 0.4912306\n2 WtGain Satterthwaite   Unequal 29.69359 -0.6969002 0.4912856\n\n$Equality\n     VAR   METHOD NDF DDF     FVAL     PROBF\n1 WtGain Folded F  15  15 1.226136 0.6980614\n\n\nViewer Output:"
  },
  {
    "objectID": "R/nonpara_wilcoxon_ranksum.html",
    "href": "R/nonpara_wilcoxon_ranksum.html",
    "title": "Wilcoxon Rank Sum (Mann Whitney-U) in R",
    "section": "",
    "text": "Wilcoxon rank sum test, or equivalently, Mann-Whitney U-test is a rank based non-paramatric method. The aim is to examine the differences between two groups. To be more specific, it tests whether the median difference between pairs is equal to zero.\nIt is the non-parametric equivalent to two-sample t-test, where the two groups are not paired.\n\n\nThe stats package implements various classic statistical tests, including wilcoxon rank sum test.\n\n# x, y are two unpaired vectors. Do not necessary need to be of the same length.\nstats::wilcox.test(x, y, paired = F)\n\n\n\n\nData source: Table 30.4, Kirkwood BR. and Sterne JAC. Essentials of medical statistics. Second Edition. ISBN 978-0-86542-871-3\nComparison of birth weights (kg) of children born to 15 non-smokers with those of children born to 14 heavy smokers.\n\n# bw_ns: non smokers\n# bw_s: smokers\nbw_ns &lt;- c(3.99, 3.89, 3.6, 3.73, 3.31, \n            3.7, 4.08, 3.61, 3.83, 3.41, \n            4.13, 3.36, 3.54, 3.51, 2.71)\nbw_s &lt;- c(3.18, 2.74, 2.9, 3.27, 3.65, \n           3.42, 3.23, 2.86, 3.6, 3.65, \n           3.69, 3.53, 2.38, 2.34)\n\nCan visualize the data on two histograms. Red lines indicate the location of medians.\n\npar(mfrow =c(1,2))\nhist(bw_ns, main = 'Birthweight: non-smokers')\nabline(v = median(bw_ns), col = 'red', lwd = 2)\nhist(bw_s, main = 'Birthweight: smokers')\nabline(v = median(bw_s), col = 'red', lwd = 2)\n\n\n\n\n\n\n\n\nIt is possible to see that for non-smokers, the median birthweight is higher than those of smokers. Now we can formally test it with wilcoxon rank sum test.\nThe default test is two-sided with confidence level of 0.95, and does continuity correction.\n\n# default is two sided\nstats::wilcox.test(bw_ns, bw_s, paired = F)\n\nWarning in wilcox.test.default(bw_ns, bw_s, paired = F): cannot compute exact\np-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  bw_ns and bw_s\nW = 164.5, p-value = 0.01001\nalternative hypothesis: true location shift is not equal to 0\n\n\nWe can also carry out a one-sided test, by specifying alternative = greater (if the first item is greater than the second).\n\n# default is two sided\nstats::wilcox.test(bw_ns, bw_s, paired = F, alternative = 'greater')\n\nWarning in wilcox.test.default(bw_ns, bw_s, paired = F, alternative =\n\"greater\"): cannot compute exact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  bw_ns and bw_s\nW = 164.5, p-value = 0.005003\nalternative hypothesis: true location shift is greater than 0"
  },
  {
    "objectID": "R/nonpara_wilcoxon_ranksum.html#available-r-package",
    "href": "R/nonpara_wilcoxon_ranksum.html#available-r-package",
    "title": "Wilcoxon Rank Sum (Mann Whitney-U) in R",
    "section": "",
    "text": "The stats package implements various classic statistical tests, including wilcoxon rank sum test.\n\n# x, y are two unpaired vectors. Do not necessary need to be of the same length.\nstats::wilcox.test(x, y, paired = F)"
  },
  {
    "objectID": "R/nonpara_wilcoxon_ranksum.html#example-birth-weight",
    "href": "R/nonpara_wilcoxon_ranksum.html#example-birth-weight",
    "title": "Wilcoxon Rank Sum (Mann Whitney-U) in R",
    "section": "",
    "text": "Data source: Table 30.4, Kirkwood BR. and Sterne JAC. Essentials of medical statistics. Second Edition. ISBN 978-0-86542-871-3\nComparison of birth weights (kg) of children born to 15 non-smokers with those of children born to 14 heavy smokers.\n\n# bw_ns: non smokers\n# bw_s: smokers\nbw_ns &lt;- c(3.99, 3.89, 3.6, 3.73, 3.31, \n            3.7, 4.08, 3.61, 3.83, 3.41, \n            4.13, 3.36, 3.54, 3.51, 2.71)\nbw_s &lt;- c(3.18, 2.74, 2.9, 3.27, 3.65, \n           3.42, 3.23, 2.86, 3.6, 3.65, \n           3.69, 3.53, 2.38, 2.34)\n\nCan visualize the data on two histograms. Red lines indicate the location of medians.\n\npar(mfrow =c(1,2))\nhist(bw_ns, main = 'Birthweight: non-smokers')\nabline(v = median(bw_ns), col = 'red', lwd = 2)\nhist(bw_s, main = 'Birthweight: smokers')\nabline(v = median(bw_s), col = 'red', lwd = 2)\n\n\n\n\n\n\n\n\nIt is possible to see that for non-smokers, the median birthweight is higher than those of smokers. Now we can formally test it with wilcoxon rank sum test.\nThe default test is two-sided with confidence level of 0.95, and does continuity correction.\n\n# default is two sided\nstats::wilcox.test(bw_ns, bw_s, paired = F)\n\nWarning in wilcox.test.default(bw_ns, bw_s, paired = F): cannot compute exact\np-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  bw_ns and bw_s\nW = 164.5, p-value = 0.01001\nalternative hypothesis: true location shift is not equal to 0\n\n\nWe can also carry out a one-sided test, by specifying alternative = greater (if the first item is greater than the second).\n\n# default is two sided\nstats::wilcox.test(bw_ns, bw_s, paired = F, alternative = 'greater')\n\nWarning in wilcox.test.default(bw_ns, bw_s, paired = F, alternative =\n\"greater\"): cannot compute exact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  bw_ns and bw_s\nW = 164.5, p-value = 0.005003\nalternative hypothesis: true location shift is greater than 0"
  },
  {
    "objectID": "R/ttest_1Sample.html",
    "href": "R/ttest_1Sample.html",
    "title": "One Sample t-test",
    "section": "",
    "text": "The One Sample t-test is used to compare a single sample against an expected hypothesis value. In the One Sample t-test, the mean of the sample is compared against the hypothesis value. In R, a One Sample t-test can be performed using the Base R t.test() from the stats package or the proc_ttest() function from the procs package.\n\n\nThe following data was used in this example.\n\n# Create sample data\nread &lt;- tibble::tribble(\n  ~score, ~count,\n  40, 2,   47, 2,   52, 2,   26, 1,   19, 2,\n  25, 2,   35, 4,   39, 1,   26, 1,   48, 1,\n  14, 2,   22, 1,   42, 1,   34, 2 ,  33, 2,\n  18, 1,   15, 1,   29, 1,   41, 2,   44, 1,\n  51, 1,   43, 1,   27, 2,   46, 2,   28, 1,\n  49, 1,   31, 1,   28, 1,   54, 1,   45, 1\n)\n\n\n\n\nBy default, the R one sample t-test functions assume normality in the data and use a classic Student’s t-test.\n\n\n\n\nThe following code was used to test the comparison in Base R. Note that the baseline null hypothesis goes in the “mu” parameter.\n\n  # Perform t-test\n  t.test(read$score, mu = 30)\n\n\n    One Sample t-test\n\ndata:  read$score\nt = 2.3643, df = 29, p-value = 0.02497\nalternative hypothesis: true mean is not equal to 30\n95 percent confidence interval:\n 30.67928 39.38739\nsample estimates:\nmean of x \n 35.03333 \n\n\n\n\n\n\n\n\nThe following code from the procs package was used to perform a one sample t-test. Note that the null hypothesis value goes in the “options” parameter.\n\n  library(procs)\n\n  # Perform t-test\n  proc_ttest(read, var = score,\n             options = c(\"h0\" = 30))\n\n$Statistics\n    VAR  N     MEAN      STD   STDERR MIN MAX\n1 score 30 35.03333 11.66038 2.128884  14  54\n\n$ConfLimits\n    VAR     MEAN     LCLM     UCLM      STD  LCLMSTD  UCLMSTD\n1 score 35.03333 30.67928 39.38739 11.66038 9.286404 15.67522\n\n$TTests\n    VAR DF        T     PROBT\n1 score 29 2.364306 0.0249741\n\n\nViewer Output:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Base R t.test() function does not have an option for lognormal data. Likewise, the procs proc_ttest() function also does not have an option for lognormal data.\nOne possibility may be the tTestLnormAltPower() function from the EnvStats package. This package has not been evaluated yet."
  },
  {
    "objectID": "R/ttest_1Sample.html#normal",
    "href": "R/ttest_1Sample.html#normal",
    "title": "One Sample t-test",
    "section": "",
    "text": "By default, the R one sample t-test functions assume normality in the data and use a classic Student’s t-test.\n\n\n\n\nThe following code was used to test the comparison in Base R. Note that the baseline null hypothesis goes in the “mu” parameter.\n\n  # Perform t-test\n  t.test(read$score, mu = 30)\n\n\n    One Sample t-test\n\ndata:  read$score\nt = 2.3643, df = 29, p-value = 0.02497\nalternative hypothesis: true mean is not equal to 30\n95 percent confidence interval:\n 30.67928 39.38739\nsample estimates:\nmean of x \n 35.03333 \n\n\n\n\n\n\n\n\nThe following code from the procs package was used to perform a one sample t-test. Note that the null hypothesis value goes in the “options” parameter.\n\n  library(procs)\n\n  # Perform t-test\n  proc_ttest(read, var = score,\n             options = c(\"h0\" = 30))\n\n$Statistics\n    VAR  N     MEAN      STD   STDERR MIN MAX\n1 score 30 35.03333 11.66038 2.128884  14  54\n\n$ConfLimits\n    VAR     MEAN     LCLM     UCLM      STD  LCLMSTD  UCLMSTD\n1 score 35.03333 30.67928 39.38739 11.66038 9.286404 15.67522\n\n$TTests\n    VAR DF        T     PROBT\n1 score 29 2.364306 0.0249741\n\n\nViewer Output:"
  },
  {
    "objectID": "R/ttest_1Sample.html#lognormal",
    "href": "R/ttest_1Sample.html#lognormal",
    "title": "One Sample t-test",
    "section": "",
    "text": "The Base R t.test() function does not have an option for lognormal data. Likewise, the procs proc_ttest() function also does not have an option for lognormal data.\nOne possibility may be the tTestLnormAltPower() function from the EnvStats package. This package has not been evaluated yet."
  },
  {
    "objectID": "R/mcnemar.html",
    "href": "R/mcnemar.html",
    "title": "McNemar’s test in R",
    "section": "",
    "text": "Performing McNemar’s test in R\nTo demonstrate McNemar’s test, data was used concerning the presence or absence of cold symptoms reported by the same children at age 12 and age 14. A total of 2638 participants were involved.\n\nUsing the epibasix::mcnemar function\nTesting for a significant difference in cold symptoms between the two ages using the mcNemar function from the epibasix package can be performed as below. The symptoms for participants at age 12 and age 14 are tabulated and stored as an object, then passed to the mcNemar function. A more complete view of the output is achieved by calling the summary function.\n\nlibrary(epibasix)\n\nX &lt;- table(colds$age12, colds$age14)\nepi_mcn &lt;- mcNemar(X)\nsummary(epi_mcn)\n\n\nMatched Pairs Analysis: McNemar's Statistic and Odds Ratio (Detailed Summary):\n \n     \n       No Yes\n  No  707 256\n  Yes 144 212\n\nEntries in above matrix correspond to number of pairs. \n \nMcNemar's Chi^2 Statistic (corrected for continuity) = 30.802 which has a p-value of: 0\nNote: The p.value for McNemar's Test corresponds to the hypothesis test: H0: OR = 1 vs. HA: OR != 1\nMcNemar's Odds Ratio (b/c): 1.778\n95% Confidence Limits for the OR are: [1.449, 2.208]\nThe risk difference is: 0.085\n95% Confidence Limits for the rd are: [0.055, 0.115]\n\n\n\n\nUsing the stats::mcnemar.test function\nMcNemar’s test can also be performed using stats::mcnemar.test as shown below, using the same table X as in the previous section.\n\nmcnemar.test(X)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  X\nMcNemar's chi-squared = 30.802, df = 1, p-value = 2.857e-08\n\n\nThe result is shown without continuity correction by specifying correct=FALSE.\n\nmcnemar.test(X, correct=FALSE)\n\n\n    McNemar's Chi-squared test\n\ndata:  X\nMcNemar's chi-squared = 31.36, df = 1, p-value = 2.144e-08\n\n\n\n\nResults\nAs default, using summary with epibasix::mcNemar gives additional information to the McNemar’s chi-square statistic. This includes a table to view proportions, and odds ratio and risk difference with 95% confidence limits. The result uses Edward’s continuity correction without the option to remove this, which is consistent with other functions within the package.\nstats::mcnemar.test uses a continuity correction as default but does allow for this to be removed. This function does not output any other coefficients for agreement or proportions but (if required) these can be achieved within other functions or packages in R."
  },
  {
    "objectID": "R/ttest_Paired.html",
    "href": "R/ttest_Paired.html",
    "title": "Paired t-test",
    "section": "",
    "text": "The Paired t-test is used when two samples are naturally correlated. In the Paired t-test, the difference of the means between the two samples is compared to a given number that represents the null hypothesis. For a Paired t-test, the number of observations in each sample must be equal.\nIn R, a Paired t-test can be performed using the Base R t.test() from the stats package or the proc_ttest() function from the procs package.\n\n\nBy default, the R paired t-test functions assume normality in the data and use a classic Student’s t-test.\n\n\nThe following data was used in this example.\n\n# Create sample data\npressure &lt;- tibble::tribble(\n  ~SBPbefore, ~SBPafter,\n  120, 128,   \n  124, 131,   \n  130, 131,   \n  118, 127,\n  140, 132,   \n  128, 125,   \n  140, 141,   \n  135, 137,\n  126, 118,   \n  130, 132,   \n  126, 129,   \n  127, 135\n)\n\n\n\n\n\n\nThe following code was used to test the comparison in Base R.\n\n  # Perform t-test\n  t.test(pressure$SBPbefore, pressure$SBPafter, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pressure$SBPbefore and pressure$SBPafter\nt = -1.0896, df = 11, p-value = 0.2992\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -5.536492  1.869825\nsample estimates:\nmean difference \n      -1.833333 \n\n\n\n\n\n\n\n\nThe following code from the procs package was used to perform a paired t-test.\n\n  library(procs)\n\n  # Perform t-test\n  proc_ttest(pressure,\n     paired = \"SBPbefore*SBPafter\")\n\n$Statistics\n       VAR1     VAR2               DIFF  N      MEAN      STD   STDERR MIN MAX\n1 SBPbefore SBPafter SBPbefore-SBPafter 12 -1.833333 5.828353 1.682501  -9   8\n\n$ConfLimits\n       VAR1     VAR2               DIFF      MEAN      LCLM     UCLM      STD\n1 SBPbefore SBPafter SBPbefore-SBPafter -1.833333 -5.536492 1.869825 5.828353\n   LCLMSTD  UCLMSTD\n1 4.128777 9.895832\n\n$TTests\n       VAR1     VAR2               DIFF DF         T     PROBT\n1 SBPbefore SBPafter SBPbefore-SBPafter 11 -1.089648 0.2991635\n\n\nViewer Output:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Base R t.test() function does not have an option for lognormal data. Likewise, the procs proc_ttest() function also does not have an option for lognormal data.\nOne possibility may be the tTestLnormAltPower() function from the EnvStats package. This package has not been evaluated yet."
  },
  {
    "objectID": "R/ttest_Paired.html#normal",
    "href": "R/ttest_Paired.html#normal",
    "title": "Paired t-test",
    "section": "",
    "text": "By default, the R paired t-test functions assume normality in the data and use a classic Student’s t-test.\n\n\nThe following data was used in this example.\n\n# Create sample data\npressure &lt;- tibble::tribble(\n  ~SBPbefore, ~SBPafter,\n  120, 128,   \n  124, 131,   \n  130, 131,   \n  118, 127,\n  140, 132,   \n  128, 125,   \n  140, 141,   \n  135, 137,\n  126, 118,   \n  130, 132,   \n  126, 129,   \n  127, 135\n)\n\n\n\n\n\n\nThe following code was used to test the comparison in Base R.\n\n  # Perform t-test\n  t.test(pressure$SBPbefore, pressure$SBPafter, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pressure$SBPbefore and pressure$SBPafter\nt = -1.0896, df = 11, p-value = 0.2992\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -5.536492  1.869825\nsample estimates:\nmean difference \n      -1.833333 \n\n\n\n\n\n\n\n\nThe following code from the procs package was used to perform a paired t-test.\n\n  library(procs)\n\n  # Perform t-test\n  proc_ttest(pressure,\n     paired = \"SBPbefore*SBPafter\")\n\n$Statistics\n       VAR1     VAR2               DIFF  N      MEAN      STD   STDERR MIN MAX\n1 SBPbefore SBPafter SBPbefore-SBPafter 12 -1.833333 5.828353 1.682501  -9   8\n\n$ConfLimits\n       VAR1     VAR2               DIFF      MEAN      LCLM     UCLM      STD\n1 SBPbefore SBPafter SBPbefore-SBPafter -1.833333 -5.536492 1.869825 5.828353\n   LCLMSTD  UCLMSTD\n1 4.128777 9.895832\n\n$TTests\n       VAR1     VAR2               DIFF DF         T     PROBT\n1 SBPbefore SBPafter SBPbefore-SBPafter 11 -1.089648 0.2991635\n\n\nViewer Output:"
  },
  {
    "objectID": "R/ttest_Paired.html#lognormal",
    "href": "R/ttest_Paired.html#lognormal",
    "title": "Paired t-test",
    "section": "",
    "text": "The Base R t.test() function does not have an option for lognormal data. Likewise, the procs proc_ttest() function also does not have an option for lognormal data.\nOne possibility may be the tTestLnormAltPower() function from the EnvStats package. This package has not been evaluated yet."
  },
  {
    "objectID": "R/mi_mar_regression.html",
    "href": "R/mi_mar_regression.html",
    "title": "Multiple Imputaton: Linear Regression",
    "section": "",
    "text": "Multiple imputation with regression is one step further from mean imputation (i.e. by a single value: the average of observed). In the case for continuous, normally distributed variable, linear regression can use information from other variables hence could be closer to the true missing values."
  },
  {
    "objectID": "R/mi_mar_regression.html#overview",
    "href": "R/mi_mar_regression.html#overview",
    "title": "Multiple Imputaton: Linear Regression",
    "section": "",
    "text": "Multiple imputation with regression is one step further from mean imputation (i.e. by a single value: the average of observed). In the case for continuous, normally distributed variable, linear regression can use information from other variables hence could be closer to the true missing values."
  },
  {
    "objectID": "R/mi_mar_regression.html#imputation-with-mice",
    "href": "R/mi_mar_regression.html#imputation-with-mice",
    "title": "Multiple Imputaton: Linear Regression",
    "section": "Imputation with mice",
    "text": "Imputation with mice\nmice is a powerful R package developed by Stef van Buuren, Karin Groothuis-Oudshoorn and other contributors. Regression methods (continuous, normal outcome) are implemented in mice with methods starting with norm.\n\nLinear regression without parameter uncertainty, mice.impute.norm.nob\nLinear regression through prediction, mice.impute.norm.predict\nBayesian linear regression, mice.impute.norm\nLinear regression bootstrap, mice.impute.norm.boot"
  },
  {
    "objectID": "R/mi_mar_regression.html#example",
    "href": "R/mi_mar_regression.html#example",
    "title": "Multiple Imputaton: Linear Regression",
    "section": "Example",
    "text": "Example\nHere I use the small dataset nhanes included in mice package. It has 25 rows, and three out of four variables have missings.\nThe original NHANES data is a large national level survey, some are publicly available via R package nhanes.\n\nlibrary(mice)\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n# load example dataset from mice\nhead(nhanes)\n\n  age  bmi hyp chl\n1   1   NA  NA  NA\n2   2 22.7   1 187\n3   1   NA   1 187\n4   3   NA  NA  NA\n5   1 20.4   1 113\n6   3   NA  NA 184\n\nsummary(nhanes)\n\n      age            bmi             hyp             chl       \n Min.   :1.00   Min.   :20.40   Min.   :1.000   Min.   :113.0  \n 1st Qu.:1.00   1st Qu.:22.65   1st Qu.:1.000   1st Qu.:185.0  \n Median :2.00   Median :26.75   Median :1.000   Median :187.0  \n Mean   :1.76   Mean   :26.56   Mean   :1.235   Mean   :191.4  \n 3rd Qu.:2.00   3rd Qu.:28.93   3rd Qu.:1.000   3rd Qu.:212.0  \n Max.   :3.00   Max.   :35.30   Max.   :2.000   Max.   :284.0  \n                NA's   :9       NA's   :8       NA's   :10     \n\n\nExamine missing pattern with md.pattern(data).\n\n# 27 missing in total\n# by col: 8 for hyp, 9 for bmi, 10 for chl\n# by row: n missing numbers\n\nmd.pattern(nhanes)\n\n\n\n\n\n\n\n\n   age hyp bmi chl   \n13   1   1   1   1  0\n3    1   1   1   0  1\n1    1   1   0   1  1\n1    1   0   0   1  2\n7    1   0   0   0  3\n     0   8   9  10 27\n\n\n\nRegression without parameter uncertainty\nWe can generate two imputed datasets by setting m=2.\nThere is a certain level of randomness, so would be a good idea to set seed.\n\nset.seed(1)\nimpr0 &lt;- mice(nhanes, method = 'norm.nob', m=2, maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  1   2  bmi  hyp  chl\n\nimpr0\n\nClass: mids\nNumber of multiple imputations:  2 \nImputation methods:\n       age        bmi        hyp        chl \n        \"\" \"norm.nob\" \"norm.nob\" \"norm.nob\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\nnhanes_impr0 &lt;- complete(impr0) # by default, returns the first imputation\nnhanes_impr0\n\n   age      bmi       hyp      chl\n1    1 35.53430 1.2503841 256.6153\n2    2 22.70000 1.0000000 187.0000\n3    1 27.31412 1.0000000 187.0000\n4    3 25.31243 2.3880837 267.1435\n5    1 20.40000 1.0000000 113.0000\n6    3 17.94547 1.5855064 184.0000\n7    1 22.50000 1.0000000 118.0000\n8    1 30.10000 1.0000000 187.0000\n9    2 22.00000 1.0000000 238.0000\n10   2 26.99782 1.0810473 206.9927\n11   1 32.71511 0.7819353 213.7222\n12   2 27.65399 0.7904680 209.6716\n13   3 21.70000 1.0000000 206.0000\n14   2 28.70000 2.0000000 204.0000\n15   1 29.60000 1.0000000 252.1596\n16   1 27.47980 0.6071353 145.9557\n17   3 27.20000 2.0000000 284.0000\n18   2 26.30000 2.0000000 199.0000\n19   1 35.30000 1.0000000 218.0000\n20   3 25.50000 2.0000000 245.7884\n21   1 35.12809 0.5807116 232.4652\n22   1 33.20000 1.0000000 229.0000\n23   1 27.50000 1.0000000 131.0000\n24   3 24.90000 1.0000000 268.3929\n25   2 27.40000 1.0000000 186.0000\n\n\nWhen we have two imputed datasets, we can check the values for each of the variables. For example, extract bmi variable from the imputed data imp,\n\n# two imputed datasets (m=2)\nimpr0$imp$bmi\n\n          1        2\n1  35.53430 32.26078\n3  27.31412 22.55473\n4  25.31243 14.90410\n6  17.94547 22.59196\n10 26.99782 25.08534\n11 32.71511 27.71485\n12 27.65399 25.76286\n16 27.47980 30.34985\n21 35.12809 29.89142\n\n\nWe can also specify which imputed dataset to use as our complete data. Set index to 0 (action = 0) returns the original dataset with missing values.\nHere we check which of the imputed data is being used as the completed dataset. First take a note of the row IDs (based on bmi, for example). Then we generate completed dataset.\n\nif no action argument is set, then it returns the first imputation by default\naction=0 corresponds to the original data with missing values\n\n\n# check which imputed data is used for the final result, take note of row id\nid_missing &lt;- which(is.na(nhanes$bmi))\nid_missing\n\n[1]  1  3  4  6 10 11 12 16 21\n\nnhanes_impr0_action0 &lt;- complete(impr0, action = 0) \nnhanes_impr0_action0[id_missing, ] # original data with missing bmi\n\n   age bmi hyp chl\n1    1  NA  NA  NA\n3    1  NA   1 187\n4    3  NA  NA  NA\n6    3  NA  NA 184\n10   2  NA  NA  NA\n11   1  NA  NA  NA\n12   2  NA  NA  NA\n16   1  NA  NA  NA\n21   1  NA  NA  NA\n\nnhanes_impr0_action1 &lt;- complete(impr0, action = 1) \nnhanes_impr0_action1[id_missing, ] # using first imputation\n\n   age      bmi       hyp      chl\n1    1 35.53430 1.2503841 256.6153\n3    1 27.31412 1.0000000 187.0000\n4    3 25.31243 2.3880837 267.1435\n6    3 17.94547 1.5855064 184.0000\n10   2 26.99782 1.0810473 206.9927\n11   1 32.71511 0.7819353 213.7222\n12   2 27.65399 0.7904680 209.6716\n16   1 27.47980 0.6071353 145.9557\n21   1 35.12809 0.5807116 232.4652\n\nnhanes_impr0_action2 &lt;- complete(impr0, action = 2) \nnhanes_impr0_action2[id_missing, ] # using second imputation\n\n   age      bmi       hyp      chl\n1    1 32.26078 0.4616324 228.0022\n3    1 22.55473 1.0000000 187.0000\n4    3 14.90410 1.4558818 212.7958\n6    3 22.59196 1.7664882 184.0000\n10   2 25.08534 1.2940549 201.5872\n11   1 27.71485 0.9410698 169.2427\n12   2 25.76286 1.3570093 168.5961\n16   1 30.34985 0.6878971 163.7262\n21   1 29.89142 1.0452062 212.9144\n\n\n\n\nOther imputation by linear regression\nOther various of imputaton via linear regression can be implemented simply by changing the method argument.\n\nLinear regression through prediction, mice.impute.norm.predict\nBayesian linear regression, mice.impute.norm\nLinear regression bootstrap, mice.impute.norm.boot\n\n\nimpr &lt;- mice(nhanes, method = 'norm.predict', m=1, maxit=1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimpr$imp$bmi\n\n          1\n1  28.33396\n3  28.33396\n4  22.75613\n6  21.17519\n10 27.19573\n11 29.12443\n12 26.26576\n16 30.28688\n21 28.33396\n\n\nBayesian linear regression\n\nimpb &lt;- mice(nhanes, method = 'norm', m=1, maxit=1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimpb$imp$bmi\n\n          1\n1  33.82959\n3  28.98754\n4  20.88810\n6  19.11391\n10 27.32990\n11 29.44117\n12 22.68062\n16 32.13267\n21 22.03164\n\n# nhanes_impb &lt;- complete(impb)\n\nBootstrap\n\nimpbt &lt;- mice(nhanes, method = 'norm.boot', m=1, maxit=1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimpbt$imp$bmi\n\n          1\n1  24.19248\n3  28.77464\n4  22.42321\n6  23.47542\n10 21.95529\n11 23.12703\n12 25.84230\n16 27.68216\n21 26.43770"
  },
  {
    "objectID": "publication/index.html",
    "href": "publication/index.html",
    "title": "Publications",
    "section": "",
    "text": "Whitepaper\nKey Considerations When Understanding Differences in Statistical Methodology Implementations Across Programming Languages - An Introduction to the CAMIS Project\nRead the whitepaper here\n\n\nConference presentations\n\n2024 Conference Schedule\nList of seminars and conferences that the CAMIS team will be attending in 2024.\nIf you are a volunteer on the CAMIS project and plan to present at a seminar or conference, please add details of the conference below. For help with slides or content go to HERE.\nTo cite the CAMIS project work in online content or presentations please use: “Content reproduced with the permission of PHUSE CAMIS - A DVOST Working Group”.\n\n\n\nConference name\nDate (2024)\nLocation\nName Attending\nDetails\nWebsite\n\n\n\n\nPhuse US Connect\n25-28 Feb\nBethesa, Maryland, USA\nSoma Sekhar Sriadibhatla, Vikash Jain, Brian Varney\nPoster\nConnect\n\n\nPhuse/FDA CSS\n3-5 June\nSilver Spring Maryland, USA\nSoma Sekhar Sriadibhatla, Vikash Jain, Harshal Khanolkar\nWorkshop\nCSS\n\n\nRSS Local Group Seminar\n28 Feb\nSheffield, England\nLyn Taylor\nSeminar\nRSS\n\n\n\n\n\nYearly Conference Planner\nTo help to plan our attendance throughout the year, here is a list of conferences we are looking to send representation to. If you plan to attend one of these conferences and are interested in representing us, then please get in touch.\n\n\n\nConference name\nUsual Abstract Deadline\nUsual Conference Date\nRegion\nLinks\n\n\n\n\nJoint Statistical Meetings (JSM) American Statistical Association (ASA)\n1st February\n1st week of August\nUSA\nJSM-ASA\n\n\nASA Biopharmaceutical Section Regulatory-Industry Statistics workshop\nEnd March\nLast week of September\nUSA\nBIOP\n\n\nPhuse US Connect\nNovember\nLast week of Feb\nUSA\nCDISC\n\n\nPhuse/FDA Computational Science Symposium(CSS)\nDecember\n1st week of June\nUSA\nCSS\n\n\nIASCT (ConSPIC)\nMid March\nEarly May\nIndia\nIASCT\n\n\nSociety of Clinical Trials (SCT)\nJanuary\nMid May\nUSA\nSCT\n\n\nPharmaSUG\nMid Jan\nMid May\nUSA\nPharmaSUG\n\n\nuseR\nEarly March\nEarly July\nEurope/Online\nuseR\n\n\nPSI\nNov-oral, Feb-Poster\nMid June\nEurope\nPSI\n\n\nDIA Global\nEnd Feb-poster\nMid June\nUSA\nDIA-USA\n\n\nDIA Europe\nNov\nMid March\nEurope\nDIA-Europe\n\n\nDIA China\nJan\nMid May\nChina\nDIA-China\n\n\nInternational Society for Clinical Biostatistics (ISCB)\nMid Feb\nMid July\nEurope\nISCB\n\n\nRoyal Statistical Society (RSS)\nEarly April\nEarly September\nEngland\nRSS\n\n\nSouthEast SAS User Group (SESUG)\nEnd Feb\nEnd Sept\nMaryland, USA\nSESUG\n\n\nPHUSE EU Connect\nMid March\nMid Nov\nEurope\nPHUSE EU Connect\n\n\nR/Pharma\nApril\nMid October\nVirtual\nR/pharma\n\n\nPOSIT conf.\nInvite only\nSeptember\nUSA\nPOSIT conf"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CAMIS - A PHUSE DVOST Working Group",
    "section": "",
    "text": "Introduction\nSeveral discrepancies have been discovered in statistical analysis results between different programming languages, even in fully qualified statistical computing environments. Subtle differences exist between the fundamental approaches implemented by each language, yielding differences in results which are each correct in their own right. The fact that these differences exist causes unease on the behalf of sponsor companies when submitting to a regulatory agency, as it is uncertain if the agency will view these differences as problematic. In its Statistical Software Clarifying Statement, the US Food and Drug Administration (FDA) states that it “FDA does not require use of any specific software for statistical analyses” and that “the computer software used for data management and statistical analysis should be reliable.” Observing differences across languages can reduce the analyst’s confidence in reliability and, by understanding the source of any discrepancies, one can reinstate confidence in reliability.\n\nMotivation\nThe goal of this project is to demystify conflicting results between software and to help ease the transitions to new languages by providing comparison and comprehensive explanations.\n\n\nRepository\nThe repository below provides examples of statistical methodology in different software and languages, along with a comparison of the results obtained and description of any discrepancies.\n\n\n\n\n\n\n\n\n\nMethods\nR\nSAS\nComparison\n\n\n\n\nSummary Statistics\nRounding\nR\nSAS\nR vs SAS\n\n\nSummary statistics\nR\nSAS\nR vs SAS\n\n\nSkewness/Kurtosis\nR\nSAS\nR vs SAS\n\n\nGeneral Linear Models\nOne Sample t-test\nR\nSAS\nR vs SAS\n\n\nPaired t-test\nR\nSAS\nR vs SAS\n\n\nTwo Sample t-test\nR\nSAS\nR vs SAS\n\n\nANOVA\nR\nSAS\nR vs SAS\n\n\nANCOVA\nR\nSAS\nR vs SAS\n\n\nMANOVA\nR\nSAS\nR vs SAS\n\n\nLinear Regression\nR\nSAS\nR vs SAS\n\n\nGeneralized Linear Models\nLogistic Regression\nR\n\n\n\n\nPoisson/Negative Binomial Regression\n\n\n\n\n\nCategorical Repeated Measures\n\n\n\n\n\nCategorical Multiple Imputation\n\n\n\n\n\nNon-parametric Analysis\nWilcoxon signed rank\n\n\n\n\n\nMann-Whitney U/Wilcoxon rank sum\nR\n\n\n\n\nKolmogorov-Smirnov test\n\n\n\n\n\nKruskall-Wallis test\nR\nSAS\nR vs SAS\n\n\nFriedman test\n\n\n\n\n\nJonckheere test\n\n\n\n\n\nCategorical Data Analysis\nBinomial test\n\n\n\n\n\nMcNemar's test\nR\nSAS\nR vs SAS\n\n\nChi-Square Association/Fishers exact\nR\n\nR vs SAS\n\n\nCochran Mantel Haenszel\nR\nSAS\nR vs SAS\n\n\nConfidence Intervals for proportions\n\n\n\n\n\nRepeated Measures\nLinear Mixed Model (MMRM)\nR\nSAS\nR vs SAS\n\n\nGeneralized Linear Mixed Model (MMRM)\n\n\n\n\n\nBayesian MMRM\n\n\n\n\n\nMultiple Imputation - Continuous Data MAR\nMCMC\n\n\n\n\n\nLinear regression\nR\n\n\n\n\nPredictive Mean Matching\nR\n\n\n\n\nPropensity Scores\n\n\n\n\n\nMultiple Imputation - Continuous Data MNAR\nDelta Adjustment/Tipping Point\n\n\n\n\n\nReference-Based Imputation/Sequential Methods\n\n\n\n\n\nReference-Based Imputation/Joint Modelling\n\n\n\n\n\nCorrelation\nPearson's/ Spearman's/ Kendall's Rank\nR\n\n\n\n\nSurvival Models\nKaplan-Meier Log-rank test and Cox-PH\nR\nSAS\nR vs SAS\n\n\nAccelerated Failure Time\n\n\n\n\n\nNon-proportional hazards methods\n\n\n\n\n\nSample size /Power calculations\nSingle timepoint analysis\n\n\n\n\n\nGroup-sequential designs\n\n\n\n\n\nMultivariate methods\nClustering\n\n\n\n\n\nFactor analysis\n\n\n\n\n\nPCA\n\n\n\n\n\nCanonical correlation\n\n\n\n\n\nPLS\n\n\n\n\n\nOther Methods\nSurvey statistics\n\n\n\n\n\nNearest neighbour\n\n\n\n\n\nCausal inference\n\n\n\n\n\nMachine learning"
  },
  {
    "objectID": "non_website_content/conferences/2024/abstract_useR2024.html",
    "href": "non_website_content/conferences/2024/abstract_useR2024.html",
    "title": "Conference information",
    "section": "",
    "text": "Conference information\nUseR 2024 (Salzburg)\nJuly 8-11 2024\nhttps://events.linuxfoundation.org/user/\n\nSubmission\nDeadline: March 11\nWho is submitting: Chi Zhang\n\n\n\nTitle\nIntroducing CAMIS: an open-source, community endeavor for Comparing Analysis Method Implementations in Software\n\n\nAbstract\n(No longer than 1200 characters)\nStatisticians using multiple softwares (SAS, R, Python) will have found differences in analysis results that warrant further justification. Whilst some industries may accept results not being the same as long as they are “close”, the highly regulated pharmaceutical industry would require an identical match in results. Yet, discrepancies might still occur, and knowing the reasons (different methods, options, algorithms etc) is critical to the modern statistician and subsequent regulatory submissions.\nIn this talk I will introduce CAMIS: Comparing Analysis Method Implementations in Software (CAMIS). https://psiaims.github.io/CAMIS/. It is a joint-project between PHUSE DVOST, the R Validation Hub, PSI AIMS, R consortium and ASA openstatsware. The aim of CAMIS is to investigate and document differences and similarities between different statistical softwares such as SAS and R. We use Quarto and Github to document methods, algorithms and comparisons between softwares through small case studies, and all articles are contributed by the community. In the transition from proprietary to open source technology in the industry, CAMIS can serve as a guidebook to navigate this process.\n\nkeywords: cross industry collaboration, multi-lingua, open-source, quarto\n\n\n\nLearning outcomes\n\nCAMIS is a cross-industry collaboration, focusing on documenting differences between statistical softwares (SAS, R)\nWe use open source technology (Quarto, Github, R), all articles are contributed by the community\nCAMIS aims to facilitate the transition from proprietary to open source in the industry"
  },
  {
    "objectID": "SAS/cmh.html",
    "href": "SAS/cmh.html",
    "title": "CMH Test",
    "section": "",
    "text": "The CMH procedure tests for conditional independence in partial contingency tables for a 2 x 2 x K design. However, it can be generalized to tables of X x Y x K dimensions.\n\n\nThe cmh test is calculated in SAS using the PROC FREQ procedure. By default, it outputs the chi square statistic, degrees of freedom and p-value for each of the three alternative hypothesis: general association, row means differ, and nonzero correlation. It is up to the statistical analyst or statistician to know which result is appropriate for their analysis.\nWhen the design of the contingency table is 2 x 2 x K (i.e, X == 2 levels, Y == 2 levels, K &gt;= 2 levels), the Mantel-Haenszel Common Odds Ratio (odds ratio estimate, 95% CI, P-value) and the Breslow-Day Test for Homogeneity of the Odds Ratios (chi-square statistic, degrees of freedom, P-value) are also output.\nBelow is the syntax to conduct a CMH analysis in SAS:\n\nProc freq data = filtered_data; \ntables K * X * Y / cmh; \n* the order of K, X, and Y appearing on the line is important!;\nrun;"
  },
  {
    "objectID": "SAS/cmh.html#cmh-in-sas",
    "href": "SAS/cmh.html#cmh-in-sas",
    "title": "CMH Test",
    "section": "",
    "text": "The cmh test is calculated in SAS using the PROC FREQ procedure. By default, it outputs the chi square statistic, degrees of freedom and p-value for each of the three alternative hypothesis: general association, row means differ, and nonzero correlation. It is up to the statistical analyst or statistician to know which result is appropriate for their analysis.\nWhen the design of the contingency table is 2 x 2 x K (i.e, X == 2 levels, Y == 2 levels, K &gt;= 2 levels), the Mantel-Haenszel Common Odds Ratio (odds ratio estimate, 95% CI, P-value) and the Breslow-Day Test for Homogeneity of the Odds Ratios (chi-square statistic, degrees of freedom, P-value) are also output.\nBelow is the syntax to conduct a CMH analysis in SAS:\n\nProc freq data = filtered_data; \ntables K * X * Y / cmh; \n* the order of K, X, and Y appearing on the line is important!;\nrun;"
  },
  {
    "objectID": "SAS/anova.html",
    "href": "SAS/anova.html",
    "title": "linear-models",
    "section": "",
    "text": "Getting Started\nTo demonstrate the various types of sums of squares, we’ll create a data frame called df_disease taken from the SAS documentation.\n\n\nThe Model\nFor this example, we’re testing for a significant difference in stem_length using ANOVA.\n\nproc glm;\n   class drug disease;\n   model y=drug disease drug*disease;\nrun;\n\n\n\nSums of Squares Tables\nSAS has four types of sums of squares calculations. To get these calculations, the sum of squares option needs to be added (/ ss1 ss2 ss3 ss4) to the model statement.\n\nproc glm;\n   class drug disease;\n   model y=drug disease drug*disease / ss1 ss2 ss3 ss4;\nrun;\n\n\nType I\n\n\n\n\n\n\n\n\n\n\n\nType II\n\n\n\n\n\n\n\n\n\n\n\nType III\n\n\n\n\n\n\n\n\n\n\n\nType IV"
  },
  {
    "objectID": "SAS/summary_skew_kurt.html",
    "href": "SAS/summary_skew_kurt.html",
    "title": "Skewness/Kurtosis",
    "section": "",
    "text": "In SAS, Skewness and Kurtosis are usually calculated using PROC MEANS. The procedures can produce both statistics in the same call. The procedure provides options for different methodologies.\n\n\nThe following data was used in this example.\n  data dat;\n      input team $ points assists;\n      datalines;\n  A 10 2\n  A 17 5\n  A 17 6\n  A 18 3\n  A 15 0\n  B 10 2\n  B 14 5\n  B 13 4\n  B 29 0\n  B 25 2\n  C 12 1\n  C 30 1\n  C 34 3\n  C 12 4\n  C 11 7\n  ;\n  run;\n\n\n\nBy default, SAS PROC MEANS uses VARDEF option “DF”. The other options are “N”, “WEIGHT”, and “WDF. Note that the WEIGHT and WDF options produce no results, as weighted calculations are not supported in PROC MEANS for Skewness and Kurtosis.\nThe following shows the SAS documentation for the two measures.\n\n\nThe SAS documentation for Skewness is provided here for convenience:\n\n\n\n\n\n\n\n\n\n\n\n\nThe SAS documentation for Kurtosis is as follows:\n\n\n\n\n\n\n\n\n\n\n\n\nSkewness and Kurtosis are commonly calculated in SAS as follows:\n  proc means data=dat SKEWNESS KURTOSIS;\n  var points;\n  run;\nOutput:\n\n\n\n\n\n\n\n\n\nThe above results correspond to the Type 2 methodology in R.\n\n\n\nThe N option produces the following results\n  proc means data=dat SKEWNESS KURTOSIS vardef = N;\n  var points;\n  run;\nOutput:\n\n\n\n\n\n\n\n\n\nThe above results correspond to the Type 1 methodology in R.\n\n\n\n\nSAS options provide for Type 1 and Type 2 Skewness and Kurtosis. Skewness Type 3 and Kurtosis Type 3 are not supported. Also Pearson’s Kurtosis is not supported."
  },
  {
    "objectID": "SAS/summary_skew_kurt.html#sas",
    "href": "SAS/summary_skew_kurt.html#sas",
    "title": "Skewness/Kurtosis",
    "section": "",
    "text": "By default, SAS PROC MEANS uses VARDEF option “DF”. The other options are “N”, “WEIGHT”, and “WDF. Note that the WEIGHT and WDF options produce no results, as weighted calculations are not supported in PROC MEANS for Skewness and Kurtosis.\nThe following shows the SAS documentation for the two measures.\n\n\nThe SAS documentation for Skewness is provided here for convenience:\n\n\n\n\n\n\n\n\n\n\n\n\nThe SAS documentation for Kurtosis is as follows:\n\n\n\n\n\n\n\n\n\n\n\n\nSkewness and Kurtosis are commonly calculated in SAS as follows:\n  proc means data=dat SKEWNESS KURTOSIS;\n  var points;\n  run;\nOutput:\n\n\n\n\n\n\n\n\n\nThe above results correspond to the Type 2 methodology in R.\n\n\n\nThe N option produces the following results\n  proc means data=dat SKEWNESS KURTOSIS vardef = N;\n  var points;\n  run;\nOutput:\n\n\n\n\n\n\n\n\n\nThe above results correspond to the Type 1 methodology in R."
  },
  {
    "objectID": "SAS/summary_skew_kurt.html#summary",
    "href": "SAS/summary_skew_kurt.html#summary",
    "title": "Skewness/Kurtosis",
    "section": "",
    "text": "SAS options provide for Type 1 and Type 2 Skewness and Kurtosis. Skewness Type 3 and Kurtosis Type 3 are not supported. Also Pearson’s Kurtosis is not supported."
  },
  {
    "objectID": "SAS/survival.html",
    "href": "SAS/survival.html",
    "title": "Survival Analysis Using SAS",
    "section": "",
    "text": "The most commonly used survival analysis methods in clinical trials include:\nAdditionally, other methods for analyzing time-to-event data are available, such as:\nWhile these models may be explored in a separate document, this particular document focuses solely on the three most prevalent methods: KM estimators, log-rank test and Cox PH model."
  },
  {
    "objectID": "SAS/survival.html#example-data",
    "href": "SAS/survival.html#example-data",
    "title": "Survival Analysis Using SAS",
    "section": "Example Data",
    "text": "Example Data\nData source: https://stats.idre.ucla.edu/sas/seminars/sas-survival/\nThe data include 500 subjects from the Worcester Heart Attack Study. This study examined several factors, such as age, gender and BMI, that may influence survival time after heart attack. Follow up time for all participants begins at the time of hospital admission after heart attack and ends with death or loss to follow up (censoring). The variables used here are:\n\nlenfol: length of followup, terminated either by death or censoring - time variable\nfstat: loss to followup = 0, death = 1 - censoring variable\nafb: atrial fibrillation, no = 0, 1 = yes - explanatory variable\ngender: males = 0, females = 1 - stratification factor\n\n\nlibname mylib \"..\\data\";\n\ndata dat;\nset mylib.whas500;\nlenfoly = round(lenfol/365.25, 0.01);  /* change follow-up days to years for better visualization*/\nrun;"
  },
  {
    "objectID": "SAS/survival.html#the-non-stratified-model",
    "href": "SAS/survival.html#the-non-stratified-model",
    "title": "Survival Analysis Using SAS",
    "section": "The Non-stratified Model",
    "text": "The Non-stratified Model\nFirst we try a non-stratified analysis following the mock-up above to describe the association between survival time and afb (atrial fibrillation).\nThe KM estimators and log-rank test are from PROC LIFETEST, and Cox PH model is conducted using PROC PHREG.\n\nKM estimators and log-rank test\n\nproc lifetest data=dat outsurv=_SurvEst timelist= 1 3 5 reduceout stderr; \ntime lenfoly*fstat(0);\nstrata afb;\nrun;\n\nThe landmark estimates and quartile estimates for AFB = 0 group are as shown in below:\n\n\n\n\n\n\n\n\n\nThe logrank test result is in below:\n\n\n\n\n\n\n\n\n\n\n\nCox PH model\n\nproc phreg data = dat;\nclass afb;\nmodel lenfol*fstat(0) = afb/rl;\nrun;\n\nThe hazard ratio and confidence intervals are shown as below:"
  },
  {
    "objectID": "SAS/survival.html#the-stratified-model",
    "href": "SAS/survival.html#the-stratified-model",
    "title": "Survival Analysis Using SAS",
    "section": "The Stratified Model",
    "text": "The Stratified Model\nIn a stratified model, the Kaplan-Meier estimators remain the same as those in the non-stratified model. To implement stratified log-rank tests and Cox proportional hazards models, simply add the STRATA option in both PROC LIFETEST and PROC PHREG.\n\n# KM estimators and log-rank test\nproc lifetest data=dat;\ntime lenfoly*fstat(0);\nstrata gender/group = afb;\nrun;\n\n# Cox PH model\nproc phreg data=dat;\nclass afb;\nmodel lenfol*fstat(0) = afb/rl;\nstrata gender;\nrun;"
  },
  {
    "objectID": "SAS/ttest_2Sample.html",
    "href": "SAS/ttest_2Sample.html",
    "title": "Independant Two-Sample t-test",
    "section": "",
    "text": "Data Used\nThe following data was used in this example.\ndata d1;\n  length trt_grp $ 9;\n  input trt_grp $ WtGain @@;\n  datalines;\nplacebo    94 placebo    12 placebo    26 placebo    89 \nplacebo    88 placebo    96 placebo    85 placebo   130 \nplacebo    75 placebo    54 placebo   112 placebo    69 \nplacebo   104 placebo    95 placebo    53 placebo    21 \ntreatment  45 treatment  62 treatment  96 treatment 128 \ntreatment 120 treatment  99 treatment  28 treatment  50 \ntreatment 109 treatment 115 treatment  39 treatment  96 \ntreatment  87 treatment 100 treatment  76 treatment  80 \n;\nrun;\n\n\n\nIndependent Two-Sample t-test in SAS\nThe null hypothesis of the Independent Samples t-test is, the means for the two populations are equal.\nIn SAS the following code was used to test the mean comparison (mean of Weight Gain) of two independent treatment groups (Treatment and Placebo).\nFor this example, we’re testing the significant difference in mean of Weight gain (WtGain) between treatment and placebo (trt_grp) using PROC TTEST procedure in SAS.\n\n  proc ttest data=d1; \n     class trt_grp; \n     var WtGain; \n  run; \n\nOutput:\n             Figure 1: Test results for independent t-test using PROC TTEST in SAS\n\n\n\n\n\n\n\n\n\nHere the t-value is –0.70, degrees of freedom is 30 and P value is 0.4912 which is greater than 0.05, so we accept the null hypothesis that there is no evidence of a significant difference between the means of treatment groups. The mean in placebo group is 75.1875 and mean in Treatment group is 83.1250. The mean difference the treatment groups (Treatment-Placebo) is –7.9375 and the 95% CI for the mean difference is [–31.1984, 15.3234]. The 95% confidence interval includes a treatment difference of 0, which supports the conclusion that the data fail to provide any evidence of a difference between the treatment groups.\nNote: Before entering straight into the t-test we need to check whether the assumptions (like the equality of variance, the observations should be independent, observations should be normally distributed) are met or not. If normality is not satisfied, we may consider using a suitable non-parametric test.\n\nNormality: You can check for data to be normally distributed by plotting a histogram of the data by treatment. Alternatively, you can use the Shapiro-Wilk test or the Kolmogorov-Smirnov test. If the test is &lt;0.05 and your sample is quite small then this suggests you should not use the t-test. However, if your sample in each treatment group is large (say &gt;30 in each group), then you do not need to rely so heavily on the assumption that the data have an underlying normal distribution in order to apply the two-sample t-test. This is where plotting the data using histograms can help to support investigation into the normality assumption. We have checked the normality of the observations using the code below. Here for both the treatment groups we have P value greater than 0.05 (Shapiro-Wilk test is used), therefore the normality assumption is there for our data.\n\n\n    proc univariate data=d1 normal;  \n      qqplot WtGain; \n      by trt_grp; \n    run; \n\nOutput:\n        Figure 2: The results of normality test for Treatment group\n\n\n\n\n\n\n\n\n\n       Figure 3: The results of normality test for Placebo group\n\n\n\n\n\n\n\n\n\n\nHomogeneity of variance (or Equality of variance): Homogeniety of variance will be tested by default in PROC TTEST itself by Folded F-test. In our case the P values is 0.6981 which is greater than 0.05. So we accept the null hypothesis of F-test, i.e. variances are same. Then we will consider the pooled method for t-test. If the F test is statistically significant (p&lt;0.05), then the pooled t-test may give erroneous results. In this instance, if it is believed that the population variances may truly differ, then the Satterthwaite (unequal variances) analysis results should be used. These are provided in the SAS output alongside the Pooled results as default.\n\nOutput:\n                    Figure 4: Folded F-test result in PROC TTEST"
  },
  {
    "objectID": "SAS/ttest_1Sample.html",
    "href": "SAS/ttest_1Sample.html",
    "title": "One Sample t-test",
    "section": "",
    "text": "In SAS, a one sample t-test is usually performed using PROC TTEST. The one sample t-test compares the mean of the sample to a provided null hypothesis, called “h0”. The h0 value is provided as an option. By default, the h0 value is zero (0). Running the procedure produces a set of results that suggest whether or not the null hypothesis should be rejected.\n\n\nThe following data was used in this example.\n  data read;\n     input score count @@;\n     datalines;\n  40 2   47 2   52 2   26 1   19 2\n  25 2   35 4   39 1   26 1   48 1\n  14 2   22 1   42 1   34 2   33 2\n  18 1   15 1   29 1   41 2   44 1\n  51 1   43 1   27 2   46 2   28 1\n  49 1   31 1   28 1   54 1   45 1\n  ;\n\n\n\nBy default, SAS PROC TTEST t-test assumes normality in the data and uses a classic Student’s t-test.\n\n\nThe following code was used to test the comparison of a reading scores against a baseline hypothesis value of 30:\n  proc ttest data=read h0=30;\n     var score;\n  run;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe SAS one sample t-test also supports lognormal analysis for a one sample t-test.\n\n\nUsing the same data as above, we will set the “DIST” option to “lognormal” to perform this analysis:\n  proc ttest data=read h0=30 dist=lognormal;\n     var score;\n  run;\nOutput:\n\n\n\n\n\n\n\n\n\nAs can be seen in the figure above, the lognormal variation of the one sample TTEST provides results for geometric mean, coefficient of variation, and 95% confidence limits for the coefficient of variation."
  },
  {
    "objectID": "SAS/ttest_1Sample.html#normal",
    "href": "SAS/ttest_1Sample.html#normal",
    "title": "One Sample t-test",
    "section": "",
    "text": "By default, SAS PROC TTEST t-test assumes normality in the data and uses a classic Student’s t-test.\n\n\nThe following code was used to test the comparison of a reading scores against a baseline hypothesis value of 30:\n  proc ttest data=read h0=30;\n     var score;\n  run;\nOutput:"
  },
  {
    "objectID": "SAS/ttest_1Sample.html#lognormal",
    "href": "SAS/ttest_1Sample.html#lognormal",
    "title": "One Sample t-test",
    "section": "",
    "text": "The SAS one sample t-test also supports lognormal analysis for a one sample t-test.\n\n\nUsing the same data as above, we will set the “DIST” option to “lognormal” to perform this analysis:\n  proc ttest data=read h0=30 dist=lognormal;\n     var score;\n  run;\nOutput:\n\n\n\n\n\n\n\n\n\nAs can be seen in the figure above, the lognormal variation of the one sample TTEST provides results for geometric mean, coefficient of variation, and 95% confidence limits for the coefficient of variation."
  },
  {
    "objectID": "SAS/ttest_Paired.html",
    "href": "SAS/ttest_Paired.html",
    "title": "Paired t-test",
    "section": "",
    "text": "The Paired t-test is used when two samples are naturally correlated. In the Paired t-test, the difference of the means between the two samples is compared to a given number that represents the null hypothesis. For a Paired t-test, the number of observations in each sample must be equal.\nIn SAS, a Paired t-test is typically performed using PROC TTEST.\n\n\nBy default, SAS PROC TTEST t-test assumes normality in the data and uses a classic Student’s t-test.\n\n\nThe following data was used in this example.\n  data pressure;\n     input SBPbefore SBPafter @@;\n     datalines;\n  120 128   124 131   130 131   118 127\n  140 132   128 125   140 141   135 137\n  126 118   130 132   126 129   127 135\n  ;\n\n\n\nThe following code was used to test the comparison of two paired samples of Systolic Blood Pressure before and after a procedure.\n  proc ttest data=pressure;\n     paired SBPbefore*SBPafter;\n  run;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe SAS paired t-test also supports analysis of lognormal data. Here is the data used for the lognormal analysis.\n\n\n  data auc;\n     input TestAUC RefAUC @@;\n     datalines;\n  103.4 90.11  59.92 77.71  68.17 77.71  94.54 97.51\n  69.48 58.21  72.17 101.3  74.37 79.84  84.44 96.06\n  96.74 89.30  94.26 97.22  48.52 61.62  95.68 85.80\n  ;\n\n\n\nFor cases when the data is lognormal, SAS offers the “DIST” option to chose between a normal and lognormal distribution. The procedure also offers the TOST option to specify the equivalence bounds.\n  proc ttest data=auc dist=lognormal tost(0.8, 1.25);\n     paired TestAUC*RefAUC;\n  run;\nOutput:\n\n\n\n\n\n\n\n\n\nAs can be seen in the figure above, the lognormal variation of the TTEST procedure offers additional results for geometric mean, coefficient of variation, and TOST equivalence analysis. The output also includes multiple p-values."
  },
  {
    "objectID": "SAS/ttest_Paired.html#normal",
    "href": "SAS/ttest_Paired.html#normal",
    "title": "Paired t-test",
    "section": "",
    "text": "By default, SAS PROC TTEST t-test assumes normality in the data and uses a classic Student’s t-test.\n\n\nThe following data was used in this example.\n  data pressure;\n     input SBPbefore SBPafter @@;\n     datalines;\n  120 128   124 131   130 131   118 127\n  140 132   128 125   140 141   135 137\n  126 118   130 132   126 129   127 135\n  ;\n\n\n\nThe following code was used to test the comparison of two paired samples of Systolic Blood Pressure before and after a procedure.\n  proc ttest data=pressure;\n     paired SBPbefore*SBPafter;\n  run;\nOutput:"
  },
  {
    "objectID": "SAS/ttest_Paired.html#lognormal",
    "href": "SAS/ttest_Paired.html#lognormal",
    "title": "Paired t-test",
    "section": "",
    "text": "The SAS paired t-test also supports analysis of lognormal data. Here is the data used for the lognormal analysis.\n\n\n  data auc;\n     input TestAUC RefAUC @@;\n     datalines;\n  103.4 90.11  59.92 77.71  68.17 77.71  94.54 97.51\n  69.48 58.21  72.17 101.3  74.37 79.84  84.44 96.06\n  96.74 89.30  94.26 97.22  48.52 61.62  95.68 85.80\n  ;\n\n\n\nFor cases when the data is lognormal, SAS offers the “DIST” option to chose between a normal and lognormal distribution. The procedure also offers the TOST option to specify the equivalence bounds.\n  proc ttest data=auc dist=lognormal tost(0.8, 1.25);\n     paired TestAUC*RefAUC;\n  run;\nOutput:\n\n\n\n\n\n\n\n\n\nAs can be seen in the figure above, the lognormal variation of the TTEST procedure offers additional results for geometric mean, coefficient of variation, and TOST equivalence analysis. The output also includes multiple p-values."
  },
  {
    "objectID": "SAS/summary-stats.html",
    "href": "SAS/summary-stats.html",
    "title": "Deriving Quantiles or Percentiles in SAS",
    "section": "",
    "text": "Percentiles can be calculated in SAS using the UNIVARIATE procedure. The procedure has the option PCTLDEF which allows for five different percentile definitions to be used. The default is PCTLDEF=5, which uses the empirical distribution function to find percentiles.\nThis is how the 25th and 40th percentiles of aval in the dataset adlb could be calculated, using the default option for PCTLDEF.\n\nproc univariate data=adlb;\n  var aval;\n  output out=stats pctlpts=25 40 pctlpre=p;\nrun;\n\nThe pctlpre=p option tells SAS the prefix to use in the output dataset for the percentile results. In the above example, SAS will create a dataset called stats, containing variables p25 and p40."
  },
  {
    "objectID": "blogs/posts/202312 highlights blog.html",
    "href": "blogs/posts/202312 highlights blog.html",
    "title": "2023: A Year of Progress for the PHUSE CAMIS Working Group Project",
    "section": "",
    "text": "As we draw towards the end of 2023, the PHUSE DVOST CAMIS Working Group Project reflect on their key progress and successes this year.\nThe CAMIS repository went live in January 2023, drawing on the content from the CSRMLW Working Group. This searchable repository compares analysis method implementations in software (CAMIS) such as SAS, R and python.\nThe white paper, Key Considerations When Understanding Differences in Statistical Methodology Implementations Across Programming Languages – An Introduction to the CAMIS Project was published in June, which highlighted the importance of clearly specifying your analysis, such that it can be replicated in different software and doesn’t rely on default options, which can be different.\nFor more complex analyses, it can still be hard to understand what defaults and algorithms your software is using, so the team focused 2023 on expanding our github repo content, comparing SAS vs R methods. By August, we had covered the following topics in the repo: quartiles, rounding, ANOVA, MMRM, the CMH test, log-rank, Cox PH, the McNemar test, the Kruskal-Wallis test and logistic. October saw the launch of the CAMIS-Oncology sub-group, led by Somasekhar Sriadibhatla (AstraZeneca). This team will focus specifically on oncology endpoints and analysing them in SAS, R and Python.\nThe CAMIS team have expanded in membership this year and presented at conferences around the world. In November, we welcomed Harshal Khanolkar (Novo Nordisk) to join the leadership team alongside Christina Fillmore (GSK) and Lyn Taylor (Parexel). Our focus for 2024 will be on creating additional content for the repo and sharing awareness of the project across the medical research and wider community.\nWe would like to take this opportunity to thank all of our team members and contributors, and encourage everyone to check out the repository and help us grow our content. If you would like to join the team, please get in touch through the github repo."
  },
  {
    "objectID": "Comp/r-sas_kruskalwallis.html",
    "href": "Comp/r-sas_kruskalwallis.html",
    "title": "Kruskal Wallis R v SAS",
    "section": "",
    "text": "From the individual R and SAS pages, performing the Kruskal-Wallis test in R using:\n\nkruskal.test(Sepal_Width~Species, data=iris_sub)\n\nand in SAS using:\n\nproc npar1way data=iris_sub wilcoxon;\nclass Species;\nvar Sepal_Width;\nexact;\nrun;\n\nproduced the same results for the test statistic and asymptotic p-value.\nThere is a difference between languages in that SAS provides the EXACT option to easily output the exact p-value, where R does not seem to have an equivalent. A Monte Carlo permutation test may offer an alternative to the exact test on R. The coin package could help in implementing this."
  },
  {
    "objectID": "Comp/r-sas_kruskalwallis.html#kruskal-wallis-r-and-sas",
    "href": "Comp/r-sas_kruskalwallis.html#kruskal-wallis-r-and-sas",
    "title": "Kruskal Wallis R v SAS",
    "section": "",
    "text": "From the individual R and SAS pages, performing the Kruskal-Wallis test in R using:\n\nkruskal.test(Sepal_Width~Species, data=iris_sub)\n\nand in SAS using:\n\nproc npar1way data=iris_sub wilcoxon;\nclass Species;\nvar Sepal_Width;\nexact;\nrun;\n\nproduced the same results for the test statistic and asymptotic p-value.\nThere is a difference between languages in that SAS provides the EXACT option to easily output the exact p-value, where R does not seem to have an equivalent. A Monte Carlo permutation test may offer an alternative to the exact test on R. The coin package could help in implementing this."
  },
  {
    "objectID": "Comp/r-sas_summary_skew_kurt.html",
    "href": "Comp/r-sas_summary_skew_kurt.html",
    "title": "R vs SAS Skewness/Kurtosis",
    "section": "",
    "text": "The following table shows the types of Skewness, the capabilities of each language, and whether or not the results from each language match.\n\n\n\nAnalysis\nSupported in R\nSupported in SAS\nResults Match\nNotes\n\n\n\n\nSkewness, Type 1\nYes\nYes\nYes\nIn e1071, use type = 1.In SAS use vardef = N.\n\n\nSkewness, Type 2\nYes\nYes\nYes\nIn e1071, use type = 2.In SAS use vardef = DF.procs and sasLM use defaults.\n\n\nSkewness, Type 3\nYes\nNo\nNA\nIn e1071, use type = 3.Not supported in SAS.\n\n\n\nNote that the SAS default is Type 2."
  },
  {
    "objectID": "Comp/r-sas_summary_skew_kurt.html#comparison-results",
    "href": "Comp/r-sas_summary_skew_kurt.html#comparison-results",
    "title": "R vs SAS Skewness/Kurtosis",
    "section": "Comparison Results",
    "text": "Comparison Results\n\nSkewness\nHere is a table of Skewness comparison values between the four R packages examined and SAS:\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistic\ne1071\nmoments\nprocs\nsasLM\nSAS\nMatch\nNotes\n\n\n\n\nSkewness, Type 1\n0.9054442\n0.9054442\nNA\nNA\n0.9054442\nYes\n\n\n\nSkewness, Type 2\n1.009318\nNA\n1.009318\n1.009318\n1.0093179\nYes\n\n\n\nSkewness, Type 3\n0.8164261\nNA\nNA\nNA\nNA\nNA\nType 3 not supported in SAS\n\n\n\n\n\nKurtosis\nHere is a table of Kurtosis comparison values between the four R packages examined and SAS:\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistic\ne1071\nmoments\nprocs\nsasLM\nSAS\nMatch\nNotes\n\n\n\n\nKurtosis, Type 1\n-0.5833411\nNA\nNA\nNA\n-0.5833411\nYes\n\n\n\nKurtosis, Type 2\n-0.2991564\nNA\n-0.2991564\n-0.2991564\n-0.2991564\nYes\n\n\n\nKurtosis, Type 3\n-0.8948216\nNA\nNA\nNA\nNA\nNA\nType 3 not supported in SAS\n\n\nKurtosis, Pearson’s\nNA\n2.416659\nNA\nNA\nNA\nNA\nPearson’s not supported in SAS"
  },
  {
    "objectID": "Comp/r-sas_ttest_2Sample.html",
    "href": "Comp/r-sas_ttest_2Sample.html",
    "title": "R vs SAS Two Sample T-Test",
    "section": "",
    "text": "The following table shows the types of Two Sample t-test analysis, the capabilities of each language, and whether or not the results from each language match.\n\n\n\nAnalysis\nSupported in R\nSupported in SAS\nResults Match\nNotes\n\n\n\n\nTwo sample Student’s t-test\nYes\nYes\nYes\nIn Base R, use t.test() function with paired = FALSE and var.equal = TRUE\n\n\nTwo sample Welch’s t-test\nYes\nYes\nYes\nIn Base R, use t.test() function with paired = FALSE and var.equal = FALSE\n\n\n\n\n\n\n\nHere is a table of comparison values between t.test(), proc_ttest(), and SAS PROC TTEST:\n\n\n\n\n\n\n\n\n\n\n\nStatistic\nt.test()\nproc_ttest()\nPROC TTEST\nMatch\nNotes\n\n\n\n\nDegrees of Freedom\n30\n30\n30\nYes\n\n\n\nt value\n-0.6969002\n-0.6969002\n-0.6969002\nYes\n\n\n\np value\n0.4912306\n0.4912306\n0.4912306\nYes\n\n\n\n\n\n\n\nHere is a table of comparison values between t.test(), proc_ttest(), and SAS PROC TTEST:\n\n\n\n\n\n\n\n\n\n\n\nStatistic\nt.test()\nproc_ttest()\nPROC TTEST\nMatch\nNotes\n\n\n\n\nDegrees of Freedom\n29.69359\n29.69359\n29.69359\nYes\n\n\n\nt value\n-0.6969002\n-0.6969002\n-0.6969002\nYes\n\n\n\np value\n0.4912856\n0.4912856\n0.4912856\nYes"
  },
  {
    "objectID": "Comp/r-sas_ttest_2Sample.html#comparison-results",
    "href": "Comp/r-sas_ttest_2Sample.html#comparison-results",
    "title": "R vs SAS Two Sample T-Test",
    "section": "",
    "text": "Here is a table of comparison values between t.test(), proc_ttest(), and SAS PROC TTEST:\n\n\n\n\n\n\n\n\n\n\n\nStatistic\nt.test()\nproc_ttest()\nPROC TTEST\nMatch\nNotes\n\n\n\n\nDegrees of Freedom\n30\n30\n30\nYes\n\n\n\nt value\n-0.6969002\n-0.6969002\n-0.6969002\nYes\n\n\n\np value\n0.4912306\n0.4912306\n0.4912306\nYes\n\n\n\n\n\n\n\nHere is a table of comparison values between t.test(), proc_ttest(), and SAS PROC TTEST:\n\n\n\n\n\n\n\n\n\n\n\nStatistic\nt.test()\nproc_ttest()\nPROC TTEST\nMatch\nNotes\n\n\n\n\nDegrees of Freedom\n29.69359\n29.69359\n29.69359\nYes\n\n\n\nt value\n-0.6969002\n-0.6969002\n-0.6969002\nYes\n\n\n\np value\n0.4912856\n0.4912856\n0.4912856\nYes"
  },
  {
    "objectID": "Comp/r-sas_linear-regression.html",
    "href": "Comp/r-sas_linear-regression.html",
    "title": "R vs SAS Linear Regression",
    "section": "",
    "text": "Summary of R vs SAS Comparison for Linear Regression\nTo date the lm function in R and proc reg in sas have been found to 100% agree.\nSee R and SAS for more detail."
  },
  {
    "objectID": "Comp/r-sas_ttest_Paired.html",
    "href": "Comp/r-sas_ttest_Paired.html",
    "title": "R vs SAS Paired T-Test",
    "section": "",
    "text": "The following table shows the types of Paired t-test analysis, the capabilities of each language, and whether or not the results from each language match.\n\n\n\nAnalysis\nSupported in R\nSupported in SAS\nResults Match\nNotes\n\n\n\n\nPaired t-test, normal data\nYes\nYes\nYes\nIn Base R, use paired = TRUE on t.test() function\n\n\nPaired t-test, lognormal data\nMaybe\nYes\nNA\nMay be supported by envstats package\n\n\n\n\n\n\n\nHere is a table of comparison values between t.test(), proc_ttest(), and SAS PROC TTEST:\n\n\n\n\n\n\n\n\n\n\n\nStatistic\nt.test()\nproc_ttest()\nPROC TTEST\nMatch\nNotes\n\n\n\n\nDegrees of Freedom\n11\n11\n11\nYes\n\n\n\nt value\n-1.089648\n-1.089648\n-1.089648\nYes\n\n\n\np value\n0.2992\n0.2992\n0.2992\nYes\n\n\n\n\n\n\n\nSince there is currently no known support for lognormal t-test in R, this comparison is not applicable."
  },
  {
    "objectID": "Comp/r-sas_ttest_Paired.html#comparison-results",
    "href": "Comp/r-sas_ttest_Paired.html#comparison-results",
    "title": "R vs SAS Paired T-Test",
    "section": "",
    "text": "Here is a table of comparison values between t.test(), proc_ttest(), and SAS PROC TTEST:\n\n\n\n\n\n\n\n\n\n\n\nStatistic\nt.test()\nproc_ttest()\nPROC TTEST\nMatch\nNotes\n\n\n\n\nDegrees of Freedom\n11\n11\n11\nYes\n\n\n\nt value\n-1.089648\n-1.089648\n-1.089648\nYes\n\n\n\np value\n0.2992\n0.2992\n0.2992\nYes\n\n\n\n\n\n\n\nSince there is currently no known support for lognormal t-test in R, this comparison is not applicable."
  },
  {
    "objectID": "Comp/r-sas_mmrm.html",
    "href": "Comp/r-sas_mmrm.html",
    "title": "R vs SAS MMRM",
    "section": "",
    "text": "In this vignette we briefly compare the mmrm::mmrm, SAS’s PROC GLIMMIX, nlme::gls, lme4::lmer, and glmmTMB::glmmTMB functions for fitting mixed models for repeated measures (MMRMs). A primary difference in these implementations lies in the covariance structures that are supported “out of the box”. In particular, PROC GLIMMIX and mmrm are the only procedures which provide support for many of the most common MMRM covariance structures. Most covariance structures can be implemented in gls, though users are required to define them manually. lmer and glmmTMB are more limited. We find that mmmrm converges more quickly than other R implementations while also producing estimates that are virtually identical to PROC GLIMMIX’s."
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#fev-data",
    "href": "Comp/r-sas_mmrm.html#fev-data",
    "title": "R vs SAS MMRM",
    "section": "FEV Data",
    "text": "FEV Data\nThe FEV dataset contains measurements of FEV1 (forced expired volume in one second), a measure of how quickly the lungs can be emptied. Low levels of FEV1 may indicate chronic obstructive pulmonary disease (COPD). It is summarized below.\n                                      Stratified by ARMCD\n                               Overall       PBO           TRT\n  n                              800           420           380\n  USUBJID (%)\n     PT[1-200]                   200           105 (52.5)     95 (47.5)\n  AVISIT\n     VIS1                        200           105            95\n     VIS2                        200           105            95\n     VIS3                        200           105            95\n     VIS4                        200           105            95\n  RACE (%)\n     Asian                       280 (35.0)    152 (36.2)    128 (33.7)\n     Black or African American   300 (37.5)    184 (43.8)    116 (30.5)\n     White                       220 (27.5)     84 (20.0)    136 (35.8)\n  SEX = Female (%)               424 (53.0)    220 (52.4)    204 (53.7)\n  FEV1_BL (mean (SD))          40.19 (9.12)  40.46 (8.84)  39.90 (9.42)\n  FEV1 (mean (SD))             42.30 (9.32)  40.24 (8.67)  44.45 (9.51)\n  WEIGHT (mean (SD))            0.52 (0.23)   0.52 (0.23)   0.51 (0.23)\n  VISITN (mean (SD))            2.50 (1.12)   2.50 (1.12)   2.50 (1.12)\n  VISITN2 (mean (SD))          -0.02 (1.03)   0.01 (1.07)  -0.04 (0.98)"
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#bcva-data",
    "href": "Comp/r-sas_mmrm.html#bcva-data",
    "title": "R vs SAS MMRM",
    "section": "BCVA Data",
    "text": "BCVA Data\nThe BCVA dataset contains data from a randomized longitudinal ophthalmology trial evaluating the change in baseline corrected visual acuity (BCVA) over the course of 10 visits. BCVA corresponds to the number of letters read from a visual acuity chart. A summary of the data is given below:\n                                      Stratified by ARMCD\n                               Overall         CTL            TRT\n  n                             8605          4123           4482\n  USUBJID (%)\n     PT[1-1000]                 1000           494 (49.4)     506 (50.6)\n  AVISIT\n     VIS1                        983           482            501\n     VIS2                        980           481            499\n     VIS3                        960           471            489\n     VIS4                        946           458            488\n     VIS5                        925           454            471\n     VIS6                        868           410            458\n     VIS7                        816           388            428\n     VIS8                        791           371            420\n     VIS9                        719           327            392\n     VIS10                       617           281            336\n  RACE (%)\n     Asian                       297 (29.7)    151 (30.6)     146 (28.9)\n     Black or African American   317 (31.7)    149 (30.1)     168 (33.2)\n     White                       386 (38.6)    194 (39.3)     192 (37.9)\n  BCVA_BL (mean (SD))          75.12 (9.93)  74.90 (9.76)   75.40 (10.1)\n  BCVA_CHG (mean (SD))\n     VIS1                       5.59 (1.31)   5.32 (1.23)    5.86 (1.33)\n     VIS10                      9.18 (2.91)   7.49 (2.58)   10.60 (2.36)"
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#ante-dependence-heterogeneous",
    "href": "Comp/r-sas_mmrm.html#ante-dependence-heterogeneous",
    "title": "R vs SAS MMRM",
    "section": "Ante-dependence (heterogeneous)",
    "text": "Ante-dependence (heterogeneous)\n\nPROC GLIMMIX\nPROC GLIMMIX DATA = fev_data;\nCLASS AVISIT(ref = 'VIS1') ARMCD(ref = 'PBO') USUBJID;\nMODEL FEV1 = AVISIT|ARMCD / ddfm=satterthwaite solution chisq;\nRANDOM AVISIT / subject=USUBJID type=ANTE(1);\n\n\n\nmmrm\nmmrm(\n  formula = FEV1 ~ ARMCD * AVISIT + adh(VISITN | USUBJID),\n  data = fev_data\n)"
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#ante-dependence-homogeneous",
    "href": "Comp/r-sas_mmrm.html#ante-dependence-homogeneous",
    "title": "R vs SAS MMRM",
    "section": "Ante-dependence (homogeneous)",
    "text": "Ante-dependence (homogeneous)\n\nmmrm\nmmrm(\n  formula =FEV1 ~ ARMCD * AVISIT + ad(VISITN | USUBJID),\n  data = fev_data\n)"
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#auto-regressive-heterogeneous",
    "href": "Comp/r-sas_mmrm.html#auto-regressive-heterogeneous",
    "title": "R vs SAS MMRM",
    "section": "Auto-regressive (heterogeneous)",
    "text": "Auto-regressive (heterogeneous)\n\nPROC GLIMMIX\nPROC GLIMMIX DATA = fev_data;\nCLASS AVISIT(ref = 'VIS1') ARMCD(ref = 'PBO') USUBJID;\nMODEL FEV1 = AVISIT|ARMCD / ddfm=satterthwaite solution chisq;\nRANDOM AVISIT / subject=USUBJID type=ARH(1);\n\n\n\nmmrm\nmmrm(\n  formula = FEV1 ~ ARMCD * AVISIT + ar1h(VISITN | USUBJID),\n  data = fev_data\n)\n\n\n\ngls\ngls(\n  formula = FEV1 ~ ARMCD * AVISIT,\n  data = fev_data,\n  correlation = corCAR1(form = ~AVISIT | USUBJID),\n  weights = varIdent(form = ~1|AVISIT),\n  na.action = na.omit\n)"
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#auto-regressive-homogeneous",
    "href": "Comp/r-sas_mmrm.html#auto-regressive-homogeneous",
    "title": "R vs SAS MMRM",
    "section": "Auto-regressive (homogeneous)",
    "text": "Auto-regressive (homogeneous)\n\nPROC GLIMMIX\nPROC GLIMMIX DATA = fev_data;\nCLASS AVISIT(ref = 'VIS1') ARMCD(ref = 'PBO') USUBJID;\nMODEL FEV1 =  ARMCD|AVISIT / ddfm=satterthwaite solution chisq;\nRANDOM AVISIT / subject=USUBJID type=AR(1);\n\n\n\nmmrm\nmmrm(\n  formula = FEV1 ~ ARMCD * AVISIT + ar1(VISITN | USUBJID),\n  data = fev_data\n)\n\n\n\ngls\ngls(\n  formula = FEV1 ~ ARMCD * AVISIT,\n  data = fev_data,\n  correlation = corCAR1(form = ~AVISIT | USUBJID),\n  na.action = na.omit\n)\n\n\n\nglmmTMB\nglmmTMB(\n  FEV1 ~ ARMCD * AVISIT + ar1(0 + AVISIT | USUBJID),\n  dispformula = ~ 0,\n  data = fev_data\n)"
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#compound-symmetry-heterogeneous",
    "href": "Comp/r-sas_mmrm.html#compound-symmetry-heterogeneous",
    "title": "R vs SAS MMRM",
    "section": "Compound symmetry (heterogeneous)",
    "text": "Compound symmetry (heterogeneous)\n\nPROC GLIMMIX\nPROC GLIMMIX DATA = fev_data;\nCLASS AVISIT(ref = 'VIS1') ARMCD(ref = 'PBO') USUBJID;\nMODEL FEV1 = AVISIT|ARMCD / ddfm=satterthwaite solution chisq;\nRANDOM AVISIT / subject=USUBJID type=CSH;\n\n\n\nmmrm\nmmrm(\n  formula = FEV1 ~ ARMCD * AVISIT + csh(VISITN | USUBJID),\n  data = fev_data\n)\n\n\n\ngls\ngls(\n  formula = FEV1 ~ ARMCD * AVISIT,\n  data = fev_data,\n  correlation = corCompSymm(form = ~AVISIT | USUBJID),\n  weights = varIdent(form = ~1|AVISIT),\n  na.action = na.omit\n)\n\n\n\nglmmTMB\nglmmTMB(\n  FEV1 ~ ARMCD * AVISIT + cs(0 + AVISIT | USUBJID),\n  dispformula = ~ 0,\n  data = fev_data\n)"
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#compound-symmetry-homogeneous",
    "href": "Comp/r-sas_mmrm.html#compound-symmetry-homogeneous",
    "title": "R vs SAS MMRM",
    "section": "Compound symmetry (homogeneous)",
    "text": "Compound symmetry (homogeneous)\n\nPROC GLIMMIX\nPROC GLIMMIX DATA = fev_data;\nCLASS AVISIT(ref = 'VIS1') ARMCD(ref = 'PBO') USUBJID;\nMODEL FEV1 = AVISIT|ARMCD / ddfm=satterthwaite solution chisq;\nRANDOM AVISIT / subject=USUBJID type=CS;\n\n\n\nmmrm\nmmrm(\n  formula = FEV1 ~ ARMCD * AVISIT + cs(VISITN | USUBJID),\n  data = fev_data\n)\n\n\n\ngls\ngls(\n  formula = FEV1 ~ ARMCD * AVISIT,\n  data = fev_data,\n  correlation = corCompSymm(form = ~AVISIT | USUBJID),\n  na.action = na.omit\n)"
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#spatial-exponential",
    "href": "Comp/r-sas_mmrm.html#spatial-exponential",
    "title": "R vs SAS MMRM",
    "section": "Spatial exponential",
    "text": "Spatial exponential\n\nPROC GLIMMIX\nPROC GLIMMIX DATA = fev_data;\nCLASS AVISIT(ref = 'VIS1') ARMCD(ref = 'PBO') USUBJID;\nMODEL FEV1 = AVISIT|ARMCD / ddfm=satterthwaite solution chisq;\nRANDOM / subject=USUBJID type=sp(exp)(visitn) rcorr;\n\n\n\nmmrm\nmmrm(\n  formula = FEV1 ~ ARMCD * AVISIT + sp_exp(VISITN | USUBJID),\n  data = fev_data\n)\n\n\n\ngls\ngls(\n  formula = FEV1 ~ ARMCD * AVISIT,\n  data = fev_data,\n  correlation = corExp(form = ~AVISIT | USUBJID),\n  weights = varIdent(form = ~1|AVISIT),\n  na.action = na.omit\n)\n\n\n\nglmmTMB\n# NOTE: requires use of coordinates\nglmmTMB(\n  FEV1 ~ ARMCD * AVISIT + exp(0 + AVISIT | USUBJID),\n  dispformula = ~ 0,\n  data = fev_data\n)"
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#toeplitz-heterogeneous",
    "href": "Comp/r-sas_mmrm.html#toeplitz-heterogeneous",
    "title": "R vs SAS MMRM",
    "section": "Toeplitz (heterogeneous)",
    "text": "Toeplitz (heterogeneous)\n\nPROC GLIMMIX\nPROC GLIMMIX DATA = fev_data;\nCLASS AVISIT(ref = 'VIS1') ARMCD(ref = 'PBO') USUBJID;\nMODEL FEV1 = AVISIT|ARMCD / ddfm=satterthwaite solution chisq;\nRANDOM AVISIT / subject=USUBJID type=TOEPH;\n\n\n\nmmrm\nmmrm(\n  formula = FEV1 ~ ARMCD * AVISIT + toeph(AVISIT | USUBJID),\n  data = fev_data\n)\n\n\n\nglmmTMB\n glmmTMB(\n  FEV1 ~ ARMCD * AVISIT + toep(0 + AVISIT | USUBJID),\n  dispformula = ~ 0,\n  data = fev_data\n)"
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#toeplitz-homogeneous",
    "href": "Comp/r-sas_mmrm.html#toeplitz-homogeneous",
    "title": "R vs SAS MMRM",
    "section": "Toeplitz (homogeneous)",
    "text": "Toeplitz (homogeneous)\n\nPROC GLIMMIX\nPROC GLIMMIX DATA = fev_data;\nCLASS AVISIT(ref = 'VIS1') ARMCD(ref = 'PBO') USUBJID;\nMODEL FEV1 = AVISIT|ARMCD / ddfm=satterthwaite solution chisq;\nRANDOM AVISIT / subject=USUBJID type=TOEP;\n\n\n\nmmrm\nmmrm(\n  formula = FEV1 ~ ARMCD * AVISIT + toep(AVISIT | USUBJID),\n  data = fev_data\n)"
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#unstructured",
    "href": "Comp/r-sas_mmrm.html#unstructured",
    "title": "R vs SAS MMRM",
    "section": "Unstructured",
    "text": "Unstructured\n\nPROC GLIMMIX\nPROC GLIMMIX DATA = fev_data;\nCLASS AVISIT(ref = 'VIS1') ARMCD(ref = 'PBO') USUBJID;\nMODEL FEV1 = ARMCD|AVISIT / ddfm=satterthwaite solution chisq;\nRANDOM AVISIT / subject=USUBJID type=un;\n\n\n\nmmrm\nmmrm(\n  formula = FEV1 ~ ARMCD * AVISIT + us(AVISIT | USUBJID),\n  data = fev_data\n)\n\n\n\ngls\ngls(\n  formula = FEV1 ~  ARMCD * AVISIT,\n  data = fev_data,\n  correlation = corSymm(form = ~AVISIT | USUBJID),\n  weights = varIdent(form = ~1|AVISIT),\n  na.action = na.omit\n)\n\n\n\nlmer\nlmer(\n  FEV1 ~ ARMCD * AVISIT + (0 + AVISIT | USUBJID),\n  data = fev_data,\n  control = lmerControl(check.nobs.vs.nRE = \"ignore\"),\n  na.action = na.omit\n)\n\n\n\nglmmTMB\nglmmTMB(\n  FEV1 ~ ARMCD * AVISIT + us(0 + AVISIT | USUBJID),\n  dispformula = ~ 0,\n  data = fev_data\n)"
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#convergence-times",
    "href": "Comp/r-sas_mmrm.html#convergence-times",
    "title": "R vs SAS MMRM",
    "section": "Convergence Times",
    "text": "Convergence Times\n\nFEV Data\nThe mmrm, PROC GLIMMIX, gls, lmer, and glmmTMB functions are applied to the FEV dataset 10 times. The convergence times are recorded for each replicate and are reported in the table below.\n\n\n\nComparison of convergence times: milliseconds\n\n\nImplementation\nMedian\nFirst Quartile\nThird Quartile\n\n\n\n\nmmrm\n56.15\n55.76\n56.30\n\n\nPROC GLIMMIX\n100.00\n100.00\n100.00\n\n\nlmer\n247.02\n245.25\n257.46\n\n\ngls\n687.63\n683.50\n692.45\n\n\nglmmTMB\n715.90\n708.70\n721.57\n\n\n\n\n\nIt is clear from these results that mmrm converges significantly faster than other R functions. Though not demonstrated here, this is generally true regardless of the sample size and covariance structure used. mmrm is faster than PROC GLIMMIX.\n\n\nBCVA Data\nThe MMRM implementations are now applied to the BCVA dataset 10 times. The convergence times are presented below.\n\n\n\nComparison of convergence times: seconds\n\n\nImplementation\nMedian\nFirst Quartile\nThird Quartile\n\n\n\n\nmmrm\n3.36\n3.32\n3.46\n\n\nglmmTMB\n18.65\n18.14\n18.87\n\n\nPROC GLIMMIX\n36.25\n36.17\n36.29\n\n\ngls\n164.36\n158.61\n165.93\n\n\nlmer\n165.26\n157.46\n166.42\n\n\n\n\n\nWe again find that mmrm produces the fastest convergence times on average."
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#marginal-treatment-effect-estimates-comparison",
    "href": "Comp/r-sas_mmrm.html#marginal-treatment-effect-estimates-comparison",
    "title": "R vs SAS MMRM",
    "section": "Marginal Treatment Effect Estimates Comparison",
    "text": "Marginal Treatment Effect Estimates Comparison\nWe next estimate the marginal mean treatment effects for each visit in the FEV and BCVA datasets using the MMRM fitting procedures. All R implementations’ estimates are reported relative to PROC GLIMMIX’s estimates. Convergence status is also reported.\n\nFEV Data\n\n\n\n\n\n\n\n\n\nThe R procedures’ estimates are very similar to those output by PROC GLIMMIX, though mmrm and gls generate the estimates that are closest to those produced when using SAS. All methods converge using their default optimization arguments.\n\n\nBCVA Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmmrm, gls and lmer produce estimates that are virtually identical to PROC GLIMMIX’s, while glmmTMB does not. This is likely explained by glmmTMB’s failure to converge. Note too that lmer fails to converge."
  },
  {
    "objectID": "Comp/r-sas_mmrm.html#impact-of-missing-data-on-convergence-rates",
    "href": "Comp/r-sas_mmrm.html#impact-of-missing-data-on-convergence-rates",
    "title": "R vs SAS MMRM",
    "section": "Impact of Missing Data on Convergence Rates",
    "text": "Impact of Missing Data on Convergence Rates\nThe results of the previous benchmark suggest that the amount of patients missing from later time points affect certain implementations’ capacity to converge. We investigate this further by simulating data using a data-generating process similar to that of the BCVA datasets, though with various rates of patient dropout.\nTen datasets of 200 patients are generated each of the following levels of missingness: none, mild, moderate, and high. In all scenarios, observations are missing at random. The number patients observed at each visit is obtained for one replicated dataset at each level of missingness is presented in the table below.\n\n\n\nNumber of patients per visit\n\n\n\nnone\nmild\nmoderate\nhigh\n\n\n\n\nVIS01\n200\n196.7\n197.6\n188.1\n\n\nVIS02\n200\n195.4\n194.4\n182.4\n\n\nVIS03\n200\n195.1\n190.7\n175.2\n\n\nVIS04\n200\n194.1\n188.4\n162.8\n\n\nVIS05\n200\n191.6\n182.5\n142.7\n\n\nVIS06\n200\n188.2\n177.3\n125.4\n\n\nVIS07\n200\n184.6\n168.0\n105.9\n\n\nVIS08\n200\n178.5\n155.4\n82.6\n\n\nVIS09\n200\n175.3\n139.9\n58.1\n\n\nVIS10\n200\n164.1\n124.0\n39.5\n\n\n\n\n\nThe convergence rates of all implementations for stratified by missingness level is presented in the plot below.\n\n\n\n\n\n\n\n\n\nmmrm, gls, and PROC GLIMMIX are resilient to missingness, only exhibiting some convergence problems in the scenarios with the most missingness. These implementations converged in all the other scenarios’ replicates. glmmTMB, on the other hand, has convergence issues in the no-, mild-, and high-missingness datasets, with the worst convergence rate occurring in the datasets with the most dropout. Finally, lmer is unreliable in all scenarios, suggesting that it’s convergence issues stem from something other than the missing observations.\nNote that the default optimization schemes are used for each method; these schemes can be modified to potentially improve convergence rates.\nA more comprehensive simulation study using data-generating processes similar to the one used here is outlined in the simulations/missing-data-benchmarks subdirectory. In addition to assessing the effect of missing data on software convergence rates, we also evaluate these methods’ fit times and empirical bias, variance, 95% coverage rates, type I error rates and type II error rates. mmrm is found to be the most most robust software for fitting MMRMs in scenarios where a large proportion of patients are missing from the last time points. Additionally, mmrm has the fastest average fit times regardless of the amount of missingness. All implementations considered produce similar empirical biases, variances, 95% coverage rates, type I error rates and type II error rates."
  },
  {
    "objectID": "Comp/r-sas_mcnemar.html",
    "href": "Comp/r-sas_mcnemar.html",
    "title": "R v SAS McNemar’s test",
    "section": "",
    "text": "McNemar’s test; R and SAS\nIn R, the mcNemar function from the epibasix package can be used to perform McNemar’s test.\n\nX&lt;-table(colds$age12,colds$age14)\nsummary(mcNemar(X))\n\nThe FREQ procedure can be used in SAS with the AGREE option to run the McNemar test, with OR, and RISKDIFF options stated for production of odds ratios and risk difference. These options were added as epibasix::mcNemar outputs the odds ratio and risk difference with confidence limits as default. In contrast to R, SAS outputs the Kappa coefficients with confident limits as default.\n\nproc freq data=colds;\n    tables age12*age14 / agree or riskdiff;\nrun;\n\nWhen calculating the odds ratio and risk difference confidence limits, SAS is not treating the data as matched-pairs. There is advice on the SAS blog and SAS support page to amend this, which requires a lot of additional coding.\nR is using Edward’s continuity correction with no option to remove this. In contrast, there is no option to include Edward’s continuity correction in SAS, but this can be manually coded to agree with R. However, its use is controversial due to being seen as overly conservative.\nR’s use of the continuity correction is consistent with other functions within the epibasix package, which was categorised as ‘High Risk’ by the Risk Assessment Shiny App created by the R Validation Hub. Risk is quantified by the app through a number of metrics relating to maintenance and community usage. It was found that the author is no longer maintaining the package and there was no documentation available for certain methods used. Therefore, the use of the epibasix package is advised against and other packages may be more suitable.\nThe mcnemar.test function in the stats package provides the option to remove continuity corrections which results in a match with SAS. This function does not output any other coefficients for agreement/difference in proportions etc. but (if required) these can be achieved within other functions and/or packages.\n\nmcnemar.test(X, correct = FALSE)"
  },
  {
    "objectID": "Comp/r-sas_rounding.html",
    "href": "Comp/r-sas_rounding.html",
    "title": "R v SAS rounding",
    "section": "",
    "text": "Rounding; R and SAS\nOn comparing the documentation of rounding rules for both languages, it will be noted that the default rounding rule (implemented in the respective language’s round() function) are different. Numerical differences arise in the knife-edge case where the number being rounded is equidistant between the two possible results. The round() function in SAS will round the number ‘away from zero’, meaning that 12.5 rounds to the integer 13. The round() function in Base R will round the number ‘to even’, meaning that 12.5 rounds to the integer 12. SAS does provide the rounde() function which rounds to even and the janitor package in R contains a function round_half_up() that rounds away from zero. In this use case, SAS produces a correct result from its round() function, based on its documentation, as does R. Both are right based on what they say they do, but they produce different results (Rimler, M.S. et al.).\nReferences\nRimler M.S., Rickert J., Jen M-H., Stackhouse M. Understanding differences in statistical methodology implementations across programming languages (2022, Fall). ASA Biopharmaceutical Report Issue 3, Volume 29.  Retrieved from https://higherlogicdownload.s3.amazonaws.com/AMSTAT/fa4dd52c-8429-41d0-abdf-0011047bfa19/UploadedImages/BIOP%20Report/BioPharm_fall2022FINAL.pdf"
  },
  {
    "objectID": "minutes/index.html",
    "href": "minutes/index.html",
    "title": "Meeting Minutes",
    "section": "",
    "text": "How to select packages, Content & Conferences\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWebsite structure update, Team list, Conferences\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCAMIS-ONCO, Conferences, Academic & regulatory input plans\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEnd of year summary, plan for 2024\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nFDA quartely meeting, 1st survey feedback - general updates\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nConference updates & feedback, FDA quartely meeting, CAMIS-ONCO workshop\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nFDA quartely meeting, FDA CSS, SDEs, website & conference plans\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPlan for Advertising CAMIS progress\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWhite Paper Finalization, Advertising CAMIS\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWhite Paper, Website, Launch Plan\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWhite Paper, Website, Launch Plan\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWhite Paper, Website, ONCO, Volunteers, Conferences\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWhite Paper and Demo of connecting Rstudio with Github repo\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNew Website Discussion\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRestart Meeting\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "minutes/posts/8Jan2024.html",
    "href": "minutes/posts/8Jan2024.html",
    "title": "CAMIS-ONCO, Conferences, Academic & regulatory input plans",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\n\nattendees\n08_Jan_24\n\n\n\n\nChristina Fillmore\nYes\n\n\nLyn Taylor\nYes\n\n\nMolly MacDiarmid\nYes\n\n\nBrian Varney\nYes\n\n\nChi Zhang\nYes\n\n\nOrla Doyle\nYes\n\n\nHarshal Khanolkar\nYes\n\n\nLily Hseih\nYes\n\n\nFilip Kabaj\nYes\n\n\nMartin Brown\nYes\n\n\nMin-Hua Jen\nYes\n\n\nSarah Rathwell\nYes\n\n\nKasa Andras\nYes\n\n\nAditee Dani\nYes\n\n\nKeaven Anderson\nYes\n\n\nBenjamin Arancibia\nNo\n\n\nWilmar Igl\nNo\n\n\nVikash Jain\nNo\n\n\nMia Qi\nNo\n\n\nLeon Shi\nNo\n\n\nVandaya Yadav\nNo\n\n\nStephen McCawille\nNo\n\n\nVikrant Vijay\nNo\n\n\nVidya Gopal\nNo\n\n\nDhvani Patel\nNo\n\n\nKyle Lee\nNo\n\n\nChelsea Dickens\nNo\n\n\nDavid Bosak\nNo\n\n\nMichael Kane\nNo\n\n\nLukas Brausch\nNo\n\n\nMichael Walshe\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgenda & Minutes\n\nCAMIS- ONCO: Update on progress & next steps to include:\nRegular meetings Cheat sheet for PHUSE 2024 PHUSE CSS planning (workshop in June). Python volunteers & code creation. White paper.\nACTION: Lyn to follow up with Soma/Vikesh to assess status of CAMIS-ONCO. Also set up meeting with team to discuss python content going into website\nOther Conference planning\nLyn will update the conference tab on the repo.\nPHUSE US Connect (Soma/ Vikesh) and Brian are attending.\nUseR is now open for abstract submission (deadline mid-march). Any volunteers to submit /attend. Salzburg (Europe) 8-11 July. Chi will be going, and volunteers to submit an abstract for us.\nPharmaSUG - Abstracts due 15th January. Conference is: May 19th-22nd Baltimore. Volunteers required to submit abstract if possible.\nContent updates\nAnyone with any questions about what they are working on or how to assign themselves?\n\nMMRM - Volunteer please to look at Proc Mixed vs Proc GLIMMIX and use this to expand the SAS/mmrm.qmd file.\nKeaven Anderson (Merck) - will start to look at SAS vs R for sample size / group sequential design / power. They use EAST, gsDesign, but others use rpact. Does anyone have experience of this (& using SAS for sample size)? Lyn & Martin & Keaven will meet to discuss on Friday.\nChristina: will add sales pitch to Website - Why CAMIS !? + re-arrangment of some of the content.\n\nObjective to get more regulatory input\nWork with PSI AIMS as they plan a EMA regulatory panel discussion on R Any other ideas?\nFDA/ Other regulators input/discussion.\nGit training plan for 2024 PSI conference abstract rejected. Creation of a short training session (like the R/pharma workshop) or 6 week 2 hr/ week course. ACTION: Lyn/Christina/Martin to follow up with PSI re: delivery of training. Restart GIT training meetings (Christina/Alex/ Irene)\nInteraction with more Academics & Universities\nPlease can you present/advertise to your universities contacts. Anyone got contacts they can utilize? Ideas for spreading the word? Lyn doing Presentation at University of Sheffield on 28th Feb for RSS local group.\nAcademia Projects ALL: to think about possible dissertation projects. Plan to list available projects in repo & write descriptions of what the project would entail such that universities students can use them at dissertation projects Prof Richard Stevens (Oxford) is open to projects if we have any. Also Novonordisk : working with Alberg Denmark university to have a proposal for project.\nRaising awareness within companies to flag issues to CAMIS\nALL: brainstorm how we can spread awareness within our organization & wider community\nEFSPI - PSI strategy day / heads meeting\nASA OpenStatsware - Orla Doyle: Focus is more on package development. If a gap comes up we could make them aware package is needed. Can also look to sassy r package to see if that replicates SAS (if it’s right to do so) David Bosak. Lyn meeting with David next later this week.\n\nPlan for next Blogs:\n\n\nadd blog tab to repo, then when we post we can link through.\nIdeas for next blogs? - perhaps pick a topic we have content already for & post blog on it.\n\n\nFunding requirements NOTE: We can apply for a grant for any funding if anyone sees an opportunity to progress our work quicker through this method. NOTE: if any university project or individuals need funding to do this CAMIS work (creation of content), then we do have an option to apply to the R Consortium for funding.\nAOB\n\nLinear Regression SAS & R, text are now live on website. Results match, but would be good to add a COMP file which just says what we checked & what matched… for example incase something comes up in future that does not match."
  },
  {
    "objectID": "minutes/posts/15May2023.html",
    "href": "minutes/posts/15May2023.html",
    "title": "White Paper, Website, Launch Plan",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\n\nattendees\n15_may_2023\n\n\n\n\nAiming Yang\nYes\n\n\nBen Arancibia\nYes\n\n\nBrian Varney\nNo\n\n\nChristina Fillmore\nYes\n\n\nChelsea Dickens\nNo\n\n\nChi Zhang\nNo\n\n\nClara Beck\nNo\n\n\nAditee Dani\nYes\n\n\nDoug Kelkoff\nNo\n\n\nDhvani Patel\nNo\n\n\nFilip Kabaj\nNo\n\n\nHarshal Khanolkar\nNo\n\n\nIris Wu\nNo\n\n\nJayashree Vedanayagam\nYes\n\n\nJoe Rickert\nNo\n\n\nKyle Lee\nNo\n\n\nLeon Shi\nYes\n\n\nLily Hsieh\nNo\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nYes\n\n\nMichael Kane\nNo\n\n\nMichael Rimler\nNo\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nNo\n\n\nMolly MacDiarmid\nYes\n\n\nMona Mehraj\nNo\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nYes\n\n\nVandana Yadav\nNo\n\n\nVidya Gopal\nNo\n\n\nVikash Jain\nYes\n\n\nWilmar Igl\nYes\n\n\nOrla Doyle\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgenda\n\nWhite paper - Lyn/Min-Hua\nWebsite progress - Christina\nUpdate on Launch - Lyn\nCAMIS-ONCO - Soma Sekhar\nVolunteer Open Roles\nConference Attendance\nAOB\n\n\n\nMeeting minutes\nWhite Paper Update: Min-Hua Paula at PHUSE will distribute for public review. Over the next days we’ll get a link to the official review. ACTION :Christina will put the white paper onto the website as draft open for public review\nWebsite progress: Christina Website content progressing well. Ben Arancibia - progressing MMRMs + other areas.\nSurvival - With Christina to fix importing. ANCOVA - Aditee in progress - change to CSV not SAV (SPSS file). Update to call it linear regression, Lyn to help find ANCOVA (testing treatments) Independant Two-Sample t-test in SAS - Vikash got a few changes then will load ok.\nLyn: To create a FAQ doc for the website. Make sure it references available material elsewhere so it doesn’t become out of date quickly.\nNOTE: When you do a pull request, check your action to see if the checks pass/fail & reach out to Lyn/CHristina if you have problems. Remember to do snapshot::renv, so that any packages you install were snapshot to the central repo. Else it will fail when you do the pull request as the repo wont have the packages in it that your code needs.\nLaunch Update\n\nBlog video now available here\nBlog text to use with various lengths also available here\nConference slides & abstract available here\n\nRather than a standard set, plan is to have abstracts & slides/posters put into this folder (inc. name & date of conference) then people can use the contact that they have preference to use.\nHarshal has loaded IASCT slides to same location.\n\nContacts for Societies\n\nPSI /EFSPI (Martin) - Content sent.\nR Consortium / PHUSE / RSS (Lyn) - Content sent.\nIASCT (Harshal) - Conference went well and lots of interest from IASCT.\nASA (Leon) - TBC who are ASA to reach out to? If Ben has any contacts that Leon could use please let him know.\nSAS - we may reach out to SAS directly through PHUSE. TBC if they would be Ok with us including their data, copyright. Hopefully they’d give approval as not or project & advertising what you can do in SAS. ACTION: to find contact who may be interested in update/review of SAS. Does PHUSE have a contact already that we can use. Lyn to ask Paula. Aiming/ Martin to let Lyn know if she has a contact. ACTION : Lyn/Christina to Add a disclaimer that we are volunteers adding open source content, but if you see anything that infringes copyright please let us know and we’ll remove it immediately.\n\n\nCAMIS- ONCO: Soma Sekhar\n\nValidation of endpoints (primary/secondary oncology endpoints). Propose to do poster at PHUSE CSS. Once Mia’s survival section is loaded. Sema Sekhar to review. Then highlight what’s missing - what else you want to add. Max combo. BICR vs RECIST? In future we can discuss how these fit with current CAMIS structure. Focus on the Stats method ideally. ACTION :Christina to email Semar Sekhar once Survival is live on Website.\n\nConferences Let’s review\nVikesh- plan for CSS. Abstract deadline 12th June, 30th june registration opens.\nPosters only - only invited people can be speakers. Somar Sekhar, Aditee Dani would be happy to do posters. Suggest all 3 meet to discuss contribution to poster or doing separate ones but not duplicating the same content.\nAsk PHUSE CSS working group (Data visualization and open source technology) DVOST - if we can have a presentation next year at the CSS.\nJSM - ASA conference. Leon attending. Abstract due Feb 2024 - so try have a rep there next year.\nPHUSE Single day event (SDE- Toronto Mississauga), PHUSE EU got a poster abstract: Jayashree Vendanayagam PHUSE Single day event (New york - regeneron hosting Oct 16, check check if Aiming can do any poster/presentation/advert)\nAZ R pharm conference 7th June. LYn & Martin presenting."
  },
  {
    "objectID": "minutes/posts/21Aug2023.html",
    "href": "minutes/posts/21Aug2023.html",
    "title": "FDA quartely meeting, FDA CSS, SDEs, website & conference plans",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\n\nattendees\n21_aug_2023\n\n\n\n\nAiming Yang\nNo\n\n\nBen Arancibia\nNo\n\n\nBrian Varney\nYes\n\n\nChristina Fillmore\nYes\n\n\nChelsea Dickens\nYes\n\n\nChi Zhang\nYes\n\n\nClara Beck\nNo\n\n\nAditee Dani\nYes\n\n\nDoug Kelkoff\nNo\n\n\nDhvani Patel\nNo\n\n\nFilip Kabaj\nYes\n\n\nHarshal Khanolkar\nYes\n\n\nIris Wu\nNo\n\n\nJayashree Vedanayagam\nYes\n\n\nJoe Rickert\nNo\n\n\nKyle Lee\nNo\n\n\nLeon Shi\nNo\n\n\nLily Hsieh\nYes\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nYes\n\n\nMichael Kane\nNo\n\n\nMichael Rimler\nNo\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nNo\n\n\nMolly MacDiarmid\nYes\n\n\nMona Mehraj\nNo\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nYes\n\n\nVandana Yadav\nYes\n\n\nVidya Gopal\nNo\n\n\nVikash Jain\nYes\n\n\nWilmar Igl\nYes\n\n\nOrla Doyle\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgenda\nPHUSE FDA CSS Poster acceptance & white paper planning: Soma Sekhar\nSocial Media update : Harshal\nPHUSE SDEs: Missisauga: June 8th feedback: Jayashreee\nPreparation for PHUSE FDA Quarterly meeting 13th sept:  Questions/Slides feedback: Lyn\nWebsite Christina/ All\n\nNew Role: “Content curation lead”\n\nWe have been missing some posts when added to issues/discussion pages on website\nThis role will Monitor the  “Discussion” and “Issues” pages of the repo, and help to raise at each meeting where we need volunteers to answer questions/ add to discussion.\nWe will add a Standard agenda item lead by the “Content curation lead” to go through issues, & assign to people / close down issues/ discussions.\nIf you would like to volunteer please let Lyn / Christina know.\n\nHow can we encourage creation of more content?\n\n\n\nWhat areas are key for us to focus on\nMMRM update\ngithub training plan (R/Pharma workshop & PSI training course)\n\nUpcoming conference planning.\n\nRSS 7th Sept: Lyn presentation\n\n\n\nPOSIT Conf: Lyn to reach out to Juliane / Doug to ask to include slide for CAMIS\nPHUSE SDE New York: Oct 16th :  Aiming\n\n\n\nMeeting minutes\nReview of Action log\n\n\n\nAction\nAssigned to\nStatus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to new members: Chi Zhang & Filip Kabaj\nPHUSE SDEs: Missisauga: June 8th feedback: Jayashreee\nGPT chat & machine learning for oncology presentations, needs for guidelines on how to use next level tools were discussed. Jayashree presented on behalf of CAMIS but on a Shiny App & got good questions including highlighting that for Endpoint /efficacy analysis they may require very specific standards so not easy to be generic/default. Chris Hurley (PHUSE SDE) also mentioned CAMIS which is great exposure for us.\nPreparation for PHUSE FDA Quarterly meeting 27th sept:  Questions/Slides feedback: Lyn/ Harshal\nMeeting was postponed by 4 weeks so we have time to prepare a short survey and send out on social media. Harshal went through the proposed questionnaire. Filip suggested Q2 to refer to frequency more specifically. Harshal to update & distribute\nSocial Media update : Harshal\nHarshal has posted the PSI poster post to social media. Going forward the proposal is to run a series of posts to focus on the content on the website - perhaps a short post just to say have you seen this new content and provide links.\nRE: Workshop - FDA CSS event (5-7 June): could run a comparison of SAS vs R workshop. Could focus on a set of issues & work though them make content & resolve issues. Could turn to linkedIn to ask wider community to vote for the biggest issue outstanding that they’d like to look into and select these for resolving at the workshop. See item below, as can discuss with Soma following this years FDA CSS event.\nWebsite Christina/ All\n\nThank you ALL!! Much content has been recenly pushed to the website.\nNew Role needed for a “Content curation lead”\n\nWe have been missing some posts when added to issues/discussion pages on website\nThis role will Monitor the  “Discussion” and “Issues” pages of the repo, and help to raise at each meeting where we need volunteers to answer questions/ add to discussion.\nWe will add a Standard agenda item lead by the “Content curation lead” to go through issues, & assign to people / close down issues/ discussions.\nJayashree & Chi volunteered to take on the role & to help monitor the repo activity. Lyn & Christina can put together guidance of what’s needed. Currently it’s just the issues & discussion, as pull requests are currently Ok being approved by Lyn & Christina as it’s a bit tricky to make sure it fits in with the repo and doens’t break anything!\nChi suggested that Christina check out projects to see if that would help to monitor whose doing what - may not work if can only be accessed by those already with granted access as we want anyone to be able to assign themselves. ACTION Christina to: Change the readme to say how to assign yourselves to content tasks.\n\nHow can we encourage creation of more content? / What areas are key for us to focus on\n\nGreat increase in content pre-meeting so Ok to grow organically for now.\nMMRM update : Christina to add link to MMRM website to cross reference.\n\ngithub training plan (R/Pharma workshop - free to attend in end October & PSI training course - series of session each week for x weeks, & PSI Conference Amsterdam - workshop 1.5 hrs.)\n\nPHUSE FDA CSS Poster acceptance (sept 20th) & white paper planning/ CAMIS ONCO: Soma Sekhar\nSoma demonstrated the poster which he’ll present with Vikash at CSS. Focused on Solid tumors OS/PFS but could broaden CAMIS ONCO with time to include other cancer.\nLonger term plan to create a white paper and to load survival analysis in python to repo.\nSoma to work with Harshil & others, to plan a sub team to work on CSS workshop for June 2023.\nLyn asked if there is demand for packages to be written that do standard stats analysis? The difficulty with this is how to standardise the programming and what it adds in addition to existing packages. It may not be worthwhile as options need to be considered so can’t automate.\nUpcoming conference planning.\n\nRSS 7th Sept: Lyn presentation\n\n\n\nPOSIT Conf: Lyn to reach out to Juliane / Doug to ask to include slide for CAMIS\nPHUSE SDE New York: Oct 16th :  Aiming\nR/Pharma - Christina submitted presentation\n\nAOQ/AOB - None."
  },
  {
    "objectID": "minutes/posts/12Dec2022.html",
    "href": "minutes/posts/12Dec2022.html",
    "title": "Restart Meeting",
    "section": "",
    "text": "Attendees\n\n\nNew names:\nRows: 34 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(13): attendees, 12_dec_2022, 23_Jan_2023, 13_feb_2023, 13_mar_2023, 17_... lgl\n(1): ...14\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...14`\n\n\n\n\n\n\n\n\n\nattendees\n12_dec_2022\n\n\n\n\nAiming Yang\nNo\n\n\nBen Arancibia\nYes\n\n\nBrian Varney\nNo\n\n\nChristina Fillmore\nNo\n\n\nChelsea Dickens\nNo\n\n\nChi Zhang\nNo\n\n\nClara Beck\nNo\n\n\nAditee Dani\nNo\n\n\nDoug Kelkoff\nNo\n\n\nDhvani Patel\nNo\n\n\nFilip Kabaj\nNo\n\n\nHarshal Khanolkar\nNo\n\n\nIris Wu\nNo\n\n\nJayashree Vedanayagam\nYes\n\n\nJoe Rickert\nYes\n\n\nKyle Lee\nYes\n\n\nLeon Shi\nNo\n\n\nLily Hsieh\nYes\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nYes\n\n\nMichael Kane\nNo\n\n\nMichael Rimler\nNo\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nYes\n\n\nMolly MacDiarmid\nYes\n\n\nMona Mehraj\nNo\n\n\nPaula Rowley\nYes\n\n\nSoma Sekhar Sriadibhatla\nYes\n\n\nVandana Yadav\nNo\n\n\nVidya Gopal\nNo\n\n\nVikash Jain\nYes\n\n\nWilmar Igl\nNo\n\n\nOrla Doyle\nNo\n\n\n\n\n\n\n\n\n\n\n\nWelcome and brief CAMIS project update: Lyn\nPlease consider which areas of the project you would like to be involved with: \n    * Repository reviewers/framework reviewers\n    * Content creators (Comparing analysis method implementations in software)\n    * Github - content review / approval\n    * Marketing, i.e. blogs and sharing with wider community (PSI, ASA, PHUSE etc) to encourage contributions \n    * Long term plan - Extend reach beyond Europe/USA.\n\n\nRepository roadmap : Lyn\n    * Sample website & templates – mid January 2022\n    * Feedback on website/templates – EOB Feb 2022\n    * Revisions – March 2022\n    * Launch – April 2022\n\n\nWhite paper status update: Min-Hua\nNOTE: we would like to put the URL of new website and mention CAMIS in paper if possible?\n\n\nOther stream updates: All\nNeed to identify who were the previous stream leads to check with them we can put content into new template formats.\n    * CMH\n    * Mixed models\n    * Linear models\n\n\nQuestions/ AOB - All\n    * Future meeting plan – Lyn set up directly so can be quickly adjust/ add more meetings if necessary?\n    * Name change: CAMIS: Comparing analysis method implementations in software\n    * Do we need our own logo. CAMIS. Volunteers?\n    * Supported by PHUSE & PSI & ASA. Assign rep (or reps) for each organization. \n    * Extend membership given many previous members no longer on project\n    * Volunteer needed – can someone create a comparison using any method (but comparing SAS to Python/Julia or R to Python/Julia) – so we can test up with not just R Vs SAS.\n    * AOB."
  },
  {
    "objectID": "minutes/posts/9oct2023.html",
    "href": "minutes/posts/9oct2023.html",
    "title": "FDA quartely meeting, 1st survey feedback - general updates",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\n\nattendees\n09_oct_2023\n\n\n\n\nAiming Yang\nNo\n\n\nBen Arancibia\nNo\n\n\nBrian Varney\nYes\n\n\nChristina Fillmore\nNo\n\n\nChelsea Dickens\nYes\n\n\nChi Zhang\nYes\n\n\nClara Beck\nNo\n\n\nAditee Dani\nNo\n\n\nDoug Kelkoff\nNo\n\n\nDhvani Patel\nYes\n\n\nFilip Kabaj\nNo\n\n\nHarshal Khanolkar\nYes\n\n\nIris Wu\nNo\n\n\nJayashree Vedanayagam\nNo\n\n\nJoe Rickert\nNo\n\n\nKyle Lee\nNo\n\n\nLeon Shi\nNo\n\n\nLily Hsieh\nYes\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nYes\n\n\nMichael Kane\nNo\n\n\nMichael Rimler\nNo\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nNo\n\n\nMolly MacDiarmid\nYes\n\n\nMona Mehraj\nNo\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nNo\n\n\nVandana Yadav\nYes\n\n\nVidya Gopal\nYes\n\n\nVikash Jain\nNo\n\n\nWilmar Igl\nYes\n\n\nOrla Doyle\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgenda\nWelcome New Members: Vikrant Vijay FDA, Ismael Rodriguez (Appsilon)\nPHUSE SDE: Missisauga: June 8th feedback: Jayashreee\nCAMIS-ONCO update: Soma Sekhar / Vikash Jain\nPHUSE US Connect 2024 planning (Poster accepted, will find out re: workshop oct 20th)\nWhite paper planning\nFDA meeting update /survey results.: presented to around 15 FDA representatives, but didn’t get any questions/ comments on the call. However, follow the meeting, our slides were passed to others at the FDA and Vikrant Vijay got in touch to join the group. He’s not available today, but will attend in future.\nQuestionnaire only got 16 responses, 9/16 had heard of CAMIS, 13/16 used R for GXP, 4/15 used python for GXP, 1/16 used Julia for GXP. 9/16 had experienced discrepancies whilst trying to replicate analysis between languages.\nACTION: To re-run maybe just before CSS 2024 / or each year to assess progress and use for conference presentations.\nCommon programming challenges\n\nWhilst executing Analysis in R, I come across challenge in Numerical Differences in Statistical results. Would it be great if Industry & regulatory work together to build Standard R package for Statistical Methods & details in CAMIS repository would be highly appreciated.\nEnsuring reproducible environments and having people accept that different results for different implementations of an algorithm should perhaps be interpreted as a hint towards the accuracy, rather than one of the methods being wrong.\nStandard deviation initially did not match between and SAS. Later resolved by using the type option\nChallenges to figure out array of methods to replicate the same results across different software platforms/ Finding why resutls differ / unclear documentation/ discrepancy in values\nLS means contrasts from GLMs or MMRMs between SAS and R (vis lme4/mmrm + emmeans)\nParsing issues, scalability issues and network crash.\nImplementation of median seem to differ between R and SAS…. Sometimes joins in dplyr can also behave differently than i would expect with raw SQL\nDifferent methodologies (e.g for sample size calculation0 and lack of non-standard methods in SAS (e.g sample size for adaptive design).\n\nSocial Media update : Harshal\nNewsletter, (quarterly, or monthly) to advertise progress (ie. content we created) & conferences we are attending. Can we advertize ourself more to EMA, PMDA etc.. Can send to Frank Petavy (methodological working party). ACTION: Harshal/Lyn put togther summary for newsletter & send to Wilmar to reach out to Frank & others. Lyn to email David to see if any wider participation.\nhttps://www.ema.europa.eu/en/committees/working-parties-other-groups/chmp/methodology-working-party\nWebsite Christina/ Chi / Jayashree\n\nAll pull requests accepted & everything up to date\nMMRM update - No update. ACTION: Chi Zhang: will follow up to see if we can get someone to add in MMRM package to our existing content.\ngithub training plan (R/Pharma workshop & PSI training course) - ongoing prep for workshop & course through PSI AIMS team.\nAgnieszka (PAREXEL) and Chi – working on Wilcoxon test content for paired & unpaired data.\nACTION: Chi & Christina to talk about local rendering & renv issue with not having the packages to be able to render… once we know a fix, can write up and put on the website.\n\nUpcoming conference planning\n\nPHUSE SDE New York: Oct 16th : Aiming\nPHUSE US Connect: Soma/Vikash\nR/Pharma – Christina?\nSESUG (South East SAS user group) late october 2023, Brian will present on CAMIS.\nNorth Carolina - SDE if anyone wants to volunteer to attend let us know.\n\nAOB - None."
  },
  {
    "objectID": "minutes/posts/13mar2023.html",
    "href": "minutes/posts/13mar2023.html",
    "title": "White Paper, Website, ONCO, Volunteers, Conferences",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\n\nattendees\n13_mar_2023\n\n\n\n\nAiming Yang\nYes\n\n\nBen Arancibia\nYes\n\n\nBrian Varney\nYes\n\n\nChristina Fillmore\nYes\n\n\nChelsea Dickens\nNo\n\n\nChi Zhang\nNo\n\n\nClara Beck\nNo\n\n\nAditee Dani\nYes\n\n\nDoug Kelkoff\nNo\n\n\nDhvani Patel\nNo\n\n\nFilip Kabaj\nNo\n\n\nHarshal Khanolkar\nYes\n\n\nIris Wu\nNo\n\n\nJayashree Vedanayagam\nYes\n\n\nJoe Rickert\nNo\n\n\nKyle Lee\nYes\n\n\nLeon Shi\nYes\n\n\nLily Hsieh\nYes\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nNo\n\n\nMichael Kane\nNo\n\n\nMichael Rimler\nYes\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nNo\n\n\nMolly MacDiarmid\nYes\n\n\nMona Mehraj\nNo\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nYes\n\n\nVandana Yadav\nNo\n\n\nVidya Gopal\nNo\n\n\nVikash Jain\nYes\n\n\nWilmar Igl\nNo\n\n\nOrla Doyle\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgenda\n\nWhite paper - Lyn\nWebsite progress - Christina\nCAMIS-ONCO - Soma Sekhar\nVolunteers Roles\nConference Reps\nAOB/ PHUSE feedback\n\n\n\nMeeting minutes\nWhite Paper Update: Lyn Min-Hua is sending the white paper to the PHUSE group lead for review soon just a bit more tidying to do following comments.\nWebsite progress: Christina\nThe home page now has the list of stats methods we are looking to collect data on.\nACTION :Christina to put actions for each stats method which we need help to complete into github. We will assign those already selected to the people below. This will enable people that want to help to be able to see which are available for people to select.\nCAMIS- ONCO: Soma Sekhar\n\nplan to launch later this week.\npossibly white paper/ conference presentation.\n\nReview of volunteer roles\n\nGeneral Co-ordination -Lyn\nWebsite Co-ordination / Home page table - Christina\nCAMIS-ONCO - Soma Sekhar\nCopying CSRMLW material to CAMIS\n\nCMH: Aiming Yang\nLinear Models: Brian Varney (ACTION: Set up call with Dani, Lyn, Vikash, christina, + anyone else whose interested in helping to please volunteer)\nMMRM: Ben Arancibia\nSurvival: Min-Hua Jen, Mia Qi\n\n\nACTION: Christina to also add “actions” for people to pick up the following duties.\n\nCo-ordinator for conference material - share standard slides/ content /abstracts\nVolunteer to design a CAMIS Logo\nSocial media rep - to co-ordinate posts (linkedIn/Twitter)\nConference reps/ attendees needed\n\nWe will also add a page which lists the conferences so we can collate and coordinate whose going with the hope of advertising the project more widely once we have content on the website. Include a column for timelines/ abstract deadlines.\n\n\n\n\n\n\n\n\n\n\n\nConference\nLocation\nDate\nMain Contact\nVolunteers to attend\nDetails\n\n\n\n\nPHUSE US Connect\nOrlando, Florida\n5-8 March 2023\nSoma Sekhar\n\nPresentation\n\n\nDISS (Duke industry statistics symposium)\nVirtual\n29-31st March 2023\nLyn Taylor\nMolly MacDiarmid\nPoster\n\n\nPSDM(Pharmaceutical statistics and data management)\nNetherlands\n19 Apr 2023\n\n\n\n\n\nIASCT (ConSPIC - conference for statistics and programming in clinical research)\nBengaluru, India\n4-6 May 2023\nHarshal Khanolkar\nHarshal Khanolkar\n\n\n\nPSI 2023 Conference\nHammersmith London West, England\n11-14 June 2023\nMartin Brown\nChristina Fillmore\nLyn Taylor\nMolly Macdiarmid\nMartin Brown\nOral & poster submission completed\n\n\nDIA 2023 Global Annual Meeting\nBoston MA, USA\n25-29 June 2023\n\n\n\n\n\nJoint statistical meeting (JSM)\nToronto, Ontario, Canada\n5-10 Aug 2023\n\n\n\n\n\nISCB Conference\nMilan-Italy\n27-31 Aug 2023\n\n\n\n\n\nRSS conference\nHarrogate, England\n4-7 sept 2023\n\n\n\n\n\nASA Bio pharmaceutical Section Regulatory-industry Statistics Workshop\nRockville, Maryland, USA\n27-29 Sept 2023\n\n\n\n\n\nEASD 2023 - European Association for study of diabetes\nHamberg Germany\n02-06 Oct 2023\n\n\n\n\n\nPHUSE EU Connect 2023\nICC Birmingham, England\n5-8 November 2023\nChristina Fillmore?\n\n\n\n\nR in Pharma /\nPOSIT conf.\nVirtual/ In person\nNov?\nChristina Fillmore?"
  },
  {
    "objectID": "minutes/posts/11Sept2023.html",
    "href": "minutes/posts/11Sept2023.html",
    "title": "Conference updates & feedback, FDA quartely meeting, CAMIS-ONCO workshop",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\n\nattendees\n11_sep_2023\n\n\n\n\nAiming Yang\nNo\n\n\nBen Arancibia\nNo\n\n\nBrian Varney\nYes\n\n\nChristina Fillmore\nNo\n\n\nChelsea Dickens\nYes\n\n\nChi Zhang\nNo\n\n\nClara Beck\nNo\n\n\nAditee Dani\nYes\n\n\nDoug Kelkoff\nNo\n\n\nDhvani Patel\nYes\n\n\nFilip Kabaj\nNo\n\n\nHarshal Khanolkar\nYes\n\n\nIris Wu\nNo\n\n\nJayashree Vedanayagam\nNo\n\n\nJoe Rickert\nNo\n\n\nKyle Lee\nNo\n\n\nLeon Shi\nNo\n\n\nLily Hsieh\nYes\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nNo\n\n\nMichael Kane\nNo\n\n\nMichael Rimler\nNo\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nNo\n\n\nMolly MacDiarmid\nYes\n\n\nMona Mehraj\nYes\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nYes\n\n\nVandana Yadav\nNo\n\n\nVidya Gopal\nYes\n\n\nVikash Jain\nYes\n\n\nWilmar Igl\nNo\n\n\nOrla Doyle\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgenda\nPHUSE SDE: Missisauga: June 8th feedback: Jayashreee not available today so see if any feedback at next meeting\nPHUSE FDA CSS Poster acceptance, white paper planning, PHUSE US Connect planning & CAMIS ONCO update: Soma Sekhar / Vikash Jain\nPoster presentation is next week. Harshil will advertise on social media.\nSubmitted a abstract for poster & presentation/workshop for US Connect 2024. White paper ongoing. Python code for CAMIS-ONCO being created. Workshop for PHUSE CSS 2024 will need planning. DVOST subgroup ½ day workshop ACTION Soma/Vikash work on the CSS workshop For 2024 .\nVikash was at the PHUSE SDE Boston last week. Met Michael Rimler and he’s introduced CAMIS to the board so they are aware of it.\nRSS conference feedback - Lyn Suggestions were to get more academics, university representatives involved with the project as it also makes for nice disseration projects for BSc/MScs.\nIBS CEN2023 conference - Feedback from Chi IBS CEN2023 conference (biometric society central Europe network held in Basel), CAMIS was mentioned along with other R working groups in a talk. The talk was about software engineering working group, the one that developed MMRM. Quite encouraging!\nSocial Media update : Harshal\nCall for volunteers.\n\nSoftware Engineering Working Group –\n\nWG goals are:\n\nCollaborate to engineer selected R-packages which will fill in gaps in the open-source statistical software landscape, and to promote software tools designed by the working group through publications, conference presentations, workshops, training courses, and others.\nDevelop best practices for engineering high-quality statistical software, and promote the use of best practices in the broader Biostatistics community via public training materials.\nCommunicate and collaborate with other R software initiatives.\n\n\nWorking group HomePage - https://rconsortium.github.io/asa-biop-swe-wg/\nCo-chairs - Daniel Sabanes Bove and Ya Wang.\n\n\n\n\nStatistical Methods in Oncology Scientific Working Group – WG goals are:\n\nEncourage increased use of systematic oncology assessment approaches and selection of best methods through training and education\n\nGain clear understanding of current regulatory environments in oncology\nPrepare a library of recommended methods including innovative methods\nUnderstand commonly used and innovative methods\nCollect and share experiences on using innovative designs\nPut together points to consider for oncology innovative designs’ implementation\nDevelop new methods if needed\n\nEducate the broader statistical community to understand and contribute to this important area\nIncrease statisticians’ leadership roles in cross-functional collaboration\nCommunicate statistical perspectives to larger clinical trial community\nCo-chairs - qjiang@seagen.com and olga.marchenko@bayer.com\n\n\nAdittee volunteered to join the Stats methods in oncology Scientific working group to represent us. ACTION: Aditee to reach out to co-chairs and ask to join, then feedback at our meetings on if there are opportunities to collaborate.\nAlso looking for volunteers to work on the Software Engineering WG & CAMIS ONCO- white paper, Harshil will work with Vikash & Soma to request that PHUSE share to advertise what we are looking for in a linkedIn post.\nPreparation for PHUSE FDA Quarterly meeting 27th sept: Slides / Survey questions : Lyn / harshil Tomorrow we can get the data back from questionnaire.   ACTION : Lyn/Harshil to meet, summarize survey & prepare slides.   Friday.\nWebsite Christina/ Chi / Jayashree\n\nCurrently, needs fixing pending RENV issue. So will be slight delay on getting content on the website\nContent curation lead items - Chi / Jayashree Made a great start to close discussions & address actions which have questions on them.\nMMRM update - No update ACTION: Lyn to follow up.\ngithub training plan (R/Pharma workshop & PSI training course) - ongoing prep for workshop & course through PSI AIMS team.\nAgnieszka (PAREXEL) and Chi – working on Wilcoxon test content for paired & unpaired data.\n\nUpcoming conference planning\n\nPHUSE SDE New York: Oct 16th : Aiming\nPHUSE US Connect: Soma/Vikash\nR/Pharma – Christina?\nSESUG (South East SAS user group) late october 2023, Brian will present on CAMIS.\nNorth Carolina - SDE if anyone wants to volunteer to attend let us know.\n\nAOB - None."
  },
  {
    "objectID": "templates/R_template.html",
    "href": "templates/R_template.html",
    "title": "R Template",
    "section": "",
    "text": "Optional If there is only one available package this can be deleted. Otherwise please make a short list, paragraph or table. If there is a reason to use one package vs another please include it. Please make sure to include what version of the packages you are using\n\n\n\nJust a sentence or two about the data and a link to the data page\n\n\n\n\n\nOptional if there is more than one package"
  },
  {
    "objectID": "templates/R_template.html#available-r-packages",
    "href": "templates/R_template.html#available-r-packages",
    "title": "R Template",
    "section": "",
    "text": "Optional If there is only one available package this can be deleted. Otherwise please make a short list, paragraph or table. If there is a reason to use one package vs another please include it. Please make sure to include what version of the packages you are using"
  },
  {
    "objectID": "templates/R_template.html#data-used",
    "href": "templates/R_template.html#data-used",
    "title": "R Template",
    "section": "",
    "text": "Just a sentence or two about the data and a link to the data page"
  },
  {
    "objectID": "templates/R_template.html#example-code",
    "href": "templates/R_template.html#example-code",
    "title": "R Template",
    "section": "",
    "text": "Optional if there is more than one package"
  },
  {
    "objectID": "contribution/contribution.html",
    "href": "contribution/contribution.html",
    "title": "Get Involved",
    "section": "",
    "text": "CAMIS is a community effort. Although this project does have a core team, the endeavor of tracking all these comparisons will fail without community contributions. We appreciate all contributions, big or small!\nYou can get involved in the following ways:\n\njoin our monthly online meetings\nreport a difference or request a new method by filing an issue\ncontribute to our documentation"
  },
  {
    "objectID": "contribution/contribution.html#how-to-get-involved",
    "href": "contribution/contribution.html#how-to-get-involved",
    "title": "Get Involved",
    "section": "",
    "text": "CAMIS is a community effort. Although this project does have a core team, the endeavor of tracking all these comparisons will fail without community contributions. We appreciate all contributions, big or small!\nYou can get involved in the following ways:\n\njoin our monthly online meetings\nreport a difference or request a new method by filing an issue\ncontribute to our documentation"
  },
  {
    "objectID": "contribution/contribution.html#how-to-contribute-to-the-documentation",
    "href": "contribution/contribution.html#how-to-contribute-to-the-documentation",
    "title": "Get Involved",
    "section": "How to contribute to the documentation",
    "text": "How to contribute to the documentation\nPlease contribute by submitting a pull request (PR) and our team will review it.\n\nAdding a new page\nIf you are adding a new page, please follow our template guideline: R template\nGood documentation on data, methods are very much appreciated!\n\n\nFirst-time contributors\nWelcome to CAMIS! Please read this article: Get started, which contains some useful information to help you navigate your first PR submission.\n\n\nAsking for help\nIf you need any assistance with setting up your workspace, do not hesitate to contact @DrLynTaylor, @statasaurus and @andreaczhang!"
  },
  {
    "objectID": "contribution/get_started.html",
    "href": "contribution/get_started.html",
    "title": "Get Started",
    "section": "",
    "text": "The following instructions provides a step-by-step workflow to set up your workspace. In general, you need\n\nGit, GitHub\nR and Rstudio (especially if you are working on an R article)\nQuarto\nSAS, python (if you work on these topics)\n\n\nSet up Git, Github, Rstudio\nYou will need to get git, github, and RStudio setup to talk to each other. To do this you will need to have a github account and git installed on your computer.\nTo connect your computer to github, we tend to recommend using a PAT because it is bit easier than SSH. We have a script that can help you set that up, found here.\nFor more information Jenny Bryan has a great bookdown explaining how to get setup, alternatively see the following link for a short guidance.\n\n\nFork CAMIS repository to your own\nNow with RStudio all setup, you will need to fork the repository, which basically mean you want to make a copy of this repository that you own, so it will be under your github profile. This will allow you to make changes, without needing direct permission.\nTo do this you will need to go into github, into the CAMIS repo, and click “fork”. This will give you some options of how you want to fork the repo, honestly you can just keep the defaults and then click “Create fork”\n\n\n\n\n\n\n\nClone to your own computer\nOnce you’ve created a copy of this repository, you’ll need to clone it from GitHub to your computer. Click the “code” button to do this.\nThe method you’ll use, either “HTTPS” or “SSH”, depends on how you’ve connected your computer to GitHub. If you’ve set up using a PAT, select the “HTTPS” tab. If you’ve used “SSH”, then choose that tab. Either way, you will need to copy the location in the box.\n\n\n\n\n\n\n\nCreate an Rstudio project with version control\nIn RStudio, you will need to create a new project and select “Version Control” in the project wizard. Then you will select “Git” and finally paste the location copied from github into the URL box. Finally hit “Create Project” and you should be good to go!\n\n\n\n\n\n\n\nCreate your own branch in Rstudio\nGo into RStudio and Create a branch – Give you are working from your own fork, this step it is a bit optional. It is up to you if you want to make a separate branch or not. But, it is generally considered good practice, especially if you are planning on contributing regularly. To do this from RStudio click the branch button (on the git tab top right). Within the box that comes up ensure you are on the “remote=origin” and “Sync branch with remote” is checked. You can name the branch something to do with the amends you intend to make.\n\n\nStart writing\nEdit and /or add files within the CAMIS directories. If you are adding SAS guidance store under sas folder, R guidance store under r folder, for “SAS vs R” comparison store under comp. Follow the naming convention of the files already stored in those folders.\n\n\nCommit your changes, push to remote\nWithin Rstudio - Commit each change or new file added, and push to the repo from within R studio. Once you have completed the change you want to make, it is time for a pull request. Before we start though, it is good to check that your branch on github contains all the update you have done. If not you may need to push from Rstudio before moving onto the pull request.\n\n\n\nThis is what it will look like if you still need to push\n\n\n\n\nCreate a pull request\nPull request in github - Back on your fork in github you will see that your repo is now ahead of the main CAMIS repository. The first thing you want to do is make sure there aren’t any conflict that have arisen with the main repository, so you need to click ‘Sync fork’.\n\n\n\n\n\nIf that is all good then you can create a pull request by clicking on ‘Contribute’ and then ‘Open pull request’. This brings you to a page where you can explain your pull request if you like or you can just confirm you would like to go through with this pull request.\nThe final step is to add a reviewer, please add DrLynTaylor, statasaurus and andreaczhang. For more details about making pull requests see create a pull request.\nOnce your change is approved, and merged into the origin, you will be able to see your changes on CAMIS. If you have made a branch in your fork the branch will be deleted and you will need to create a new branch to add further contributions. NOTE: you can make the new branch called the same as the old one if you wish but ensure you select to overwrite the previous one."
  },
  {
    "objectID": "minutes/posts/20Nov2023.html",
    "href": "minutes/posts/20Nov2023.html",
    "title": "End of year summary, plan for 2024",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\n\nattendees\n20_nov_2023\n\n\n\n\nAiming Yang\nNo\n\n\nBen Arancibia\nNo\n\n\nBrian Varney\nYes\n\n\nChristina Fillmore\nYes\n\n\nChelsea Dickens\nYes\n\n\nChi Zhang\nYes\n\n\nClara Beck\nNo\n\n\nAditee Dani\nNo\n\n\nDoug Kelkoff\nNo\n\n\nDhvani Patel\nYes\n\n\nFilip Kabaj\nYes\n\n\nHarshal Khanolkar\nYes\n\n\nIris Wu\nNo\n\n\nJayashree Vedanayagam\nYes\n\n\nJoe Rickert\nNo\n\n\nKyle Lee\nNo\n\n\nLeon Shi\nNo\n\n\nLily Hsieh\nNo\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nYes\n\n\nMichael Kane\nNo\n\n\nMichael Rimler\nYes\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nNo\n\n\nMolly MacDiarmid\nYes\n\n\nMona Mehraj\nNo\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nYes\n\n\nVandana Yadav\nNo\n\n\nVidya Gopal\nNo\n\n\nVikash Jain\nYes\n\n\nWilmar Igl\nYes\n\n\nOrla Doyle\nYes\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgenda & Minutes\n\nEnd of year summary\nPost & summary diagram of 2023 will go onto linkedIn soon, please like/share\nWe would like to welcome Harshal Khanolkar to become a co-lead of the PHUSE CAMIS Repo. Harshal has been instrumental through 2023 helping Christina and I stay on track and making suggestions for improving the social media and knowledge sharing within the group.\nMMRM now updated & Complete for now.\nGithub training will be on youtube following R/pharma conference soon\n\n2023 : A Year of Progress for PHUSE CAMIS Working Group\nAs we draw towards the end of 2023, the PHUSE CAMIS Working Group reflect on their key progress and successes this year.\nThe CAMIS repository went live in January 2023, drawing on the content from the PHUSE CSRMLW project.  This searchable repo compares analysis method implementations in software (CAMIS) such as SAS, R and python. \nThe White Paper was published in June, which highlighted the importance of clearly specifying your analysis, such that it can be replicated in different software, and isn’t relying on default options which can be different.\nFor more complex analyses, it can still be hard to understand what defaults and algorithms your software is using, so the team focused 2023 on expanding our repo content, comparing SAS vs R methods.  By August, we had covered the following topics in the repo: quartiles, rounding, anova, mmrm, cmh, log-rank, cox-ph, mcnemar’s test, kruskal-wallis test and logistic. October saw the launch of the sub-working group: CAMIS-Oncology, led by Somasekhar Sriadibhatla (AstraZeneca).  This team will focus specifically on oncology endpoints and analyzing them in SAS, R and Python.  The CAMIS team have expanded in membership during 2023 presenting at numerous conferences around the world. In November, we welcomed Harshal Khanolkar (NovoNordisk), to join the leadership team alongside Christina Fillmore (GSK) and Lyn Taylor (PAREXEL).  Our focus for 2024, will be on the creation of additional content for the repo, and sharing awareness of the project across the medical research and wider community.  We’d like to take this opportunity to thank all of our team members and contributors, and encourage everyone to check out the repository and help us to grow our content CAMIS (psiaims.github.io).  If you would like to join the team please get in touch through the repo.\n2023: lessons learnt: What we did well?\n\nAdverts to industry & linkedin posts. To be Continued into 2024 - engage more unis, internship projects, academia, posit conf, r users conf, target key conferences\nGood sharing of conference content through the repo & improving the slides in an ongoing way.\nLeadership & project progress with plans. Transparency of the work. Nice to get Agendas pre-meeting & minutes after meeting in timely manner.\nLarge range of individual contributions helping to grow repo. 1/2 contributors within the phuse group, but 1/2 outside of the group. So spreading the word is really helping us to get external contributions.\nACTION: Christina & Chi: Please can you improve the 2023 conference tab, create a 2024 tab which contains link to presentations within the github repo.\n\n2023: lessons learnt: What we didn’t do so well?\n\nTime to get pull requests approved. Aim for 2024 to reduce the time so it’s a maximum of 2 weeks. The delay was often caused by issues with renv. Christina is working with posit directly to improve renv issues & has already updated contributions guidance to help instruct people on how to contribute such as using Forks rather than needing github username access.\npython - Delayed discussion in how to design the repo to store python content.\nACTION: Vikash/ Soma / Filip - to meet with Lyn / Christina to agree format going forward.\nMore discussion on CAMIS ONCO below.\n\n\n2023: lessons learnt: What is our focus for next year?\n\nMore content\nengage more unis, internship projects, academia, posit conf, r users conf, target key conferences\nCAMIS ONCO white paper, workshop & python/sas/r comparison (See below)\nCSS 2024 workshop, interaction with audience. 3-4 hrs hopefully. TBD at separate meeting, agenda workshop. Vikash, Harshal, Soma. 3-5 June.\nIdea for 2024: Set up a method such that people with no git / github skills can still contribute to the project. Perhaps set up a CAMIS email address. Assign volunteers for someone to email, then the github experts can load it in. Decide best process to non-R, non-github people.\n\nCAMIS-ONCO\nPlan to create cheat sheet for phuse 2024 - can go on CAMIS.\nNeed more volunteers in order to address all the endpoints. Oncology / survival team members needed to join Soma & Team. AZ investing in ChatGPT AZ version, it can create python code from SAS.\nIf AI can convert SAS code to python, we will then need people to test it. Volunteers needed to run in python. Can use the CAMIS repo data to test on hopefully but may need more detailed data? To see what data we currently have in the repo: see “data-info” and “data” folders.\nAction: Chi to have a look at the data folders, and decide better way to control/document data. Chi volunteered to help with Soma’s test to test Python. Harshal may also be able to find volunteers at novonordisk. Starting point for python would be the default options vs R.\n\nACTION: Lyn to Add to members list, who can run which languages & specialist areas (CAMIS-ONCO). ACTION: Soma to put poster into non_website_content/conferences.\nCAMIS: ONCO White paper: Needs to be progressed. invite all members to see if they can contribute. Set up regular meetings in 2024.\nPlan for 2024 : Project board in github: 5 categories\n\nCAMIS : Generic Method Implementation Team: More content\nCAMIS-ONCO: items as above\nUser Experience/Demo Team\nSocial media & Engagement: Advertise/ Universities\n\nbi-monthly post re: new content (newsletter: form to subscribe to newsletter so that when we post out they get informed). ACTION: lyn to check with PHUSE if we can do this, or if we want to ask R consortium to help similar to R validation hub email list.\nAlso would be good to have blog post tab on repository. ACTION: Chi to help Christina with design. Idea would be to have 1 post which goes out on social media, to the emails subscription & on the website.\nmore relationship with ASA OpenStatsware - Orla Doyle.\nopenstatsware (rconsortium.github.io) ACTION: Lyn/Christina/Orla to set up call to discuss collaboration.\n\n\nGeneral Tasks:\n\nPlan to review & accept content within 2 weeks of pull requests.\nPOSIT help with RENV situation\nSearch Engine Optimization: CAMIS full name on website? how do we become top hit ? Any volunteers to help with this let us know.\n\n\nACTION: Lyn to Cancel Dec Meeting 11th Dec. next meeting 8th Jan 2024"
  },
  {
    "objectID": "minutes/posts/17apr2023.html",
    "href": "minutes/posts/17apr2023.html",
    "title": "White Paper, Website, Launch Plan",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\n\nattendees\n17_apr_2023\n\n\n\n\nAiming Yang\nYes\n\n\nBen Arancibia\nNo\n\n\nBrian Varney\nYes\n\n\nChristina Fillmore\nYes\n\n\nChelsea Dickens\nNo\n\n\nChi Zhang\nNo\n\n\nClara Beck\nNo\n\n\nAditee Dani\nYes\n\n\nDoug Kelkoff\nYes\n\n\nDhvani Patel\nNo\n\n\nFilip Kabaj\nNo\n\n\nHarshal Khanolkar\nYes\n\n\nIris Wu\nNo\n\n\nJayashree Vedanayagam\nYes\n\n\nJoe Rickert\nYes\n\n\nKyle Lee\nYes\n\n\nLeon Shi\nNo\n\n\nLily Hsieh\nYes\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nYes\n\n\nMichael Kane\nNo\n\n\nMichael Rimler\nYes\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nYes\n\n\nMolly MacDiarmid\nYes\n\n\nMona Mehraj\nNo\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nNo\n\n\nVandana Yadav\nNo\n\n\nVidya Gopal\nNo\n\n\nVikash Jain\nNo\n\n\nWilmar Igl\nNo\n\n\nOrla Doyle\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgenda\n\nWhite paper - Lyn/Min-Hua\nLogo - All - voting!\nWebsite progress - Christina\nLaunch Plan - Lyn\nCAMIS-ONCO - Soma Sekhar\nVolunteer Open Roles\nConference Attendance\nAOB\n\n\n\nMeeting minutes\nWhite Paper Update: Min-Hua PHUSE are doing technical review so hopefully will come back shortly with any comments. I reminded them last week. Has been reviewed by leads team, now with steering committee ( Final review team), so hopefully not much longer.\nLOGO: Lyn - By a small majority the preferred option was the calculator without the P&lt;0.05 in it. This will now be redrawn & finalized. ACTION: Lyn will update website when image available. Will save under CAMIS/images so you can use for any posters/ presentations.\nWebsite progress: Christina **All - review of progress & answer any questions\nSurvival - Mia has made great progress on survival, Christina and Lyn to help fix branch issue & then will get it pushed to the live site.\nACTION : Lyn to Create a video of creating a branch / doing updates. push/pull - github pull request. Create a FAQ doc for the website.\nLaunch Plan\n\nAlign launch of website with release of white paper. Blog writing & “Video” launch - Lyn to write & distribute for review\nOnce content created reach out to the following to help advertise\n\nPSI /EFSPI (Martin),\nR Consortium / PHUSE / RSS (Lyn)\nIASCT (Harshal)\nASA (Min-hua may have contact or See if Ben has a contact- ACTION christina to check with ben then get back to Min-hua. Lily Hsieh to ask Leon as he’s part of ASA. Aiming can also reach out to a contact to see she has a contact )\nOthers : TBC\n\n\nCAMIS- ONCO: Soma Sekhar\nPlans are in progress\nReview of volunteer open roles Still looking for volunteers to do: - Co-ordinator for conference material - share standard slides/ content /abstracts /posters - Social media rep - to co-ordinate posts (linkedIn/Twitter) - Volunteers to represent CAMIS at various conferences\nConferences All to let Lyn know or update the conferences qmd if you want to attend and represent/advertise camis"
  },
  {
    "objectID": "minutes/posts/13Feb2023.html",
    "href": "minutes/posts/13Feb2023.html",
    "title": "White Paper and Demo of connecting Rstudio with Github repo",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\n\nattendees\n13_feb_2023\n\n\n\n\nAiming Yang\nYes\n\n\nBen Arancibia\nYes\n\n\nBrian Varney\nYes\n\n\nChristina Fillmore\nYes\n\n\nChelsea Dickens\nNo\n\n\nChi Zhang\nNo\n\n\nClara Beck\nNo\n\n\nAditee Dani\nNo\n\n\nDoug Kelkoff\nNo\n\n\nDhvani Patel\nNo\n\n\nFilip Kabaj\nNo\n\n\nHarshal Khanolkar\nYes\n\n\nIris Wu\nNo\n\n\nJayashree Vedanayagam\nYes\n\n\nJoe Rickert\nNo\n\n\nKyle Lee\nNo\n\n\nLeon Shi\nYes\n\n\nLily Hsieh\nYes\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nYes\n\n\nMichael Kane\nYes\n\n\nMichael Rimler\nYes\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nYes\n\n\nMolly MacDiarmid\nYes\n\n\nMona Mehraj\nNo\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nNo\n\n\nVandana Yadav\nNo\n\n\nVidya Gopal\nNo\n\n\nVikash Jain\nNo\n\n\nWilmar Igl\nNo\n\n\nOrla Doyle\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeeting minutes\nMin-Hua went through outstanding comments on the white paper.\nChristina did a demo of how to set up R studio to link through via git project to the CAMIS github repo. See “13Feb2023_Contributing to the CAMIS project_Setting up communication between github and R studio” for more information\n\n\nNext meeting: 13th March 2023: 4:30 UTC, 11:30 EST."
  },
  {
    "objectID": "minutes/posts/10July2023.html",
    "href": "minutes/posts/10July2023.html",
    "title": "Plan for Advertising CAMIS progress",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\n\nattendees\n10_july_2023\n\n\n\n\nAiming Yang\nNo\n\n\nBen Arancibia\nNo\n\n\nBrian Varney\nYes\n\n\nChristina Fillmore\nYes\n\n\nChelsea Dickens\nNo\n\n\nChi Zhang\nNo\n\n\nClara Beck\nNo\n\n\nAditee Dani\nNo\n\n\nDoug Kelkoff\nNo\n\n\nDhvani Patel\nNo\n\n\nFilip Kabaj\nNo\n\n\nHarshal Khanolkar\nYes\n\n\nIris Wu\nYes\n\n\nJayashree Vedanayagam\nYes\n\n\nJoe Rickert\nNo\n\n\nKyle Lee\nYes\n\n\nLeon Shi\nNo\n\n\nLily Hsieh\nYes\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nYes\n\n\nMichael Kane\nNo\n\n\nMichael Rimler\nNo\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nNo\n\n\nMolly MacDiarmid\nYes\n\n\nMona Mehraj\nNo\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nYes\n\n\nVandana Yadav\nYes\n\n\nVidya Gopal\nNo\n\n\nVikash Jain\nYes\n\n\nWilmar Igl\nNo\n\n\nOrla Doyle\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgenda\n\nWelcome new members - Vandana Yadav (Novo Nordisk) and Iris Wu (incyte)\nCAMIS Advertisement Plan - Harshal\n\nWhite paper\nMolly poster prize\nConference plan\n\nWebsite\n\nMMRM/ Other\n\nCAMIS-ONCO / Prep for PHUSE CSS- Soma Sekhar\nAOB\n\nNext meeting 14th Aug. Lyn on vacation, do you want to go ahead or move back 1 week?\n\n\n\n\nMeeting minutes\nCAMIS Advertisement Plan\n\nBlog re: White paper - All to share link with colleagues and like on social media,\nBlog re: Molly poster prize - ACTION: Molly to write\nLinkedIn -\n\nShould we have our own account (CAMIS linkedIN account) or post through the other organizations. Could post through our own personal accounts & have them repost?\nACTION : Harshal to post from own personal account and we will assesss reach if we all like it and ask the organizations to repost it.\n\nAny other social media sites needed to be used? Twitter was popular but not so now so stick with LinkedIn.\nSpread awareness though individual departments working groups\nPHUSE SDE connect - talk / presentation / posters or interactive workshops at connect SDEs.\nPHUSE/ FDA CSS 2024 - Working groups interactive breakout sessions. create a DVOST break out sessions - see CSS Working Groups (phuse-events.org)\n\nMaybe take inspiration from this years event to see what format they take\nACTION Soma to report back after conference and we can plan for next year\n\nSocial media post on CAMIS engagement at conferences\nUtilise #CAMIS on social media.\nPharmaverse - Could we get a link from Pharmaverse\n\nACTION: Christina to see if we can get a link?\n\n\n\nConference plan. The conference plan on the website was reviewed and updated\n\nPHUSE FDA quarterly meeting. CAMIS invited to present. Plan to go through White paper concepts & website. Request a replacement to represent FDA on our group, since Kyle Lee no longer at FDA. Promote use of site to them. Likely to be in attendance someone from the Division of analytics and informatics in CEDER.\nPHUSE CSS: Somar producing poster. Also doing a talk on validation of oncology endpoints and why it’s important to introduce hybrid programming languanges. Will be representatives present from regulatory authorities\n\n\nWebsite progress: Christina\n\nMMRM (Ben Arancibia happy to contribute)\nOthers - Not much new content in last month. Please if anyone time please add content !\nShould we prioritize any areas, or in getting Python/Julia content. Currently we will just see what content people have, rather than priorizing however re-assess based on growth to see if we need to focus more on a single area and get more volunteers on key areas.\nFAQ Doc - still in progress\n\nCAMIS- ONCO: Soma Sekhar\n\nStarted work on poster due July 28th.\nAfter presentation could convert to a white paper. ACTION: send out a draft plan for white paper, & lyn to add to agenda for discussion next meeting. Planning to evaluate endpoints for oncology. Table of endpoints required for approval.\n\nAOB\nNext meeting 14th Aug. Lyn on vacation, will move back 1 week."
  },
  {
    "objectID": "minutes/posts/19June2023.html",
    "href": "minutes/posts/19June2023.html",
    "title": "White Paper Finalization, Advertising CAMIS",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\n\nattendees\n19_june_2023\n\n\n\n\nAiming Yang\nYes\n\n\nBen Arancibia\nNo\n\n\nBrian Varney\nYes\n\n\nChristina Fillmore\nYes\n\n\nChelsea Dickens\nNo\n\n\nChi Zhang\nNo\n\n\nClara Beck\nNo\n\n\nAditee Dani\nNo\n\n\nDoug Kelkoff\nNo\n\n\nDhvani Patel\nNo\n\n\nFilip Kabaj\nNo\n\n\nHarshal Khanolkar\nYes\n\n\nIris Wu\nNo\n\n\nJayashree Vedanayagam\nNo\n\n\nJoe Rickert\nNo\n\n\nKyle Lee\nNo\n\n\nLeon Shi\nNo\n\n\nLily Hsieh\nNo\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nNo\n\n\nMichael Kane\nYes\n\n\nMichael Rimler\nNo\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nNo\n\n\nMolly MacDiarmid\nYes\n\n\nMona Mehraj\nNo\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nNo\n\n\nVandana Yadav\nNo\n\n\nVidya Gopal\nNo\n\n\nVikash Jain\nYes\n\n\nWilmar Igl\nYes\n\n\nOrla Doyle\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgenda\n\nTeam announcements - Lyn\nWhite paper - Lyn\nWebsite progress - Christina\nConferences feedback (PSI/IASCT)- Lyn/Christina/Martin/Molly/Aiming/Harshal\nConference planning - All\nCAMIS-ONCO - Soma Sekhar\nAOB\n\n\n\nMeeting minutes\nTeam Announcements\nMin-Hua Jen, Gave birth to twins -everyone is doing well !\nWelcome to: Iris Wu (Incyte)\nHarshal volunteered to be our Social Media Rep / co-ordinate our social media posts.\nRole consists of:\n\nHelps to come up for ideas for blogs/posts. Examples could include, if someone writes content on MRMM, you ask them to write a short description on what they’ve done including link to their work on the website, or for example, once the white paper is final, one of us will write a blog post to advertise it. Molly’s poster prize at PSI conference etc !\nChases people up who said they’d write a blog to ensure we are marketing our work in a timely manner.\nEnsures that when you receive content (blog/posts), that it’s sent to all the key contacts for organizations/Societies (See end of minutes for list)\n\nSocial media post list of upcoming posts\n1) Write a short message for Molly winning Poster prize. Just something in third person like “Congrats to Molly for winning a PSI Conference 2023 Poster Prize. If you want to see the poster it’s here.. (Molly to write, Lyn review, then send to Harshil/Christine to review - then Harshil to forward to contacts for advertising).\n2) Once White paper released, PHUSE will advertize but we should write something to share with our wider contacts\nWhite Paper Update No comments from Public review. Paper is here We checked for any last comments from the group. ACTION: Lyn to send confirmation that we are good to proceed to PHUSE tomorrow if no further comments.\nWebsite progress: Christina\nSubstantial content been added in the last month.\nOnce key area we’d like to progress on though is MMRM. ACTION: Christina to ask Kevin Kunzmann if he can write up something.\nLyn - To create a FAQ doc for the website. Make sure it references available material elsewhere so it doesn’t become out of date quickly.\nPSI AIMS will create github training which we can utilize to onboard statisticians.\nConference feedback:\nPSI Conference: Lyn/Christina/Martin/Molly/Aiming. Molly’s conference poster won a Poster Prize ! Need to blog/advertise the award.\nIASCT: Harshil. Spoken to board members, currently board going through election so will restart discussion after that.\nConference planning\nVikash- plan for CSS. Abstract has been submitted by Soma Sekhar (Co-author Vikash & Adittee), 30th june registration opens.\nACTION: Lyn: Ask PHUSE CSS working group (Data visualization and open source technology) DVOST - if we can have a presentation next year at the CSS.\nJSM - ASA conference. Leon. Abstract due Feb 2024 - so try have a rep there next year.\nPHUSE Single day event (SDE- Toronto Mississauga),\nSCT - society of clinical trials - Michael kane? (ACTION : Lyn to update conf website)\nSESUG - South Eastern SAS user group: Brian Varney (ACTION : Lyn to update conf website)\nPHUSE EU got a poster abstract: Jayashree Vendanayagam\nPHUSE Single day event (New york - regeneron hosting Oct 16, Aiming emailed host to see if she can do a poster/presentation/advert - Lyn to add to conf page)\nR in Pharma - Brian or Christina to possibly submit something. Nov virtual. POSIT CONF - September in chicago. (Lyn update website - Christina wont be at POSIT, split into 2 )\nPhuse EU connect : the CAMIS abstract was selected as back up talk only. However there are some companies who have company talks - which take priority so limited independent speakers to accept talks from. TBC if anyone on the list of speakers - could include a slide to advertise us at PHUSE EU!\nLessons learnt for conferences:\n1) Put PHUSE CAMIS on abstract (part of PHUSE DVOST).\n2) submit abstract for Poster & Talk - then you have the back up of a poster if talk is rejected.\nCAMIS- ONCO: Soma Sekhar\n\nNo update this month, carry to next month. Validation of endpoints (primary/secondary oncology endpoints). Propose to do poster at PHUSE CSS. Once Mia’s survival section is loaded. Sema Sekhar to review. Then highlight what’s missing - what else you want to add. Max combo. BICR vs RECIST? In future we can discuss how these fit with current CAMIS structure. Focus on the Stats method ideally. ACTION :Christina to email Semar Sekhar once Survival is live on Website.\n\nPrevious meeting notes/ Key Information\n\nContacts for Organizations/ Societies\n\nPSI /EFSPI (Martin)\nR Consortium / PHUSE / RSS (Lyn)\nIASCT (Harshal)\nASA (Leon) - TBC who are ASA to reach out to? If Ben has any contacts that Leon could use please let him know.\nSAS - Contact TBC.\n\nRoles\n\nLyn Taylor - Lead\nChristina Fillmore - Website/ co-lead\nSoma Sekhar Sriadibhatla- CAMIS ONCO\nHarshal Khanolkar - Social media rep\nLinear models team - Brian Varney, Vikash Jain,\nMMRM - Ben Arancibia / Kevin Kunzmann"
  },
  {
    "objectID": "minutes/posts/11Mar2024.html",
    "href": "minutes/posts/11Mar2024.html",
    "title": "How to select packages, Content & Conferences",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\n\nattendees\n11_Mar_24\n\n\n\n\nChristina Fillmore\nYes,\n\n\nLyn Taylor\nYes,\n\n\nMolly MacDiarmid\nYes,\n\n\nBrian Varney\nYes,\n\n\nChi Zhang\nNo,\n\n\nOrla Doyle\nNo,\n\n\nHarshal Khanolkar\nNo,\n\n\nLily Hseih\nYes,\n\n\nFilip Kabaj\nNo,\n\n\nMartin Brown\nYes,\n\n\nMin-Hua Jen\nNo,\n\n\nSarah Rathwell\nNo,\n\n\nKasa Andras\nYes,\n\n\nAditee Dani\nNo,\n\n\nKeaven Anderson\nYes,\n\n\nBenjamin Arancibia\nYes,\n\n\nWilmar Igl\nNo,\n\n\nVikash Jain\nYes,\n\n\nMia Qi\nNo,\n\n\nLeon Shi\nYes,\n\n\nVandaya Yadav\nNo,\n\n\nStephen McCawille\nYes,\n\n\nVikrant Vijay\nNo,\n\n\nVidya Gopal\nNo,\n\n\nDhvani Patel\nYes,\n\n\nKyle Lee\nNo,\n\n\nChelsea Dickens\nYes,\n\n\nDavid Bosak\nYes,\n\n\nMichael Kane\nYes,\n\n\nLukas Brausch\nYes,\n\n\nMichael Walshe\nYes,\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgenda & Minutes\n\nThank you to all those who submitted content this month, especially Chi, David and Filip who all helped to complete new sections. The top section is now almost complete and the first python content will be loaded in the next few weeks which is a great milestone for the project. Watch out for the new column appearing in the repository Table of contents!\nPlease remember even if you dont want to contribute to a section on your own, you can still review current content and propose improvements.\nConference planning. Reminder that if you are attending a conference to represent CAMIS to add the detail here. We need to ensure we continue to advertise the project to encourage people to use the repo and add content. So far in 2024, only 3 conferences being attended, so if you are interested in attending a conference just reach out to Lyn & Christina who can help you with an abstract if needed.\n\nVikash fed back about the PHUSE US Connect conference. CAMIS was mentioned by Michael Rimler in the keynote speech and Soma/Vikash presented a poster so we received great publicity. Brian also attended meeting Soma & Vikash face to face. Thank you to all of you. The abstract for PHUSE FDA CSS has been written by Soma and submitted so all on track.\nReminder to complete CAMIS membership form\nhttps://docs.google.com/forms/d/e/1FAIpQLSdDX79P5ByStVS_3n4tK1mAWidazIiF6DMEtDMK8KqmJywjqA/viewform?vc=0&c=0&w=1&flr=0&usp=mail_form_link\nNOTE:  We will only collect: team members name, email address, organization, software used, interested in oncology, key interests and affiliations to stats organizations.  The email address is solely for the CAMIS leadership team, to make sure you are included in CAMIS emails.\nWe ask on the form: “Are you happy for your Name and company and interests to be visible on the CAMIS website. Note that email addresses will not be visible”. \nIf you do not give permission then your name will not appear on the CAMIS repo as a CAMIS team member. If you do give permission, your name and company and interests will appear but your email addresses will be hidden from public view. At any time you can ask to be removed from the website team list by emailing me.\nSelection of packages: As we continue to grow the number of packages stored in the repository is growing. We realized that this may lead to conflicts and issues for the repo running. We also dont really want packages installed that are no longer used, known to have issues. Therefore if you are writing up an analysis and there are two packages doing similar things, we would like to request that you select the one that is the most commonly used and best quality (i.e. lowest risk). Risk can be assessed using the {riskmetric} package and {riskassessment} application, using the default scoring, but packages risk should also be considered in context of the individual components such as being actively maintained, bug fixes, code coverage, with references, with a github repo or website, by a trusted author and with results being correct vs stats method.\nIt can be very useful to test multiple packages if they are able to do slightly different analysis (i.e. with different options), in these cases it’s useful to include a Table at the top of the comparison summary qmd, to show which package does which analysis, see Comparison of 1 sample t-test as an example.\nPackages that are inferior to others, should not be loaded to the repo, but instead you can add a textual summary of your findings. For example, “Package X also gives the same results” or “Package X can be used but doesn’t have options to do X and Y” or “We do not recommend Package X as during testing, the results are not in line with the statistical methodology”.\nNOTE that we agreed not to have a library of packages “approved” for CAMIS, RENV stores the lock file of the packages in our repo and we do not want to be seen to giving recommendations for/against packages, other than factual evidence based on the analysis they produce.\nIt was noted that when you load the RENV.lock file, it may give a “error downloading” bioconductor warning, this can be ignored, and should not cause issue if you aren’t using these packages. In the future, these packages will be removed as dependancies from mmrm and the issue will resolve.\nAOB\n\nWe had a discussion surrounding communication methods currently used on the project. RE: using teams vs emails, feedback was mixed, Argument for Teams was that it keeps all discussion in one place, and doesn’t fill you in box, but arguments against were that as you often have to log out of your company teams, to log into the PHUSE one, messages are often missed / ignored.\nWe agreed to perhaps send 2 emails a month, the agenda, but also any other important updates that occurr during the month & minutes. This will be supported by also posting on social media. Although small sample, we assessed how many people observed the recent post RE: Soma’s poster (only Lyn & Christina of those on the call saw the post), however when asked re: Other PHUSE posts 7 were getting them. Leadership team to discuss and see if we need to post using PHUSE admin?"
  },
  {
    "objectID": "minutes/posts/12Feb2024.html",
    "href": "minutes/posts/12Feb2024.html",
    "title": "Website structure update, Team list, Conferences",
    "section": "",
    "text": "Attendees\n\n\n\n\n\n\n\n\n\n\n\n\nattendees\n12_Feb_24\n\n\n\n\nChristina Fillmore\nYes\n\n\nLyn Taylor\nYes\n\n\nMolly MacDiarmid\nYes\n\n\nBrian Varney\nYes\n\n\nChi Zhang\nYes\n\n\nOrla Doyle\nNo\n\n\nHarshal Khanolkar\nNo\n\n\nLily Hseih\nNo\n\n\nFilip Kabaj\nNo\n\n\nMartin Brown\nYes\n\n\nMin-Hua Jen\nNo\n\n\nSarah Rathwell\nYes\n\n\nKasa Andras\nNo\n\n\nAditee Dani\nYes\n\n\nKeaven Anderson\nYes\n\n\nBenjamin Arancibia\nYes\n\n\nWilmar Igl\nYes\n\n\nVikash Jain\nYes\n\n\nMia Qi\nYes\n\n\nLeon Shi\nYes\n\n\nVandaya Yadav\nYes\n\n\nStephen McCawille\nYes\n\n\nVikrant Vijay\nYes\n\n\nVidya Gopal\nYes\n\n\nDhvani Patel\nYes\n\n\nKyle Lee\nYes\n\n\nChelsea Dickens\nNo\n\n\nDavid Bosak\nNo\n\n\nMichael Kane\nNo\n\n\nLukas Brausch\nNo\n\n\nMichael Walshe\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgenda & Minutes\n\nUpdated website demo – Chi & Christina\nCAMIS membership form / data collection – Lyn\nhttps://docs.google.com/forms/d/e/1FAIpQLSdDX79P5ByStVS_3n4tK1mAWidazIiF6DMEtDMK8KqmJywjqA/viewform?vc=0&c=0&w=1&flr=0&usp=mail_form_link\nNOTE:  We will only collect: team members name, email address, organization, software used, interested in oncology, key interests and affiliations to stats organizations.  The email address is solely for the CAMIS leadership team, to make sure you are included in CAMIS emails.\nWe ask on the form: “Are you happy for your Name and company and interests to be visible on the CAMIS website. Note that email addresses will not be visible”. \nIf you do not give permission then your name will not appear on the CAMIS repo as a CAMIS team member. If you do give permission, your name and company and interests will appear but your email addresses will be hidden from public view. At any time you can ask to be removed from the website team list by emailing me.\nPhuse css workshop for 2024 / CAMIS ONCO– Soma/Vikash/Harshil\n\nFilip, Lyn & Chrstina met re: Python content\nNext steps for workshop & white paper\n\nOther conferences\n\nKeaven - attending JSM & ISBN - and will mention CAMIS.\nChi attending use R conference, Lyn/Christina/All: please review abstract if you wish.\nRegulatory stats workshop: Leon Shih (poster)\n\nVolunteers requested for:\n\nOpenstatsware Bayesian MMRM  {brms.mmrm} package input :  Christine/Orla\nMMRM - Volunteer please to look at Proc Mixed vs Proc GLIMMIX for the SAS/mmrm.qmd file (Stephen McCawille & Leon Shi may be able to look at this in future ).\n\nAOB\n\nChristina: Create a 1 slide - This is CAMIS.\nLyn: Load up Dec2023 blog post\nChi: Add links to blogs & add blog tab when we have content"
  },
  {
    "objectID": "minutes/posts/23Jan2023.html",
    "href": "minutes/posts/23Jan2023.html",
    "title": "New Website Discussion",
    "section": "",
    "text": "attendees\n23_Jan_2023\n\n\n\n\nAiming Yang\nYes\n\n\nBen Arancibia\nYes\n\n\nBrian Varney\nYes\n\n\nChristina Fillmore\nYes\n\n\nChelsea Dickens\nNo\n\n\nChi Zhang\nNo\n\n\nClara Beck\nNo\n\n\nAditee Dani\nNo\n\n\nDoug Kelkoff\nNo\n\n\nDhvani Patel\nNo\n\n\nFilip Kabaj\nNo\n\n\nHarshal Khanolkar\nNo\n\n\nIris Wu\nNo\n\n\nJayashree Vedanayagam\nYes\n\n\nJoe Rickert\nYes\n\n\nKyle Lee\nYes\n\n\nLeon Shi\nYes\n\n\nLily Hsieh\nYes\n\n\nLyn Taylor\nYes\n\n\nMartin Brown\nYes\n\n\nMia Qi\nYes\n\n\nMichael Kane\nNo\n\n\nMichael Rimler\nYes\n\n\nMike Stackhouse\nNo\n\n\nMin-Hua Jen\nYes\n\n\nMolly MacDiarmid\nYes\n\n\nMona Mehraj\nNo\n\n\nPaula Rowley\nNo\n\n\nSoma Sekhar Sriadibhatla\nYes\n\n\nVandana Yadav\nNo\n\n\nVidya Gopal\nNo\n\n\nVikash Jain\nNo\n\n\nWilmar Igl\nNo\n\n\nOrla Doyle\nNo"
  },
  {
    "objectID": "minutes/posts/23Jan2023.html#christina-provided-a-summary-of-work-to-date-on-the-website",
    "href": "minutes/posts/23Jan2023.html#christina-provided-a-summary-of-work-to-date-on-the-website",
    "title": "New Website Discussion",
    "section": "Christina provided a summary of work to date on the website",
    "text": "Christina provided a summary of work to date on the website\nRepo now live: [https://psiaims.github.io/CAMIS/]\nPrimary mode of navigation will be the table of contents..\nComprehensive Search function is available to supplement the use of the TOC.\nThe website is build from 3 folders in github:\nR SAS Comp\nThese folders, map to the columns of the table, I.e. everything about R is in Quarto files under R.\nComp folder: for the Comparison – name sure you name the two software you are using r-sas - so we can use this when dynamically selecting.\nIn future we can add Python / Julia directories.\nThe idea would be for people to use the: [CAMIS/templates/R_template.qmd] - A template of how to write documentation for the R part of the site. They’d Edit template & save it back into the R folder naming it clearly for what it is. Template should also contain name packages being used at start of each method comparison. It’d be difficult to be exhaustive with all the survival analysis packages i.e. accelerated failure time packages, etc.., but as long as stated hopefully can grow over time.\nThe Data-info folder – contains description of all data being used for the comparisons. Going forward if different data used, the information about the data would be put into this folder. This allows the data description to sit outside of the comparison folders & where possible same data be used across comparisons.\n\nQuestions & Discussion\nJoe & Michael raised that the About tab which has information about the project is out of date, so should be updated. We also have no detail on the driving mechanism… I.e. what we would like from collaborators. Add “How to collaborate” button.\nItems to be discussed further which may need to be included in the site:\n\nupdate Methods: needs to make it more robust to future uploads - i.e. topics within linear models? (Sub categories) focus on methods, but how sort the methods for inclusion of all in future\nRating the software discrepancies. I..e How severe the difference is?\nNeed to create a template for comparisons. Discuss if we would have a purpose/highlight of comparison/ summary/conclusions at the top first. Also if we put List of R packages this comparison uses (use Tags?) - Need to consider if package superseeded/ multiple packages whether they go in 1 document or multiple.\nHow to expand to sort by: therapeutic area relevance (would be good to link from methods to Oncology somehow\nWhat if a different package.., does same analysis… have to make it clear which package is being used & include multiple packages. It was agreed that as long as we are clear on what we have compared then Its ok to not be all inclusive. That can be added by other collaborators later. It was noted by Kyle that for survival (I.e. accelerated failure time packages), it may be hard to include all. The recommendation is to start with 1 and can expand further as it grows. We may have to re-think website design as it grows to accommodate. Hence why we want everything written in smaller parts to can easily manipulate going forward."
  },
  {
    "objectID": "minutes/posts/23Jan2023.html#min-hua-provided-an-update-on-the-white-paper",
    "href": "minutes/posts/23Jan2023.html#min-hua-provided-an-update-on-the-white-paper",
    "title": "New Website Discussion",
    "section": "Min-Hua provided an update on the white paper:",
    "text": "Min-Hua provided an update on the white paper:\nIn its final stages of review by team, and will now be sent for wider review."
  },
  {
    "objectID": "Comp/r-sas_ancova.html",
    "href": "Comp/r-sas_ancova.html",
    "title": "R vs SAS ANCOVA",
    "section": "",
    "text": "The following table shows the types of One Sample t-test analysis, the capabilities of each language, and whether or not the results from each language match.\n\n\n\nAnalysis\nSupported in R\nSupported in SAS\nResults Match\nNotes\n\n\n\n\nANCOVA using general linear model and lsmeans\nYes\nYes\nYes\nGLM() function from sasLM with EMEANS=TRUE is the easiest to use and matches SAS\n\n\n\n\n\nHere is a table of comparison values between lm() from the stats package, GLM() from the sasLM package, and SAS PROC GLM:\n\n\n\n\n\n\n\n\n\n\n\nStatistic\nlm()\nGLM()\nPROC GLM\nMatch\nNotes\n\n\n\n\nType I, Sum sq, drug\n293.6000\n293.6000\n293.6000\nYes\n\n\n\nType I, Sum sq, pre\n577.897\n577.8974\n577.8974\nYes\n\n\n\nType III, Sum sq, drug\n68.554\n68.55371\n68.55371\nYes\n\n\n\nType III, Sum sq, pre\n577.897\n577.89740\n577.89740\nYes\n\n\n\nLSmean drugA\n6.71\n6.714963\n6.714963\nYes\n\n\n\nLSmean drugD\n6.82\n6.823935\n6.823935\nYes\n\n\n\nLSmean drugF\n10.16\n10.161102\n10.161102\nYes"
  },
  {
    "objectID": "Comp/r-sas_ancova.html#comp",
    "href": "Comp/r-sas_ancova.html#comp",
    "title": "R vs SAS ANCOVA",
    "section": "",
    "text": "Here is a table of comparison values between lm() from the stats package, GLM() from the sasLM package, and SAS PROC GLM:\n\n\n\n\n\n\n\n\n\n\n\nStatistic\nlm()\nGLM()\nPROC GLM\nMatch\nNotes\n\n\n\n\nType I, Sum sq, drug\n293.6000\n293.6000\n293.6000\nYes\n\n\n\nType I, Sum sq, pre\n577.897\n577.8974\n577.8974\nYes\n\n\n\nType III, Sum sq, drug\n68.554\n68.55371\n68.55371\nYes\n\n\n\nType III, Sum sq, pre\n577.897\n577.89740\n577.89740\nYes\n\n\n\nLSmean drugA\n6.71\n6.714963\n6.714963\nYes\n\n\n\nLSmean drugD\n6.82\n6.823935\n6.823935\nYes\n\n\n\nLSmean drugF\n10.16\n10.161102\n10.161102\nYes"
  },
  {
    "objectID": "Comp/r-sas_chi-sq.html",
    "href": "Comp/r-sas_chi-sq.html",
    "title": "R/SAS Chi-Squared Comparision",
    "section": "",
    "text": "Chi-Squared test is a hypothesis test for independent contingency tables, dependent on rows and column totals. The test assumes:\n\nobservations are independent of each other\nall values are 1 or more and at least 80% of the cells are greater than 5.\ndata should be categorical\n\nThe Chi-Squared statistic is found by:\n\\[\n\\chi^2=\\frac{\\sum(O-E)^2}{E}\n\\]\nWhere O is the observed and E is the expected.\nFor an r x c table (where r is the number of rows and c the number of columns), the Chi-squared distribution’s degrees of freedom is (r-1)*(c-1). The resultant statistic with correct degrees of freedom follows this distribution when its expected values are aligned with the assumptions of the test, under the null hypothesis. The resultant p value informs the magnitude of disagreement with the null hypothesis and not the magnitude of association\nFor this example we will use data about cough symptoms and history of bronchitis.\n\nbronch &lt;- matrix(c(26, 247, 44, 1002), ncol = 2)\nrow.names(bronch) &lt;- c(\"cough\", \"no cough\")\ncolnames(bronch) &lt;- c(\"bronchitis\", \"no bronchitis\")\n\nTo a chi-squared test in R you will use the following code.\n\nchisq.test(bronch)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  bronch\nX-squared = 11.145, df = 1, p-value = 0.0008424\n\n\nTo run a chi-squared test in SAS you used the following code.\n\nproc freq data=proj1.bronchitis;\ntables Cough*Bronchitis / chisq;\nrun;\n\nThe result in the “Chi-Square” section of the results table in SAS will not match R, in this case it will be 12.1804 with a p-value of 0.0005. This is because by default R does a Yate’s continuity adjustment. To change this set correct to false.\n\nchisq.test(bronch, correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  bronch\nX-squared = 12.18, df = 1, p-value = 0.0004829\n\n\nAlternatively, SAS also produces the correct chi-square value by default. It is the “Continuity Adj. Chi-Square” value in the results table."
  },
  {
    "objectID": "Comp/r-sas_manova.html",
    "href": "Comp/r-sas_manova.html",
    "title": "Multivariate Analysis of Variance in R vs SAS",
    "section": "",
    "text": "MANOVA: Testing for group mean vectors are the same vs at least one is different\nWhen applying the following hypothesis, SAS and R match identically see R and SAS.\n\nH0: Group mean vectors are the same for all groups or they don’t differ significantly.\nH1: At least one of the group mean vectors is different from the rest.\n\nHowever, if interest was in comparing 1 level of a parameter vs the others, this was only achieved using SAS. Contrast statements in SAS were easy to implement as shown here SAS however R did not replicate these results and to date a solution has not been found.\nNOTE: if you feel you can help with the above discrepancy please contribute to the CAMIS repo by following the instructions on the contributions page."
  },
  {
    "objectID": "Comp/r-sas_anova.html",
    "href": "Comp/r-sas_anova.html",
    "title": "R vs SAS Linear Models",
    "section": "",
    "text": "Matching Contrasts: R and SAS\nIt is recommended to use the emmeans package when attempting to match contrasts between R and SAS. In SAS, all contrasts must be manually defined, whereas in R, we have many ways to use pre-existing contrast definitions. The emmeans package makes simplifies this process, and provides syntax that is similar to the syntax of SAS.\nThis is how we would define a contrast in SAS.\n\n# In SAS\nproc glm data=work.mycsv;\n   class drug;\n   model post = drug pre / solution;\n   estimate 'C vs A'  drug -1  1 0;\n   estimate 'E vs CA' drug -1 -1 2;\nrun;\n\nAnd this is how we would define the same contrast in R, using the emmeans package.\n\nlm(formula = post ~ pre + drug, data = df_trial) %&gt;% \n  emmeans(\"drug\") %&gt;% \n  contrast(method = list(\n    \"C vs A\"  = c(-1,  1, 0),\n    \"E vs CA\" = c(-1, -1, 2)\n  ))\n\nNote, however, that there are some cases where the scale of the parameter estimates between SAS and R is off, though the test statistics and p-values are identical. In these cases, we can adjust the SAS code to include a divisor. As far as we can tell, this difference only occurs when using the predefined Base R contrast methods like contr.helmert.\n\nproc glm data=work.mycsv;\n   class drug;\n   model post = drug pre / solution;\n   estimate 'C vs A'  drug -1  1 0 / divisor = 2;\n   estimate 'E vs CA' drug -1 -1 2 / divisor = 6;\nrun;"
  },
  {
    "objectID": "Comp/r-sas_survival.html",
    "href": "Comp/r-sas_survival.html",
    "title": "R vs SAS - Kaplan Meier and Cox-proportion hazards modelling",
    "section": "",
    "text": "The following table shows the options available in SAS and R for Kaplan Meier and Cox Proportional Hazards modelling, the capabilities of each language, and whether or not the results from each language match.\n\n\n\n\n\n\n\n\n\n\nAnalysis\nSupported in R\nSupported in SAS\nResults Match\nNotes\n\n\n\n\nKaplan Meier with confidence intervals using log-log method\nYes (using the option conf.type = “log-log”)\nYes (Default)\nMostly\n1) Survival estimates can disagree when last event is censored and survival estimate does not cross the percentile being estimated.\n2) Survival estimates at time X can disagree when the time X is after the last observed censored time\n\n\nKaplan Meier with confidence intervals using log method\nYes (Default)\nYes (using the option conftype=log)\nMostly\nAs above.\n\n\nCox Proportional Hazards Model using breslow method for ties\nYes (using the option ties=“breslow”)\nYes (Default)\nYes\n\n\n\nCox Proportional Hazards Model using efron method for ties\nYes (Default)\nYes (using the option ties=efron)\nYes\n\n\n\n\nResults from the examples shown for R here and SAS here were compared below.\nComparing the non-stratified model results side-by-side, the CIs for the quartile estimates and landmark estimates are different between R and SAS. HR and CI also have slight differences.\n\n\n\n\n\n\n\n\n\n\n\nThe default methods for handling ties in a Cox regression model are different which can lead to a different result for the Hazard ratio and associated confidence interval.\nR uses “efron” by default. SAS uses “breslow” by default. Both R and SAS are able to change these default options. By making the changes to the code below, we can force R to use “breslow” to match SAS, or SAS to use “efron” to match R. When the software use the same methods, then we obtain an identical HR and CI.\n\nR: change method for ties to use “breslow”\n\n\nfit.cox &lt;- coxph(Surv(LENFOLY, FSTAT) ~ AFB, ties = \"breslow\", data = dat)\n\n\nSAS: change method for ties to use “efron”\n\n\nproc phreg data=dat;\nclass afb;\nmodel lenfol*fstat(0) = afb/rl ties = efron;\nrun;\n\nIf there are no tied event times, then the methods are equivalent.\nThe Breslow approximation is the easiest to program and hence it historically became the first option coded for almost all software. It then ended up as the default option when other options were added in order to maintain “backwards compatibility”. The Efron option is more accurate if there are a large number of ties, and it was therefore selected as the default option in R. In practice the number of ties is usually small, in which case all the methods are statistically indistinguishable.\nFrom the arguments of coxph in R, there are three possible choices for handling tied event times ‘ties=breslow’, ‘ties=efron’, or ‘ties=’clogit’. This last option is an exact partial likelihood approach. See here for more detail.\n\n\n\nThe default methods for calculation of the confidence interval of a KM estimator are different in the two languages (for example, for calculation of the CI associated with the Median Survival estimate, the 25th percentile and the 75th percentile).\nR uses “log” by default, and SAS uses “log-log” by default. As shown below, using ‘conf.type’ option, R can be forced to use the “log-log” method to match SAS. Alternatively, using the ‘conftype=’ option, SAS can be forced to use the “log” method to match R.\n\nR: change to “log-log”\n\n\nfit.km &lt;- survfit(Surv(LENFOLY, FSTAT) ~ AFB, conf.type = \"log-log\", data = dat)\n\n\nSAS: change to “log”\n\n\nproc lifetest data=dat conftype=log;\ntime lenfoly*fstat(0);\nstrata afb;\nrun;\n\n“log-log” prevents the problem of having confidence intervals of &gt;1 or &lt;0, which might happen if using “log” transformation. However, both R and SAS will clip the interval at [0, 1] and report a bound &gt;1 as 1 and &lt;0 as 0.\nFrom a reference: The appeal of the log-log interval is clear, but the log-scale interval has the advantage of variance stabilization. As a result, simulation studies have generally found it to have better (closer to nominal) coverage; for this reason, it is the default in the survival package.\nNow if we change the confidence interval type in SAS to “log” and tie handling to “efron”, the results will be identical to the results in R.\n\n\n\n\n\n\n\n\n\nBelow is the side-by-side comparison for stratified analysis with default methods in SAS matched to R’s, the results are also identical."
  },
  {
    "objectID": "Comp/r-sas_survival.html#reason-1-cox-regression-handling-of-tied-survival-times",
    "href": "Comp/r-sas_survival.html#reason-1-cox-regression-handling-of-tied-survival-times",
    "title": "R vs SAS - Kaplan Meier and Cox-proportion hazards modelling",
    "section": "",
    "text": "The default methods for handling ties in a Cox regression model are different which can lead to a different result for the Hazard ratio and associated confidence interval.\nR uses “efron” by default. SAS uses “breslow” by default. Both R and SAS are able to change these default options. By making the changes to the code below, we can force R to use “breslow” to match SAS, or SAS to use “efron” to match R. When the software use the same methods, then we obtain an identical HR and CI.\n\nR: change method for ties to use “breslow”\n\n\nfit.cox &lt;- coxph(Surv(LENFOLY, FSTAT) ~ AFB, ties = \"breslow\", data = dat)\n\n\nSAS: change method for ties to use “efron”\n\n\nproc phreg data=dat;\nclass afb;\nmodel lenfol*fstat(0) = afb/rl ties = efron;\nrun;\n\nIf there are no tied event times, then the methods are equivalent.\nThe Breslow approximation is the easiest to program and hence it historically became the first option coded for almost all software. It then ended up as the default option when other options were added in order to maintain “backwards compatibility”. The Efron option is more accurate if there are a large number of ties, and it was therefore selected as the default option in R. In practice the number of ties is usually small, in which case all the methods are statistically indistinguishable.\nFrom the arguments of coxph in R, there are three possible choices for handling tied event times ‘ties=breslow’, ‘ties=efron’, or ‘ties=’clogit’. This last option is an exact partial likelihood approach. See here for more detail."
  },
  {
    "objectID": "Comp/r-sas_survival.html#reason-2-kaplan-meier-median-survival-confidence-intervals",
    "href": "Comp/r-sas_survival.html#reason-2-kaplan-meier-median-survival-confidence-intervals",
    "title": "R vs SAS - Kaplan Meier and Cox-proportion hazards modelling",
    "section": "",
    "text": "The default methods for calculation of the confidence interval of a KM estimator are different in the two languages (for example, for calculation of the CI associated with the Median Survival estimate, the 25th percentile and the 75th percentile).\nR uses “log” by default, and SAS uses “log-log” by default. As shown below, using ‘conf.type’ option, R can be forced to use the “log-log” method to match SAS. Alternatively, using the ‘conftype=’ option, SAS can be forced to use the “log” method to match R.\n\nR: change to “log-log”\n\n\nfit.km &lt;- survfit(Surv(LENFOLY, FSTAT) ~ AFB, conf.type = \"log-log\", data = dat)\n\n\nSAS: change to “log”\n\n\nproc lifetest data=dat conftype=log;\ntime lenfoly*fstat(0);\nstrata afb;\nrun;\n\n“log-log” prevents the problem of having confidence intervals of &gt;1 or &lt;0, which might happen if using “log” transformation. However, both R and SAS will clip the interval at [0, 1] and report a bound &gt;1 as 1 and &lt;0 as 0.\nFrom a reference: The appeal of the log-log interval is clear, but the log-scale interval has the advantage of variance stabilization. As a result, simulation studies have generally found it to have better (closer to nominal) coverage; for this reason, it is the default in the survival package.\nNow if we change the confidence interval type in SAS to “log” and tie handling to “efron”, the results will be identical to the results in R.\n\n\n\n\n\n\n\n\n\nBelow is the side-by-side comparison for stratified analysis with default methods in SAS matched to R’s, the results are also identical."
  },
  {
    "objectID": "Comp/r-sas_survival.html#differences-observed-in-the-km-estimators",
    "href": "Comp/r-sas_survival.html#differences-observed-in-the-km-estimators",
    "title": "R vs SAS - Kaplan Meier and Cox-proportion hazards modelling",
    "section": "Differences Observed in the KM Estimators",
    "text": "Differences Observed in the KM Estimators\nSuppose we are interested to know the 25%, 50% and 75% quartile estimates, and the day 80, 100, and 120 estimates.\nBelow is the R code:\n\nfit.km &lt;- survfit(Surv(time, status) ~ 1, conf.type = \"log-log\", data = test)\n## quantile estimates\nquantile(fit.km, probs = c(0.25, 0.5, 0.75))\n## landmark estimates at 80, 100, 120-day\nsummary(fit.km, times = c(80, 100, 120), extend = T)\n\nBelow is the SAS code:\n\nproc lifetest data=dat outsurv=_SurvEst timelist= 80 100 120 reduceout stderr; \ntime lenfoly*fstat(0);\nrun;\n\nBelow is the side-by-side comparison:"
  },
  {
    "objectID": "Comp/r-sas_survival.html#reasons",
    "href": "Comp/r-sas_survival.html#reasons",
    "title": "R vs SAS - Kaplan Meier and Cox-proportion hazards modelling",
    "section": "Reasons",
    "text": "Reasons\nThe reasons for the differences are because:\nReason 1: Survival estimate does not cross the 50% percentile.\nThe kth quantile for a survival curve S(t) is the location at which a horizontal line at height p= 1-k intersects the plot of S(t) as shown in the KM curve below. Since S(t) is a step function, it is possible for the curve to have a horizontal segment at exactly 1-k, in which case the midpoint of the horizontal segment is returned.\nFor example, using the data above, the survival probability is exactly 0.5 at time=87 and remains at 0.5 until the last censored observation at 118.\n\n\n\n\n\n\n\n\n\nWhen using R, the median is the smallest time which survival estimate is &lt;= 0.5 –&gt; (87+118) / 2 = 102.5. However, SAS searches the smallest time which survival estimate is &lt; 0.5, which does not exist in this dataset, so it gives “NE” (Not evaluable).\n\npl &lt;- survminer::ggsurvplot(fit.km, \n                            conf.int = TRUE,\n                            ggtheme = theme_light()) \npl$plot +  geom_hline(yintercept = 0.5, color = \"black\", linetype = \"solid\")  \n\nsummary(fit.km)\n\nReason 2: Last event censored and prior to the required landmark estimate.\nFor the 120-day event-free estimate, SAS considers that 120 days is beyond the maximum observed day in the data (which was a censored event at time =118). Therefore, SAS considers this as Unknown and returns a result of “NE” (Not-evaluable). However, R uses the rate at last observed censored date to estimate the 120-day event free rate. As the event-free estimate at time of the last censored event at 118 was 0.5 (0.184, 0.753), R makes the assumption that this is the best estimate for the event-free rate at Time =120.\nIf we change the last observation in the dataset to be an event (instead of censored), R and SAS will both give 0 for the event-free survival estimate, because it is for sure that all subjects did not survive beyond 120 days.\n\ntest &lt;- tibble(time = c(54, 75, 77, 84, 87, 92, 103, 105, 112, 118), \n                   status = c(1, 1, 1, 1, 1, 0, 0, 0, 0, 1))\n\ntest\n\n# A tibble: 10 × 2\n    time status\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1    54      1\n 2    75      1\n 3    77      1\n 4    84      1\n 5    87      1\n 6    92      0\n 7   103      0\n 8   105      0\n 9   112      0\n10   118      1"
  },
  {
    "objectID": "Comp/r-sas_ttest_1Sample.html",
    "href": "Comp/r-sas_ttest_1Sample.html",
    "title": "R vs SAS One Sample T-Test",
    "section": "",
    "text": "The following table shows the types of One Sample t-test analysis, the capabilities of each language, and whether or not the results from each language match.\n\n\n\nAnalysis\nSupported in R\nSupported in SAS\nResults Match\nNotes\n\n\n\n\nOne sample t-test, normal data\nYes\nYes\nYes\nIn Base R, use mu parameter on t.test() function to set null hypothesis value\n\n\nOne sample t-test, lognormal data\nMaybe\nYes\nNA\nMay be supported by envstats package\n\n\n\n\n\n\n\nHere is a table of comparison values between t.test(), proc_ttest(), and SAS PROC TTEST:\n\n\n\n\n\n\n\n\n\n\n\nStatistic\nt.test()\nproc_ttest()\nPROC TTEST\nMatch\nNotes\n\n\n\n\nDegrees of Freedom\n29\n29\n29\nYes\n\n\n\nt value\n2.364306\n2.364306\n2.364306\nYes\n\n\n\np value\n0.0249741\n0.0249741\n0.0249741\nYes\n\n\n\n\n\n\n\nSince there is currently no known support for lognormal t-test in R, this comparison is not applicable."
  },
  {
    "objectID": "Comp/r-sas_ttest_1Sample.html#comparison-results",
    "href": "Comp/r-sas_ttest_1Sample.html#comparison-results",
    "title": "R vs SAS One Sample T-Test",
    "section": "",
    "text": "Here is a table of comparison values between t.test(), proc_ttest(), and SAS PROC TTEST:\n\n\n\n\n\n\n\n\n\n\n\nStatistic\nt.test()\nproc_ttest()\nPROC TTEST\nMatch\nNotes\n\n\n\n\nDegrees of Freedom\n29\n29\n29\nYes\n\n\n\nt value\n2.364306\n2.364306\n2.364306\nYes\n\n\n\np value\n0.0249741\n0.0249741\n0.0249741\nYes\n\n\n\n\n\n\n\nSince there is currently no known support for lognormal t-test in R, this comparison is not applicable."
  },
  {
    "objectID": "Comp/r-sas-summary-stats.html",
    "href": "Comp/r-sas-summary-stats.html",
    "title": "Deriving Quantiles or Percentiles in R vs SAS",
    "section": "",
    "text": "Data\nThe following data will be used show the differences between the default percentile definitions used by SAS and R:\n\n10, 20, 30, 40, 150, 160, 170, 180, 190, 200\n\n\n\nSAS Code\nAssuming the data above is stored in the variable aval within the dataset adlb, the 25th and 40th percentiles could be calculated using the following code.\n\nproc univariate data=adlb;\n  var aval;\n  output out=stats pctlpts=25 40 pctlpre=p;\nrun;\n\nThis procedure creates the dataset stats containing the variables p25 and p40.\n\n\n\n\n\n\n\n\n\nThe procedure has the option PCTLDEF which allows for five different percentile definitions to be used. The default is PCTLDEF=5.\n\n\nR code\nThe 25th and 40th percentiles of aval can be calculated using the quantile function.\n\nquantile(adlb$aval, probs = c(0.25, 0.4))\n\nThis gives the following output.\n\n\n  25%   40% \n 32.5 106.0 \n\n\nThe function has the argument type which allows for nine different percentile definitions to be used. The default is type = 7.\n\n\nComparison\nThe default percentile definition used by the UNIVARIATE procedure in SAS finds the 25th and 40th percentiles to be 30 and 95. The default definition used by R finds these percentiles to be 32.5 and 106.\nIt is possible to get the quantile function in R to use the same definition as the default used in SAS, by specifying type=2.\n\nquantile(adlb$aval, probs = c(0.25, 0.4), type=2)\n\nThis gives the following output.\n\n\n25% 40% \n 30  95 \n\n\nIt is not possible to get the UNIVARIATE procedure in SAS to use the same definition as the default used in R.\nRick Wicklin provided a blog post showing how SAS has built in support for calculations using 5 of the 9 percentile definitions available in R, and also demonstrated how you can use a SAS/IML function to calculate percentiles using the other 4 definitions.\nMore information about quantile derivation can be found in the SAS blog.\n\n\nKey references:\nCompare the default definitions for sample quantiles in SAS, R, and Python\nSample quantiles: A comparison of 9 definitions\nHyndman, R. J., & Fan, Y. (1996). Sample quantiles in statistical packages. The American Statistician, 50(4), 361-365."
  },
  {
    "objectID": "Comp/r-sas_cmh.html",
    "href": "Comp/r-sas_cmh.html",
    "title": "R vs SAS CMH",
    "section": "",
    "text": "The CMH procedure tests for conditional independence in partial contingency tables for a 2 x 2 x K design. However, it can be generalized to tables of X x Y x K dimensions.\n\n\n\n\n\n\n\n\n\n\n\nFor the remainder of this document, we adopt the following naming convention when referring to variables of a contingency table:\n\nX = exposure\nY = response\nK = control\n\n\n\n\nThe scale of the exposure (X) and response (Y) variables dictate which test statistic is computed for the contingency table. Each test statistic is evaluated on different degrees of freedom (df):\n\nGeneral association statistic (X and Y both nominal) results in (X-1) * (Y-1) dfs\nRow mean scores statistic (X is nominal and Y is ordinal) results in X-1 dfs\nNonzero correlation statistic (X and Y both ordinal) results in 1 df"
  },
  {
    "objectID": "Comp/r-sas_cmh.html#naming-convention",
    "href": "Comp/r-sas_cmh.html#naming-convention",
    "title": "R vs SAS CMH",
    "section": "",
    "text": "For the remainder of this document, we adopt the following naming convention when referring to variables of a contingency table:\n\nX = exposure\nY = response\nK = control"
  },
  {
    "objectID": "Comp/r-sas_cmh.html#scale",
    "href": "Comp/r-sas_cmh.html#scale",
    "title": "R vs SAS CMH",
    "section": "",
    "text": "The scale of the exposure (X) and response (Y) variables dictate which test statistic is computed for the contingency table. Each test statistic is evaluated on different degrees of freedom (df):\n\nGeneral association statistic (X and Y both nominal) results in (X-1) * (Y-1) dfs\nRow mean scores statistic (X is nominal and Y is ordinal) results in X-1 dfs\nNonzero correlation statistic (X and Y both ordinal) results in 1 df"
  },
  {
    "objectID": "Comp/r-sas_cmh.html#data",
    "href": "Comp/r-sas_cmh.html#data",
    "title": "R vs SAS CMH",
    "section": "Data",
    "text": "Data\nTo begin investigating the differences in the SAS and R implementations of the CMH test, we decided to use the CDISC Pilot data set, which is publicly available on the PHUSE Test Data Factory repository. We applied very basic filtering conditions upfront (see below) and this data set served as the basis of the examples to follow.\n\n\n# A tibble: 6 × 36\n  STUDYID  SITEID SITEGR1 USUBJID TRTSDT     TRTEDT     TRTP  TRTPN   AGE AGEGR1\n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;date&gt;     &lt;date&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n1 CDISCPI… 701    701     01-701… 2014-01-02 2014-07-02 Plac…     0    63 &lt;65   \n2 CDISCPI… 701    701     01-701… 2012-08-05 2012-09-01 Plac…     0    64 &lt;65   \n3 CDISCPI… 701    701     01-701… 2013-07-19 2014-01-14 Xano…    81    71 65-80 \n4 CDISCPI… 701    701     01-701… 2014-03-18 2014-03-31 Xano…    54    74 65-80 \n5 CDISCPI… 701    701     01-701… 2014-07-01 2014-12-30 Xano…    81    77 65-80 \n6 CDISCPI… 701    701     01-701… 2013-02-12 2013-03-09 Plac…     0    85 &gt;80   \n# ℹ 26 more variables: AGEGR1N &lt;dbl&gt;, RACE &lt;chr&gt;, RACEN &lt;dbl&gt;, SEX &lt;chr&gt;,\n#   ITTFL &lt;chr&gt;, EFFFL &lt;chr&gt;, COMP24FL &lt;chr&gt;, AVISIT &lt;chr&gt;, AVISITN &lt;dbl&gt;,\n#   VISIT &lt;chr&gt;, VISITNUM &lt;dbl&gt;, ADY &lt;dbl&gt;, ADT &lt;date&gt;, PARAMCD &lt;chr&gt;,\n#   PARAM &lt;chr&gt;, PARAMN &lt;dbl&gt;, AVAL &lt;dbl&gt;, ANL01FL &lt;chr&gt;, DTYPE &lt;chr&gt;,\n#   AWRANGE &lt;chr&gt;, AWTARGET &lt;dbl&gt;, AWTDIFF &lt;dbl&gt;, AWLO &lt;dbl&gt;, AWHI &lt;dbl&gt;,\n#   AWU &lt;chr&gt;, QSSEQ &lt;dbl&gt;"
  },
  {
    "objectID": "Comp/r-sas_cmh.html#schemes",
    "href": "Comp/r-sas_cmh.html#schemes",
    "title": "R vs SAS CMH",
    "section": "Schemes",
    "text": "Schemes\nIn order to follow a systematic approach to testing, and to cover variations in the CMH test, we considered the traditional 2 x 2 x K design as well as scenarios where the generalized CMH test is employed (e.g. 5 x 3 x 3).\nWe present 5 archetype test scenarios that illustrate diverging results, possibly related to sparse data and possibly considered edge cases.\n\n\n\n\n\n\n\n\n\n\nNumber\nSchema\nVariables\nRelevant Test\nDescription\n\n\n\n\n1\n2x2x2\nX = TRTP, Y = SEX, K = AGEGR1\nGeneral Association\nTRTP and AGEGR1 were limited to two categories, overall the the groups were rather balanced\n\n\n3\n2x2x3\nX = TRTP, Y = SEX, K = RACE\nGeneral Association\nGives back NaN in R because RACE is very imbalanced\n\n\n6\n2x5x2\nX = TRTP, Y = AVAL, K = SEX\nRow Means\nCompare Row Means results for R and SAS because Y is ordinal\n\n\n9\n3x5x17\nX = TRTP, Y = AVAL, K = SITEID\nRow Means\nSITEID has many strata and provokes sparse groups, AVAL is ordinal, therefore row means statistic applies here, R threw an error\n\n\n10\n5x3x3\nX = AVAL, Y = AGEGR1, K = TRTP\nCorrelation\nX and Y are ordinal variables and therefore the correlation statistics has to be taken here"
  },
  {
    "objectID": "Comp/r-sas_cmh.html#cmh-statistics",
    "href": "Comp/r-sas_cmh.html#cmh-statistics",
    "title": "R vs SAS CMH",
    "section": "CMH Statistics",
    "text": "CMH Statistics\n\n\nLoading required package: vcd\n\n\nLoading required package: grid\n\n\nLoading required package: gnm\n\n\n\nAttaching package: 'vcdExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    summarise\n\n\nscenarios this is a test\nAs it can be seen, there are two schemata where R does not provide any results:\n\n\n\n\n\n\n\n\n\n\nTest\nChi-Square\ndf\np-value\n\n\nSAS\nR\nSAS\nR\nSAS\nR\n\n\n\n\n1\nCorrelation\n0.01767040\n0.0008689711\n1\n1\n0.89424870\n0.9764831\n\n\nRow Means\n0.01767040\n2.4820278527\n1\n2\n0.89424870\n0.2890910\n\n\nGeneral Association\n0.01767040\n2.4820278527\n1\n2\n0.89424870\n0.2890910\n\n\n3*\nCorrelation\n0.00278713\nNaN\n1\n1\n0.95789662\nNaN\n\n\nRow Means\n2.38606985\nNaN\n2\n2\n0.30329938\nNaN\n\n\nGeneral Association\n2.38606985\nNaN\n2\n2\n0.30329938\nNaN\n\n\n6\nCorrelation\n1.14720000\n0.1115439738\n1\n1\n0.28410000\n0.7383931\n\n\nRow Means\n1.14720000\n2.6632420358\n1\n2\n0.28410000\n0.2640489\n\n\nGeneral Association\n2.56720000\n6.5238474681\n4\n8\n0.63260000\n0.5887637\n\n\n9†\nCorrelation\n0.08544312\nNA\n1\nNA\n0.77005225\nNA\n\n\nRow Means\n2.47631367\nNA\n2\nNA\n0.28991809\nNA\n\n\nGeneral Association\n7.03387844\nNA\n8\nNA\n0.53298189\nNA\n\n\n10\nCorrelation\n2.73816085\n0.8715295423\n1\n1\n0.09797747\n0.3505322\n\n\nRow Means\n4.40701092\n3.0445270087\n4\n4\n0.35371641\n0.5504018\n\n\nGeneral Association\n5.73053819\n5.7305381934\n8\n8\n0.67738613\n0.6773861\n\n\n\n* Reason for NaN in schema 3: Stratum k = AMERICAN INDIAN OR ALASKA NATIVE can not be compared because there are only values for one treatment and one gender.\n\n\n† Reason for Error 4: For large sparse table (many strata) CMHTest will occasionally throw an error in solve.default(AVA) because of singularity"
  },
  {
    "objectID": "blogs/posts/202305 introduction to CAMIS blog.html",
    "href": "blogs/posts/202305 introduction to CAMIS blog.html",
    "title": "Introduction Comparing Analysis Method Implementations in Software (CAMIS)",
    "section": "",
    "text": "Are you trying to replicate results using different software/languages and struggling to find out why you can’t match the results? Check out the CAMIS repository!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe CAMIS repository stores documentation detailing the reasons for observed differences when performing statistical analysis in SAS and R. The repository is housed on github, and will be populated through open-source community contributions.\nDifferences between software could be due to different default and available options, including the methods being used. By documenting these known differences in a repository, we aim to reduce time-consuming efforts within the community, where multiple people are investigating the same issues. If you find an issue not already investigated, please log an Issue in github. If you have time to investigate and document the reason for the issue, then please submit a pull request with the new content in a quarto file. Details of how to contribute can be found on the website.\nCAMIS is a PHUSE working group in collaboration with PSI and the R consortium. Initially the repository contains R and SAS analysis result comparisons, however the team hope to extend to other software/languages in the near future. Our white paper will soon be available on the website. Please help us to build a high quality and comprehensive repository."
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Introduction Comparing Analysis Method Implementations in Software (CAMIS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023: A Year of Progress for the PHUSE CAMIS Working Group Project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "SAS/manova.html",
    "href": "SAS/manova.html",
    "title": "Multivariate Analysis of Variance in SAS",
    "section": "",
    "text": "Example 39.6 Multivariate Analysis of Variance from SAS MANOVA User Guide\nThis example employs multivariate analysis of variance (MANOVA) to measure differences in the chemical characteristics of ancient pottery found at four kiln sites in Great Britain. The data are from Tubb, Parker, and Nickless (1980), as reported in Hand et al. (1994).\nFor each of 26 samples of pottery, the percentages of oxides of five metals are measured. The following statements create the data set and invoke the GLM procedure to perform a one-way MANOVA. Additionally, it is of interest to know whether the pottery from one site in Wales (Llanederyn) differs from the samples from other sites; a CONTRAST statement is used to test this hypothesis.\n\n    #Example code\n    title \"Romano-British Pottery\";\n   data pottery;\n      input Site $12. Al Fe Mg Ca Na;\n      datalines;\n   Llanederyn   14.4 7.00 4.30 0.15 0.51\n   Llanederyn   13.8 7.08 3.43 0.12 0.17\n   Llanederyn   14.6 7.09 3.88 0.13 0.20\n   Llanederyn   11.5 6.37 5.64 0.16 0.14\n   Llanederyn   13.8 7.06 5.34 0.20 0.20\n   Llanederyn   10.9 6.26 3.47 0.17 0.22\n   Llanederyn   10.1 4.26 4.26 0.20 0.18\n   Llanederyn   11.6 5.78 5.91 0.18 0.16\n   Llanederyn   11.1 5.49 4.52 0.29 0.30\n   Llanederyn   13.4 6.92 7.23 0.28 0.20\n   Llanederyn   12.4 6.13 5.69 0.22 0.54\n   Llanederyn   13.1 6.64 5.51 0.31 0.24\n   Llanederyn   12.7 6.69 4.45 0.20 0.22\n   Llanederyn   12.5 6.44 3.94 0.22 0.23\n   Caldicot     11.8 5.44 3.94 0.30 0.04\n   Caldicot     11.6 5.39 3.77 0.29 0.06\n   IslandThorns 18.3 1.28 0.67 0.03 0.03\n   IslandThorns 15.8 2.39 0.63 0.01 0.04\n   IslandThorns 18.0 1.50 0.67 0.01 0.06\n   IslandThorns 18.0 1.88 0.68 0.01 0.04\n   IslandThorns 20.8 1.51 0.72 0.07 0.10\n   AshleyRails  17.7 1.12 0.56 0.06 0.06\n   AshleyRails  18.3 1.14 0.67 0.06 0.05\n   AshleyRails  16.7 0.92 0.53 0.01 0.05\n   AshleyRails  14.8 2.74 0.67 0.03 0.05\n   AshleyRails  19.1 1.64 0.60 0.10 0.03\n   ;\n   run;\n   \n   proc glm data=pottery;\n      class Site;\n      model Al Fe Mg Ca Na = Site;\n      contrast 'Llanederyn vs. the rest' Site 1 1 1 -3;\n      manova h=_all_ / printe printh;\n   run;\n\nAfter the summary information (1), PROC GLM produces the univariate analyses for each of the dependent variables (2-6). These analyses show that sites are significantly different for all oxides individually. You can suppress these univariate analyses by specifying the NOUNI option in the MODEL statement.\n1 Summary Information about Groups\n\n\n\n\n\n\n\n\n\n2 Univariate Analysis of Variance for Aluminum Oxide (AI)\n\n\n\n\n\n\n\n\n\n3 Univariate Analysis of Variance for Iron Oxide (Fe)\n\n\n\n\n\n\n\n\n\n4 Univariate Analysis of Variance for Calcium Oxide (Ca)\n\n\n\n\n\n\n\n\n\n5 Univariate Analysis of Variance for Magnesium Oxide (Mg)\n\n\n\n\n\n\n\n\n\n6 Analysis of Variance for Sodium Oxide (Na)\n\n\n\n\n\n\n\n\n\nThe PRINTE option in the MANOVA statement displays the elements of the error matrix (7), also called the Error Sums of Squares and Crossproducts matrix. The diagonal elements of this matrix are the error sums of squares from the corresponding univariate analyses.\nThe PRINTE option also displays the partial correlation matrix (7) associated with the E matrix. In this example, none of the oxides are very strongly correlated; the strongest correlation (r=0.488) is between magnesium oxide and calcium oxide.\n7 Error SSCP Matrix and Partial Correlations\n\n\n\n\n\n\n\n\n\nThe PRINTH option produces the SSCP matrix for the hypotheses being tested (Site and the contrast); (8 and 9). Since the Type III SS are the highest-level SS produced by PROC GLM by default, and since the HTYPE= option is not specified, the SSCP matrix for Site gives the Type III H matrix. The diagonal elements of this matrix are the model sums of squares from the corresponding univariate analyses.\nFour multivariate tests are computed, all based on the characteristic roots and vectors of \\(E^{-1}H\\). These roots and vectors are displayed along with the tests. All four tests can be transformed to variates that have distributions under the null hypothesis. Note that the four tests all give the same results for the contrast, since it has only one degree of freedom. In this case, the multivariate analysis matches the univariate results: there is an overall difference between the chemical composition of samples from different sites, and the samples from Llanederyn are different from the average of the other sites.\n8 Hypothesis SSCP Matrix and Multivariate Tests for Overall Site Effect\n\n\n\n\n\n\n\n\n\n9 Hypothesis SSCP Matrix and Multivariate Tests for Differences between Llanederyn and the Other Sites\n\n\n\n\n\n\n\n\n\nReferences\nSAS MANOVA User Guide"
  },
  {
    "objectID": "SAS/mcnemar.html",
    "href": "SAS/mcnemar.html",
    "title": "McNemar’s test in SAS",
    "section": "",
    "text": "Performing McNemar’s test in SAS\nTo demonstrate McNemar’s test in SAS, data concerning the presence or absence of cold symptoms was used. The symptoms were recorded by the same children at the age of 12 and 14. A total of 2638 participants were involved.\n\nUsing PROC FREQ\nTesting for a significant difference in cold symptoms between ages, using McNemar’s test in SAS, can be performed as below. The AGREE option is stated within the FREQ procedure to produce agreement tests and measures, including McNemar’s test.\n\nproc freq data=colds;\n  tables age12*age14 / agree;\nrun;\n\n\n\nResults\n\n\n\n\n\n\n\n\n\nSAS outputs the tabulated data for proportions, the McNemar’s Chi-square statistic, and the Kappa coefficient with 95% confidence limits. There is no continuity correction used and no option to include this."
  },
  {
    "objectID": "SAS/ancova.html",
    "href": "SAS/ancova.html",
    "title": "Ancova",
    "section": "",
    "text": "ANCOVA in SAS\nIn SAS, there are several ways to perform ANCOVA analysis. One common way is to use PROC GLM with the LSMEANS option. The below example will use this method.\n\nData Used\nThe following data was used in this example.\n  data DrugTest;\n     input Drug $ PreTreatment PostTreatment @@;\n     datalines;\n  A 11  6   A  8  0   A  5  2   A 14  8   A 19 11\n  A  6  4   A 10 13   A  6  1   A 11  8   A  3  0\n  D  6  0   D  6  2   D  7  3   D  8  1   D 18 18\n  D  8  4   D 19 14   D  8  9   D  5  1   D 15  9\n  F 16 13   F 13 10   F 11 18   F  9  5   F 21 23\n  F 16 12   F 12  5   F 12 16   F  7  1   F 12 20\n  ;\n\n\nCode\nThe following code was used to test the effects of a drug pre and post treatment:\n  proc glm data=DrugTest;\n     class Drug;\n     model PostTreatment = Drug PreTreatment / solution;\n     lsmeans Drug / stderr pdiff cov out=adjmeans;\n  run;\n  proc print data=adjmeans;\n  run;\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen in the images above, the GLM procedure provides multiple types of analysis to determine the relationship between the dependent and independent variables. The last step produces a table of LSMEANS and coefficient of variation values for each of the three different drugs in the dataset."
  },
  {
    "objectID": "SAS/kruskal_wallis.html",
    "href": "SAS/kruskal_wallis.html",
    "title": "Kruskal Wallis SAS",
    "section": "",
    "text": "The Kruskal-Wallis test is a non-parametric equivalent to the one-way ANOVA. For this example, the data used is a subset of R’s datasets::iris, testing for difference in sepal width between species of flower. This data was subset in R and input manually to SAS with a data step.\n\ndata iris_sub;\n    input Species $ Sepal_Width;\n    datalines;\nsetosa 3.4\nsetosa 3.0\nsetosa 3.4\nsetosa 3.2\nsetosa 3.5\nsetosa 3.1\nversicolor 2.7\nversicolor 2.9\nversicolor 2.7\nversicolor 2.6\nversicolor 2.5\nversicolor 2.5\nvirginica 3.0\nvirginica 3.0\nvirginica 3.1\nvirginica 3.8\nvirginica 2.7\nvirginica 3.3\n;\nrun;"
  },
  {
    "objectID": "SAS/kruskal_wallis.html#introduction",
    "href": "SAS/kruskal_wallis.html#introduction",
    "title": "Kruskal Wallis SAS",
    "section": "",
    "text": "The Kruskal-Wallis test is a non-parametric equivalent to the one-way ANOVA. For this example, the data used is a subset of R’s datasets::iris, testing for difference in sepal width between species of flower. This data was subset in R and input manually to SAS with a data step.\n\ndata iris_sub;\n    input Species $ Sepal_Width;\n    datalines;\nsetosa 3.4\nsetosa 3.0\nsetosa 3.4\nsetosa 3.2\nsetosa 3.5\nsetosa 3.1\nversicolor 2.7\nversicolor 2.9\nversicolor 2.7\nversicolor 2.6\nversicolor 2.5\nversicolor 2.5\nvirginica 3.0\nvirginica 3.0\nvirginica 3.1\nvirginica 3.8\nvirginica 2.7\nvirginica 3.3\n;\nrun;"
  },
  {
    "objectID": "SAS/kruskal_wallis.html#implementing-kruskal-wallis-in-sas",
    "href": "SAS/kruskal_wallis.html#implementing-kruskal-wallis-in-sas",
    "title": "Kruskal Wallis SAS",
    "section": "Implementing Kruskal-Wallis in SAS",
    "text": "Implementing Kruskal-Wallis in SAS\nThe Kruskal-Wallis test can be implemented in SAS using the NPAR1WAY procedure with WILCOXON option. Below, the test is defined with the indicator variable (Species) by the CLASS statement, and the response variable (Sepal_Width) by the VAR statement. Adding the EXACT statement outputs the exact p-value in addition to the asymptotic result. The null hypothesis is that the samples are from identical populations.\n\nproc npar1way data=iris_sub wilcoxon;\nclass Species;\nvar Sepal_Width;\nexact;\nrun;"
  },
  {
    "objectID": "SAS/kruskal_wallis.html#results",
    "href": "SAS/kruskal_wallis.html#results",
    "title": "Kruskal Wallis SAS",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\n\n\nAs seen above, SAS outputs a table of Wilcoxon Scores for Sepal_Width by each Species including (per group): the number (N); the sum of scores; the expected sum of scores under the null hypothesis; the standard deviation under the null hypothesis, and the observed mean score. The table also includes a footnote to specify that ties were handled by using the average score.\nA table of the test results gives the Kruskal-Wallis rank sum statistic (10.922), the degrees of freedom (2), and the asymptotic p-value of the test (0.0042), and the exact p-value (0.0008). Therefore, the difference in population medians is statistically significant at the 5% level."
  },
  {
    "objectID": "SAS/linear-regression.html",
    "href": "SAS/linear-regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "To demonstrate the use of linear regression we examine a dataset that illustrates the relationship between Height and Weight in a group of 237 teen-aged boys and girls. The dataset is available at (../data/htwt.csv) and is imported to sas using proc import procedure.\n\nDescriptive Statistics\nThe first step is to obtain the simple descriptive statistics for the numeric variables of htwt data, and one-way frequencies for categorical variables. This is accomplished by employing proc means and proc freq procedures There are 237 participants who are from 13.9 to 25 years old. It is a cross-sectional study, with each participant having one observation. We can use this data set to examine the relationship of participants’ height to their age and sex.\n\nproc means data=htwt;\nrun;\n\n                    Descriptive Statistics for HTWT Data Set                  \n                             The MEANS Procedure\n\nVariable  Label     N          Mean       Std Dev       Minimum       Maximum\n-----------------------------------------------------------------------------\nAGE       AGE     237    16.4430380     1.8425767    13.9000000    25.0000000\nHEIGHT    HEIGHT  237    61.3645570     3.9454019    50.5000000    72.0000000\nWEIGHT    WEIGHT  237   101.3080169    19.4406980    50.5000000   171.5000000\n----------------------------------------------------------------------------\n\n\nproc freq data=htwt;\ntables sex;\nrun;\n\n    Oneway Frequency Tabulation for Sex for HTWT Data Set                    \n                    The FREQ Procedure\n\n                                      Cumulative    Cumulative\nSEX         Frequency     Percent     Frequency      Percent\n-------------------------------------------------------------\nf           111           46.84           111        46.84\nm           126           53.16           237       100.00\n\nIn order to create a regression model to demonstrate the relationship between age and height for females, we first need to create a flag variable identifying females and an interaction variable between age and female gender flag.\n\ndata htwt2;\n  set htwt;\n  if sex=\"f\" then female=1;\n  if sex=\"m\" then female=0; \n \n  *model to demonstrate interaction between female gender and age;\n  fem_age = female * age;  \nrun;\n\n\n\nRegression Analysis\nNext, we fit a regression model, representing the relationships between gender, age, height and the interaction variable created in the datastep above. We again use a where statement to restrict the analysis to those who are less than or equal to 19 years old. We use the clb option to get a 95% confidence interval for each of the parameters in the model. The model that we are fitting is height = b0 + b1 x female + b2 x age + b3 x fem_age + e\n\nproc reg data=htwt2;\n  where age &lt;=19;\n  model height = female age fem_age / clb;\nrun; quit;\n\n                        Number of Observations Read         219\n                        Number of Observations Used         219\n\n                                 Analysis of Variance\n\n                                        Sum of           Mean\n    Source                   DF        Squares         Square    F Value    Pr &gt; F\n    Model                     3     1432.63813      477.54604      60.93    &lt;.0001\n    Error                   215     1684.95730        7.83701\n    Corrected Total         218     3117.59543\n\n                 Root MSE              2.79947    R-Square     0.4595\n                 Dependent Mean       61.00457    Adj R-Sq     0.4520\n                 Coeff Var             4.58895\n\nWe examine the parameter estimates in the output below.\n\n                            Parameter Estimates\n                            Parameter       Standard\n       Variable     DF       Estimate          Error    t Value    Pr &gt; |t|       95% Confidence Limits\n       Intercept     1       28.88281        2.87343      10.05      &lt;.0001       23.21911       34.54650\n       female        1       13.61231        4.01916       3.39      0.0008        5.69031       21.53432\n       AGE           1        2.03130        0.17764      11.44      &lt;.0001        1.68117        2.38144\n       fem_age       1       -0.92943        0.24782      -3.75      0.0002       -1.41791       -0.44096\n\nFrom the parameter estimates table the coefficients b0,b1,b2,b3 are estimated as b0=28.88 b1=13.61 b2=2.03 b3=-0.92942\nThe resulting regression model for height, age and gender based on the available data is height=28.88281 + 13.61231 x female + 2.03130 x age -0.92943 x fem_age"
  },
  {
    "objectID": "SAS/mmrm.html",
    "href": "SAS/mmrm.html",
    "title": "MMRM in SAS",
    "section": "",
    "text": "Mixed Models\n\nFitting the MMRM in SAS\nIn SAS the following code was used (assessments at avisitn=0 should also be removed from the response variable):\n\nproc mixed data=adlbh;\n  where base ne . and avisitn not in (., 99);\n  class usubjid trtpn(ref=\"0\") avisitn;\n  by paramcd param;\n  model chg=base trtpn avisitn  trtpn*avisitn / solution cl alpha=0.05 ddfm=KR;\n  repeated avisitn/subject=usubjid type=&covar;\n  lsmeans trtpn * avisitn / diff cl slice=avisitn;\n  lsmeans trtpn / diff cl;\nrun;\n\nwhere the macro variable covar could be UN, CS or AR(1). The results were stored in .csv files that were post-processed in R and compared with the results from R."
  },
  {
    "objectID": "SAS/rounding.html",
    "href": "SAS/rounding.html",
    "title": "Rounding in SAS",
    "section": "",
    "text": "There are two rounding functions in SAS.\nThe round() function in SAS will round to the nearest whole number and ‘away from zero’ or ‘rounding up’ when equidistant meaning that exactly 12.5 rounds to the integer 13.\nThe rounde() function in SAS will round to the nearest whole number and ‘rounding to the even number’ when equidistant, meaning that exactly 12.5 rounds to the integer 12.\nBoth functions allow you to specify the number of decimal places you want to round to.\nFor example (See references for source of the example)\n\n    #Example code\n    data XXX;\n      my_number=2.2; output;\n      my_number=3.99; output;\n      my_number=1.2345; output;\n      my_number=7.876; output;\n      my_number=13.8739;  output;\n    run;\n\n    data xxx2;\n      set xxx;\n        r_1_dec = round(my_number, 0.1);\n        r_2_dec = round(my_number, 0.01);\n        r_3_dec = round(my_number, 0.001);\n        \n        re_1_dec = rounde(my_number, 0.1);\n        re_2_dec = rounde(my_number, 0.01);\n        re_3_dec = rounde(my_number, 0.001);\n    run;\n\n\n\n\n\n\n\n\n\n\n\n\n\nmy_number\nr_1_dec\nr_2_de\nr_3_dec\nre_1_dec\nre_2_dec\nre_3_dec\n\n\n\n\n2.2\n2.2\n2.2\n2.2\n2.2\n2.2\n2.2\n\n\n3.99\n4\n3.99\n3.99\n4\n3.99\n3.99\n\n\n1.2345\n1.2\n1.23\n1.235\n1.2\n1.23\n1.234\n\n\n7.876\n7.9\n7.88\n7.876\n7.9\n7.88\n7.876\n\n\n13.8739\n13.9\n13.87\n13.874\n13.9\n13.87\n13.874\n\n\n\nReferences\nHow to Round Numbers in SAS - SAS Example Code"
  },
  {
    "objectID": "non_website_content/Conferences 2023 archive.html",
    "href": "non_website_content/Conferences 2023 archive.html",
    "title": "Conferences",
    "section": "",
    "text": "Conference Programme\nWe plan to showcase the CAMIS project at a number of conferences throughout 2023 and 2024. See below the list of conferences the CAMIS team will be at and please come say hello to us !\n\n\n\n\n\n\n\n\n\n\n\nConference\n2023/2024 Planning dates\n2023 Date & Location\n2023 Main Contact\nAlso attending\nDetails\n\n\n\n\nJSM (ASA conference)\nAbstract Feb 2024\n\nLeon Shih\n\n\n\n\nPHUSE US Connect\nTBC for 2024\n5-8 March 2023 Orlando, Florida\nSoma Sekhar\n\nPresentation\n\n\nDISS (Duke industry statistics symposium)\nTBC for 2024\n29-31st March 2023 Virtual\nMolly MacDiarmid (2023)\n\nPoster\n\n\nPSDM(Pharmaceutical statistics and data management)\n\n19 Apr 2023 Netherlands\n\n\n\n\n\nIASCT (ConSPIC - conference for statistics and programming in clinical research)\nAbstract submission 14 Mar-3rd Apr\n4-6 May 2023 Bengaluru, India\nHarshal Khanolkar\n\nTalk. and/or poster\n\n\nSociety of Clinical Trials (SCT\n\n21-24 May 2023\nMichael Kane\n\n\n\n\nuse R\nTBC\nJune 2024?\n\n\n\n\n\nPSI 2023 Conference\nTalk submission Nov.\nPoster submission Feb.\n11-14 June 2023 Hammersmith London West, England\nMartin Brown\nChristina Fillmore\nLyn Taylor\nMolly Macdiarmid\nMartin Brown\nAiming Yang\nOral & poster submission completed\n\n\nDIA 2023 Global Annual Meeting\n\n25-29 June 2023 Boston MA, USA\n\n\n\n\n\nJoint statistical meeting (JSM)\nFeb 2024 submission for next year\n5-10 Aug 2023 Toronto, Ontario, Canada\n\n\n\n\n\nISCB Conference\n\n27-31 Aug 2023 Milan-Italy\n\n\n\n\n\nRSS conference\nAbstract by 6th April\n4-7 sept 2023 Harrogate, England\nLyn Taylor\n\nConfirmed oral presentation\n\n\nPHUSE/FDA Quarterly meeting\nSeptember 13 (10:00 EDT/15:00 BST)\nWG can present their work, share their progress, and request any FDA \nsupport\nLyn Taylor\n\n30 min presentation\n\n\nPHUSE CSS\n15th June Abstract open, register by 30th june\n(TBC 2024 DVOST breakout sessions)\nSept 18-20, Maryland USA\nSoma Sekhar\nVikash Jain\nAditee Dani\nPoster\n\n\nASA Bio pharmaceutical Section Regulatory-industry Statistics Workshop\n\n27-29 Sept 2023 Rockville, Maryland, USA\n\n\n\n\n\nEASD 2023 - European Association for study of diabetes\n\n02-06 Oct 2023 Hamberg Germany\n\n\n\n\n\nSESUG (South East SAS user group)\n\n17-19 Oct 2023\nBrian Varney\n\n\n\n\nPHUSE EU Connect 2023\n\n5-8 November 2023 ICC Birmingham, England\nJayashree vendanayagam\n\nPresenting on shiny App for regulatory submission (will include CAMIS advert)\n\n\nR in Pharma\n\n\nNov Virtual\nBrian Varney/ Christina Fillmore?\n\nForm open. Christina to speak to Brian.\n\n\nPOSIT conf.\nInvite only\nSeptember - Chicago\nJulianne Manitz & Doug Kelkhoff\n\nR Validation Hub team will include a slide for us. (Juliane Manitz/Doug Kelkhoff)\n\n\nPHUSE (Single day events) SDEs\n\nMississauga\nJune 8th\nJayashree vendanayagam\n\nPresenting on shiny App for regulatory submission (will include CAMIS advert)\n\n\nPHUSE (Single day events) SDEs\n\nNew York (Oct 16th)\nAiming Yang\n\nEmailed host to have poster/ talk/ advert"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "CAMIS: Comparing Analysis Method Implementations in Software\nWe are a cross-industry PHUSE DVOST Working Group, run in collaboration with members from PHUSE, PSI, ASA and IASCT."
  },
  {
    "objectID": "about.html#who-are-we",
    "href": "about.html#who-are-we",
    "title": "About",
    "section": "",
    "text": "CAMIS: Comparing Analysis Method Implementations in Software\nWe are a cross-industry PHUSE DVOST Working Group, run in collaboration with members from PHUSE, PSI, ASA and IASCT."
  },
  {
    "objectID": "about.html#objectives",
    "href": "about.html#objectives",
    "title": "About",
    "section": "Objectives",
    "text": "Objectives\nThrough the creation of the CAMIS White Paper, the group provided guidance on the types of questions statistical staff should ask to aid with replication of analysis methods in different software and to identify the fundamental sources of discrepant results between software.\nThe group aims to save unnecessary repetition of work within the community, through the creation of this open source repository. This repository welcomes contributions from the wider community and is a resource comparing and documenting differences in analysis method implementations in software."
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About",
    "section": "Background",
    "text": "Background\nTraditionally, highly regulated industries (such as the pharmaceutical industry), have limited themselves to the use of commercially available software. When taking such an approach, the responsibility for the validation and testing of the product was often delegated to the software development company themselves, to ensure the software performs in line with its documentation, producing accurate reliable and reproducible results. However, one downside of this approach is that new methods and functionality can be slow to be adopted, limiting new method implementation and tools that can bring in efficiency.\nWith the increase in popularity of data science, the rate at which community led tools and methods are being developed in open-source software is rapid. The availability of advanced analytic capabilities, has led to increased desire for statistical staff in regulated industries to have access and approval to use to open source software (Rimler et al. 2022). The use of open source software is now widely accepted (FDA 2015), however, this increased variety of tools has resulted in an overlap of capabilities. This has raised challenging questions of traditional approaches to clinical analyses – particularly in situations where the overlap yields different results.\nOne example of this challenge encompasses discrepancies which have been discovered in statistical analysis results between different programming languages, even when working within qualified statistical computing environments. Subtle differences exist between the fundamental approaches and assumptions implemented within each language, yielding differences in results which are correct and consistent with their respective documentation. The fact that these differences exist may cause unease for sponsor companies when submitting to a regulatory agency, as it is uncertain if the agency will view these differences as problematic. By understanding the source of any discrepancies, one can reinstate that confidence.\nThis cross-industry group aims to empower statistical staff to make informed choices on the implementation of statistical analyses when multiple languages yield different results."
  },
  {
    "objectID": "about.html#references",
    "href": "about.html#references",
    "title": "About",
    "section": "References",
    "text": "References\n\nMichael S. Rimler, Joseph Rickert, Min-Hua Jen, Mike Stackhouse. 2022. Understanding differences in statistical methodology implementations across programming languages.\nStatistical Software Clarifying Statement (fda.gov)\nCAMIS White Paper"
  },
  {
    "objectID": "publication/Conferences_archived.html",
    "href": "publication/Conferences_archived.html",
    "title": "Conferences 2024",
    "section": "",
    "text": "2024 Conference Schedule\nList of seminars and conferences that the CAMIS team will be attending in 2024.\nIf you are a volunteer on the CAMIS project and plan to present at a seminar or conference, please add details of the conference below. For help with slides or content go to HERE.\nTo cite the CAMIS project work in online content or presentations please use: “Content reproduced with the permission of PHUSE CAMIS - A DVOST Working Group”.\n\n\n\nConference name\nDate (2024)\nLocation\nName Attending\nDetails\nWebsite\n\n\n\n\nPhuse US Connect\n25-28 Feb\nBethesa, Maryland, USA\nSoma Sekhar Sriadibhatla, Vikash Jain, Brian Varney\nPoster\nConnect\n\n\nPhuse/FDA CSS\n3-5 June\nSilver Spring Maryland, USA\nSoma Sekhar Sriadibhatla, Vikash Jain, Harshal Khanolkar\nWorkshop\nCSS\n\n\nRSS Local Group Seminar\n28 Feb\nSheffield, England\nLyn Taylor\nSeminar\nRSS\n\n\n\n\n\nYearly Conference Planner\nTo help to plan our attendance throughout the year, here is a list of conferences we are looking to send representation to. If you plan to attend one of these conferences and are interested in representing us, then please get in touch.\n\n\n\nConference name\nUsual Abstract Deadline\nUsual Conference Date\nRegion\nLinks\n\n\n\n\nJoint Statistical Meetings (JSM) American Statistical Association (ASA)\n1st February\n1st week of August\nUSA\nJSM-ASA\n\n\nASA Biopharmaceutical Section Regulatory-Industry Statistics workshop\nEnd March\nLast week of September\nUSA\nBIOP\n\n\nPhuse US Connect\nNovember\nLast week of Feb\nUSA\nCDISC\n\n\nPhuse/FDA Computational Science Symposium(CSS)\nDecember\n1st week of June\nUSA\nCSS\n\n\nIASCT (ConSPIC)\nMid March\nEarly May\nIndia\nIASCT\n\n\nSociety of Clinical Trials (SCT)\nJanuary\nMid May\nUSA\nSCT\n\n\nPharmaSUG\nMid Jan\nMid May\nUSA\nPharmaSUG\n\n\nuseR\nEarly March\nEarly July\nEurope/Online\nuseR\n\n\nPSI\nNov-oral, Feb-Poster\nMid June\nEurope\nPSI\n\n\nDIA Global\nEnd Feb-poster\nMid June\nUSA\nDIA-USA\n\n\nDIA Europe\nNov\nMid March\nEurope\nDIA-Europe\n\n\nDIA China\nJan\nMid May\nChina\nDIA-China\n\n\nInternational Society for Clinical Biostatistics (ISCB)\nMid Feb\nMid July\nEurope\nISCB\n\n\nRoyal Statistical Society (RSS)\nEarly April\nEarly September\nEngland\nRSS\n\n\nSouthEast SAS User Group (SESUG)\nEnd Feb\nEnd Sept\nMaryland, USA\nSESUG\n\n\nPHUSE EU Connect\nMid March\nMid Nov\nEurope\nPHUSE EU Connect\n\n\nR/Pharma\nApril\nMid October\nVirtual\nR/pharma\n\n\nPOSIT conf.\nInvite only\nSeptember\nUSA\nPOSIT conf"
  },
  {
    "objectID": "R/summary-stats.html",
    "href": "R/summary-stats.html",
    "title": "Deriving Quantiles or Percentiles in R",
    "section": "",
    "text": "Percentiles can be calculated in R using the quantile function. The function has the argument type which allows for nine different percentile definitions to be used. The default is type = 7, which uses a piecewise-linear estimate of the cumulative distribution function to find percentiles.\nThis is how the 25th and 40th percentiles of aval could be calculated using the default type.\n\nquantile(aval, probs = c(0.25, 0.4))"
  },
  {
    "objectID": "R/manova.html",
    "href": "R/manova.html",
    "title": "Multivariate Analysis of Variance in R",
    "section": "",
    "text": "For a detailed description of MANOVA including assumptions see Renesh Bedre\nExample 39.6 Multivariate Analysis of Variance from SAS MANOVA User Guide\nThis example employs multivariate analysis of variance (MANOVA) to measure differences in the chemical characteristics of ancient pottery found at four kiln sites in Great Britain. The data are from Tubb, Parker, and Nickless (1980), as reported in Hand et al. (1994).\nFor each of 26 samples of pottery, the percentages of oxides of five metals are measured. The following statements create the data set and perform a one-way MANOVA. Additionally, it is of interest to know whether the pottery from one site in Wales (Llanederyn) differs from the samples from other sites.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(knitr)\nlibrary(emmeans)\n\nknitr::opts_chunk$set(echo = TRUE, cache = TRUE)\npottery &lt;- read.csv(\"../data/manova1.csv\")\npottery\n\n           site   al   fe   mg   ca   na\n1    Llanederyn 14.4 7.00 4.30 0.15 0.51\n2    Llanederyn 13.8 7.08 3.43 0.12 0.17\n3    Llanederyn 14.6 7.09 3.88 0.13 0.20\n4    Llanederyn 11.5 6.37 5.64 0.16 0.14\n5    Llanederyn 13.8 7.06 5.34 0.20 0.20\n6    Llanederyn 10.9 6.26 3.47 0.17 0.22\n7    Llanederyn 10.1 4.26 4.26 0.20 0.18\n8    Llanederyn 11.6 5.78 5.91 0.18 0.16\n9    Llanederyn 11.1 5.49 4.52 0.29 0.30\n10   Llanederyn 13.4 6.92 7.23 0.28 0.20\n11   Llanederyn 12.4 6.13 5.69 0.22 0.54\n12   Llanederyn 13.1 6.64 5.51 0.31 0.24\n13   Llanederyn 12.7 6.69 4.45 0.20 0.22\n14   Llanederyn 12.5 6.44 3.94 0.22 0.23\n15     Caldicot 11.8 5.44 3.94 0.30 0.04\n16     Caldicot 11.6 5.39 3.77 0.29 0.06\n17 IslandThorns 18.3 1.28 0.67 0.03 0.03\n18 IslandThorns 15.8 2.39 0.63 0.01 0.04\n19 IslandThorns 18.0 1.50 0.67 0.01 0.06\n20 IslandThorns 18.0 1.88 0.68 0.01 0.04\n21 IslandThorns 20.8 1.51 0.72 0.07 0.10\n22  AshleyRails 17.7 1.12 0.56 0.06 0.06\n23  AshleyRails 18.3 1.14 0.67 0.06 0.05\n24  AshleyRails 16.7 0.92 0.53 0.01 0.05\n25  AshleyRails 14.8 2.74 0.67 0.03 0.05\n26  AshleyRails 19.1 1.64 0.60 0.10 0.03\n\n\n1 Perform one way MANOVA\nResponse ID for ANOVA is order of 1=al, 2=fe, 3=mg, ca, na.\nWe are testing H0: group mean vectors are the same for all groups or they dont differ significantly vs\nH1: At least one of the group mean vectors is different from the rest.\n\ndep_vars &lt;- cbind(pottery$al,pottery$fe,pottery$mg, pottery$ca, pottery$na)\nfit &lt;-manova(dep_vars ~ pottery$site)\nsummary.aov(fit)\n\n Response 1 :\n             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \npottery$site  3 175.610  58.537  26.669 1.627e-07 ***\nResiduals    22  48.288   2.195                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response 2 :\n             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \npottery$site  3 134.222  44.741  89.883 1.679e-12 ***\nResiduals    22  10.951   0.498                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response 3 :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \npottery$site  3 103.35  34.450   49.12 6.452e-10 ***\nResiduals    22  15.43   0.701                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response 4 :\n             Df   Sum Sq  Mean Sq F value    Pr(&gt;F)    \npottery$site  3 0.204703 0.068234  29.157 7.546e-08 ***\nResiduals    22 0.051486 0.002340                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response 5 :\n             Df  Sum Sq  Mean Sq F value    Pr(&gt;F)    \npottery$site  3 0.25825 0.086082  9.5026 0.0003209 ***\nResiduals    22 0.19929 0.009059                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n‘summary(fit)’ outputs the MANOVA testing of an overall site effect.\nP&lt;0.001 suggests there is an overall difference between the chemical composition of samples from different sites.\n\nsummary(fit)\n\n             Df Pillai approx F num Df den Df    Pr(&gt;F)    \npottery$site  3 1.5539   4.2984     15     60 2.413e-05 ***\nResiduals    22                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n2 Now we test to see if the Llanaderyn site is different to the other sites\nNOTE: interest may now lie in using pre-planned contrast statements to investigate if one site differs when compared to the average of the others. You would imagine this could be done using the ‘contrast’ function something like the code below, however this result does not match the SAS user guide and so looks to be doing a different analysis. SUGGEST THIS IS NOT USED UNTIL MORE RESEARCH INTO THIS METHOD CAN BE PERFORMED. One alternative suggestion is to perform a linear descriminent analysis (LDA).\n\nmanova(dep_vars ~ pottery$site) %&gt;% \n          emmeans(\"site\") %&gt;% \n     contrast(method=list(\n          \"Llanederyn vs other sites\"= c(\"Llanederyn\"=-3, \"Caldicot\"=1, \"IslandThorns\"=1, \"AshleyRails\"=1)))\n\n contrast                  estimate    SE df t.ratio p.value\n Llanederyn vs other sites     1.51 0.661 22   2.288  0.0321\n\nResults are averaged over the levels of: rep.meas \n\n\nNOTE: if you feel you can help with the above discrepancy please contribute to the CAMIS repo by following the instructions on the contributions page."
  },
  {
    "objectID": "R/association.html",
    "href": "R/association.html",
    "title": "Association Analysis for Count Data Using R",
    "section": "",
    "text": "The most commonly used association analysis methods for count data/contingency tables compare observed frequencies with those expected under the assumption of independence:\n\\[\nX^2 = \\sum_{i=1}^k \\frac{(x_i-e_i)^2}{e_i},\n\\] where \\(k\\) is the number of contingency table cells.\nOther measures for the correlation of two continuous variables are:"
  },
  {
    "objectID": "R/association.html#chi-squared-test",
    "href": "R/association.html#chi-squared-test",
    "title": "Association Analysis for Count Data Using R",
    "section": "Chi-Squared test",
    "text": "Chi-Squared test\n\nchisq.test(tab)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tab\nX-squared = 1.8261, df = 1, p-value = 0.1766"
  },
  {
    "objectID": "R/association.html#fisher-exact-test",
    "href": "R/association.html#fisher-exact-test",
    "title": "Association Analysis for Count Data Using R",
    "section": "Fisher Exact Test",
    "text": "Fisher Exact Test\nFor \\(2 \\times 2\\) contingency tables, p-values are obtained directly using the hypergeometric distribution.\n\nfisher.test(tab)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  tab\np-value = 0.135\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.8158882 3.2251299\nsample estimates:\nodds ratio \n  1.630576"
  },
  {
    "objectID": "R/association.html#chi-squared-test-1",
    "href": "R/association.html#chi-squared-test-1",
    "title": "Association Analysis for Count Data Using R",
    "section": "Chi-Squared Test",
    "text": "Chi-Squared Test\n\nchisq.test(tab2)\n\nWarning in chisq.test(tab2): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tab2\nX-squared = 260.76, df = 15, p-value &lt; 2.2e-16\n\n\nThe warning means that the smallest expected frequencies is lower than 5. It is recommended to use the Fisher’s exact test in this case."
  },
  {
    "objectID": "R/association.html#fisher-exact-test-1",
    "href": "R/association.html#fisher-exact-test-1",
    "title": "Association Analysis for Count Data Using R",
    "section": "Fisher Exact Test",
    "text": "Fisher Exact Test\nFor contingency tables larger than \\(2 \\times 2\\), p-values are based on simulations, which might require a lot of time (see ?fisher.test for details).\n\nfisher.test(tab2, simulate.p.value=TRUE)\n\n\n    Fisher's Exact Test for Count Data with simulated p-value (based on\n    2000 replicates)\n\ndata:  tab2\np-value = 0.0004998\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "R/logistic_regr.html",
    "href": "R/logistic_regr.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "A model of the dependence of binary variables on explanatory variables. The logit of expectation is explained as linear for of explanatory variables. If we observed \\((y_i, x_i),\\) where \\(y_i\\) is a Bernoulli variable and \\(x_i\\) a vector of explanatory variables, the model for \\(\\pi_i = P(y_i=1)\\) is\n\\[\n\\text{logit}(\\pi_i)= \\log\\left\\{ \\frac{\\pi_i}{1-\\pi_i}\\right\\} = \\beta_0 + \\beta x_i, i = 1,\\ldots,n\n\\]\nThe model is especially useful in case-control studies and leads to the effect of risk factors by odds ratios.\n\nExample: Lung Cancer Data\nData source: Loprinzi CL. Laurie JA. Wieand HS. Krook JE. Novotny PJ. Kugler JW. Bartel J. Law M. Bateman M. Klatt NE. et al. Prospective evaluation of prognostic variables from patient-completed questionnaires. North Central Cancer Treatment Group. Journal of Clinical Oncology. 12(3):601-7, 1994.\nSurvival in patients with advanced lung cancer from the North Central Cancer Treatment Group. Performance scores rate how well the patient can perform usual daily activities (see ?lung for details).\n\nlibrary(survival) \nglimpse(lung)\n\nRows: 228\nColumns: 10\n$ inst      &lt;dbl&gt; 3, 3, 3, 5, 1, 12, 7, 11, 1, 7, 6, 16, 11, 21, 12, 1, 22, 16…\n$ time      &lt;dbl&gt; 306, 455, 1010, 210, 883, 1022, 310, 361, 218, 166, 170, 654…\n$ status    &lt;dbl&gt; 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ age       &lt;dbl&gt; 74, 68, 56, 57, 60, 74, 68, 71, 53, 61, 57, 68, 68, 60, 57, …\n$ sex       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, …\n$ ph.ecog   &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 2, 2, 1, 2, 1, 2, 1, NA, 1, 1, 1, 2, 2, 1,…\n$ ph.karno  &lt;dbl&gt; 90, 90, 90, 90, 100, 50, 70, 60, 70, 70, 80, 70, 90, 60, 80,…\n$ pat.karno &lt;dbl&gt; 100, 90, 90, 60, 90, 80, 60, 80, 80, 70, 80, 70, 90, 70, 70,…\n$ meal.cal  &lt;dbl&gt; 1175, 1225, NA, 1150, NA, 513, 384, 538, 825, 271, 1025, NA,…\n$ wt.loss   &lt;dbl&gt; NA, 15, 15, 11, 0, 0, 10, 1, 16, 34, 27, 23, 5, 32, 60, 15, …\n\n\n\n\nModel Fit\nWe analyze the weight loss in lung cancer patients in dependency of age, sex, ECOG performance score and calories consumed at meals.\n\nlung2 &lt;- survival::lung %&gt;% \n  mutate(\n    wt_grp = factor(wt.loss &gt; 0, labels = c(\"weight loss\", \"weight gain\"))\n  ) \n\n\nm1 &lt;- glm(wt_grp ~ age + sex + ph.ecog + meal.cal, data = lung2, family = binomial(link=\"logit\"))\nsummary(m1)\n\n\nCall:\nglm(formula = wt_grp ~ age + sex + ph.ecog + meal.cal, family = binomial(link = \"logit\"), \n    data = lung2)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  3.2631673  1.6488207   1.979   0.0478 *\nage         -0.0101717  0.0208107  -0.489   0.6250  \nsex         -0.8717357  0.3714042  -2.347   0.0189 *\nph.ecog      0.4179665  0.2588653   1.615   0.1064  \nmeal.cal    -0.0008869  0.0004467  -1.985   0.0471 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 202.36  on 169  degrees of freedom\nResidual deviance: 191.50  on 165  degrees of freedom\n  (58 observations deleted due to missingness)\nAIC: 201.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe model summary contains the parameter estimates \\(\\beta_j\\) for each explanatory variable \\(x_j\\), corresponding to the log-odds for the response variable to take the value \\(1\\), conditional on all other explanatory variables remaining constant. For better interpretation, we can exponentiate these estimates, to obtain estimates for the odds instead and provide \\(95\\)% confidence intervals:\n\nexp(coef(m1))\n\n(Intercept)         age         sex     ph.ecog    meal.cal \n 26.1321742   0.9898798   0.4182250   1.5188698   0.9991135 \n\nexp(confint(m1))\n\nWaiting for profiling to be done...\n\n\n                2.5 %      97.5 %\n(Intercept) 1.0964330 730.3978786\nage         0.9495388   1.0307216\nsex         0.1996925   0.8617165\nph.ecog     0.9194053   2.5491933\nmeal.cal    0.9982107   0.9999837\n\n\n\n\nModel Comparison\nTo compare two logistic models, one tests the difference in residual variances from both models using a \\(\\chi^2\\)-distribution with a single degree of freedom (here at the \\(5\\)% level):\n\nm2 &lt;- glm(wt_grp ~ sex + ph.ecog + meal.cal, data = lung2, family = binomial(link=\"logit\"))\nsummary(m2)\n\n\nCall:\nglm(formula = wt_grp ~ sex + ph.ecog + meal.cal, family = binomial(link = \"logit\"), \n    data = lung2)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  2.5606595  0.7976887   3.210  0.00133 **\nsex         -0.8359241  0.3637378  -2.298  0.02155 * \nph.ecog      0.3794295  0.2469030   1.537  0.12435   \nmeal.cal    -0.0008334  0.0004346  -1.918  0.05517 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 202.36  on 169  degrees of freedom\nResidual deviance: 191.74  on 166  degrees of freedom\n  (58 observations deleted due to missingness)\nAIC: 199.74\n\nNumber of Fisher Scoring iterations: 4\n\nanova(m1, m2, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: wt_grp ~ age + sex + ph.ecog + meal.cal\nModel 2: wt_grp ~ sex + ph.ecog + meal.cal\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       165     191.50                     \n2       166     191.75 -1 -0.24046   0.6239\n\n\n\n\nPrediction\nPredictions from the model for the log-odds of a patient with new data to experience a weight loss are derived using predict():\n\n# new female, symptomatic but completely ambulatory patient consuming 2500 calories\nnew_pt &lt;- data.frame(sex=2, ph.ecog=1, meal.cal=2500)\npredict(m2, new_pt, type = \"response\")\n\n       1 \n0.306767"
  },
  {
    "objectID": "R/ancova.html",
    "href": "R/ancova.html",
    "title": "Ancova",
    "section": "",
    "text": "In this example, we’re looking at Analysis of Covariance. ANCOVA is typically used to analyse treatment differences, to see examples of prediction models go to the simple linear regression page."
  },
  {
    "objectID": "R/ancova.html#introduction",
    "href": "R/ancova.html#introduction",
    "title": "Ancova",
    "section": "",
    "text": "In this example, we’re looking at Analysis of Covariance. ANCOVA is typically used to analyse treatment differences, to see examples of prediction models go to the simple linear regression page."
  },
  {
    "objectID": "R/ancova.html#data-summary",
    "href": "R/ancova.html#data-summary",
    "title": "Ancova",
    "section": "Data Summary",
    "text": "Data Summary\n\ndf_sas %&gt;% glimpse()\n\nRows: 30\nColumns: 3\n$ drug &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, D, D, D, D, D, D, D, D, D, D, F, F,…\n$ pre  &lt;dbl&gt; 11, 8, 5, 14, 19, 6, 10, 6, 11, 3, 6, 6, 7, 8, 18, 8, 19, 8, 5, 1…\n$ post &lt;dbl&gt; 6, 0, 2, 8, 11, 4, 13, 1, 8, 0, 0, 2, 3, 1, 18, 4, 14, 9, 1, 9, 1…\n\ndf_sas %&gt;% summary()\n\n drug        pre             post      \n A:10   Min.   : 3.00   Min.   : 0.00  \n D:10   1st Qu.: 7.00   1st Qu.: 2.00  \n F:10   Median :10.50   Median : 7.00  \n        Mean   :10.73   Mean   : 7.90  \n        3rd Qu.:13.75   3rd Qu.:12.75  \n        Max.   :21.00   Max.   :23.00"
  },
  {
    "objectID": "R/ancova.html#the-model",
    "href": "R/ancova.html#the-model",
    "title": "Ancova",
    "section": "The Model",
    "text": "The Model\n\nmodel_ancova &lt;- lm(post ~ drug + pre, data = df_sas)\nmodel_glance &lt;- model_ancova %&gt;% glance()\nmodel_tidy   &lt;- model_ancova %&gt;% tidy()\nmodel_glance %&gt;% gt()\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.6762609\n0.6389064\n4.005778\n18.10386\n1.501369e-06\n3\n-82.05377\n174.1075\n181.1135\n417.2026\n26\n30\n\n\n\n\n\n\n\nmodel_tidy   %&gt;% gt()\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-3.8808094\n1.9862017\n-1.9538849\n6.155192e-02\n\n\ndrugD\n0.1089713\n1.7951351\n0.0607037\n9.520594e-01\n\n\ndrugF\n3.4461383\n1.8867806\n1.8264647\n7.928458e-02\n\n\npre\n0.9871838\n0.1644976\n6.0012061\n2.454330e-06\n\n\n\n\n\n\n\n\n\nmodel_table &lt;- \n  model_ancova %&gt;% \n  anova() %&gt;% \n  tidy() %&gt;% \n  add_row(term = \"Total\", df = sum(.$df), sumsq = sum(.$sumsq))\nmodel_table %&gt;% gt()\n\n\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\ndrug\n2\n293.6000\n146.80000\n9.148553\n9.812371e-04\n\n\npre\n1\n577.8974\n577.89740\n36.014475\n2.454330e-06\n\n\nResiduals\n26\n417.2026\n16.04625\nNA\nNA\n\n\nTotal\n29\n1288.7000\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\nType 1\n\ndf_sas %&gt;%\n  anova_test(post ~ drug + pre, type = 1, detailed = TRUE) %&gt;% \n  get_anova_table() %&gt;%\n  gt()\n\n\n\n\n\n\n\n\nEffect\nDFn\nDFd\nSSn\nSSd\nF\np\np&lt;.05\nges\n\n\n\n\ndrug\n2\n26\n293.600\n417.203\n9.149\n9.81e-04\n*\n0.413\n\n\npre\n1\n26\n577.897\n417.203\n36.014\n2.45e-06\n*\n0.581\n\n\n\n\n\n\n\n\n\n\nType 2\n\ndf_sas %&gt;% \n  anova_test(post ~ drug + pre, type = 2, detailed = TRUE) %&gt;% \n  get_anova_table() %&gt;% \n  gt()\n\n\n\n\n\n\n\n\nEffect\nSSn\nSSd\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\n\ndrug\n68.554\n417.203\n2\n26\n2.136\n1.38e-01\n\n0.141\n\n\npre\n577.897\n417.203\n1\n26\n36.014\n2.45e-06\n*\n0.581\n\n\n\n\n\n\n\n\n\n\nType 3\n\ndf_sas %&gt;%\n  anova_test(post ~ drug + pre, type = 3, detailed = TRUE) %&gt;% \n  get_anova_table() %&gt;% \n  gt()\n\n\n\n\n\n\n\n\nEffect\nSSn\nSSd\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\n\n(Intercept)\n31.929\n417.203\n1\n26\n1.990\n1.70e-01\n\n0.071\n\n\ndrug\n68.554\n417.203\n2\n26\n2.136\n1.38e-01\n\n0.141\n\n\npre\n577.897\n417.203\n1\n26\n36.014\n2.45e-06\n*\n0.581\n\n\n\n\n\n\n\n\n\n\nLS Means\n\nmodel_ancova %&gt;% emmeans::lsmeans(\"drug\") %&gt;% emmeans::pwpm(pvals = TRUE, means = TRUE) \n\n        A       D       F\nA [ 6.71]  0.9980  0.1809\nD  -0.109 [ 6.82]  0.1893\nF  -3.446  -3.337 [10.16]\n\nRow and column labels: drug\nUpper triangle: P values   adjust = \"tukey\"\nDiagonal: [Estimates] (lsmean) \nLower triangle: Comparisons (estimate)   earlier vs. later\n\nmodel_ancova %&gt;% emmeans::lsmeans(\"drug\") %&gt;% plot(comparisons = TRUE)"
  },
  {
    "objectID": "R/ancova.html#saslm-package",
    "href": "R/ancova.html#saslm-package",
    "title": "Ancova",
    "section": "sasLM Package",
    "text": "sasLM Package\nThe following code performs an ANCOVA analysis using the sasLM package. This package was written specifically to replicate SAS statistics. The console output is also organized in a manner that is similar to SAS.\n\nlibrary(sasLM)\n\nsasLM::GLM(post ~ drug + pre, df_sas, BETA = TRUE, EMEAN = TRUE)\n\n$ANOVA\nResponse : post\n                Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nMODEL            3  871.5 290.499  18.104 1.501e-06 ***\nRESIDUALS       26  417.2  16.046                      \nCORRECTED TOTAL 29 1288.7                              \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Fitness\n Root MSE post Mean Coef Var  R-square  Adj R-sq\n 4.005778       7.9 50.70604 0.6762609 0.6389064\n\n$`Type I`\n     Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndrug  2  293.6   146.8  9.1486 0.0009812 ***\npre   1  577.9   577.9 36.0145 2.454e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$`Type II`\n     Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndrug  2  68.55   34.28  2.1361    0.1384    \npre   1 577.90  577.90 36.0145 2.454e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$`Type III`\n     Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndrug  2  68.55   34.28  2.1361    0.1384    \npre   1 577.90  577.90 36.0145 2.454e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Parameter\n            Estimate Estimable Std. Error Df t value  Pr(&gt;|t|)    \n(Intercept)  -0.4347         0     2.4714 26 -0.1759   0.86175    \ndrugA        -3.4461         0     1.8868 26 -1.8265   0.07928 .  \ndrugD        -3.3372         0     1.8539 26 -1.8001   0.08346 .  \ndrugF         0.0000         0     0.0000 26                      \npre           0.9872         1     0.1645 26  6.0012 2.454e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$`Expected Mean`\n               LSmean  LowerCL   UpperCL        SE Df\n(Intercept)  7.900000 6.396685  9.403315 0.7313516 26\ndrugA        6.714963 4.066426  9.363501 1.2884943 26\ndrugD        6.823935 4.208337  9.439532 1.2724690 26\ndrugF       10.161102 7.456182 12.866021 1.3159234 26\npre          7.900000 6.396685  9.403315 0.7313516 26\n\n\nNote that the LSMEANS statistics are produced using the EMEAN = TRUE option. The BETA = TRUE option is equivalent to the SOLUTION option in SAS. See the sasLM documentation for additional information."
  },
  {
    "objectID": "R/mi_mar_predictive_mean_match.html",
    "href": "R/mi_mar_predictive_mean_match.html",
    "title": "Multiple Imputation: Predictive Mean Matching",
    "section": "",
    "text": "Predictive mean matching is a technique for missing value imputation. It calculates the predicted value of the missing variable based on a regression model from complete data, then selects one value (from the observed) that produces the closest prediction. PMM is robust to transformation, less vulnerable to model misspecification. More theoretical details for PMM can be found here.\nAssumption for PMM: distribution of missing is the same aas obsereved data of the candidates that produce the closest values to the predicted value by the missing entry."
  },
  {
    "objectID": "R/mi_mar_predictive_mean_match.html#overview",
    "href": "R/mi_mar_predictive_mean_match.html#overview",
    "title": "Multiple Imputation: Predictive Mean Matching",
    "section": "",
    "text": "Predictive mean matching is a technique for missing value imputation. It calculates the predicted value of the missing variable based on a regression model from complete data, then selects one value (from the observed) that produces the closest prediction. PMM is robust to transformation, less vulnerable to model misspecification. More theoretical details for PMM can be found here.\nAssumption for PMM: distribution of missing is the same aas obsereved data of the candidates that produce the closest values to the predicted value by the missing entry."
  },
  {
    "objectID": "R/mi_mar_predictive_mean_match.html#available-r-package",
    "href": "R/mi_mar_predictive_mean_match.html#available-r-package",
    "title": "Multiple Imputation: Predictive Mean Matching",
    "section": "Available R package",
    "text": "Available R package\nmice is a powerful R package developed by Stef van Buuren, Karin Groothuis-Oudshoorn and other contributors.\nImplementation of PMM in mice:\n\nPredictive mean matching, mice.impute.pmm\nWeighted predictive mean matching, mice.impute.midastouch\nMultivariate predictive mean matching, mice.impute.mpmm"
  },
  {
    "objectID": "R/mi_mar_predictive_mean_match.html#example",
    "href": "R/mi_mar_predictive_mean_match.html#example",
    "title": "Multiple Imputation: Predictive Mean Matching",
    "section": "Example",
    "text": "Example\nWe use the small dataset nhanes included in mice package. It has 25 rows, and three out of four variables have missings.\nThe original NHANES data is a large national level survey, some are publicly available via R package nhanes.\n\nlibrary(mice)\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n# load example dataset from mice\nhead(nhanes)\n\n  age  bmi hyp chl\n1   1   NA  NA  NA\n2   2 22.7   1 187\n3   1   NA   1 187\n4   3   NA  NA  NA\n5   1 20.4   1 113\n6   3   NA  NA 184\n\nsummary(nhanes)\n\n      age            bmi             hyp             chl       \n Min.   :1.00   Min.   :20.40   Min.   :1.000   Min.   :113.0  \n 1st Qu.:1.00   1st Qu.:22.65   1st Qu.:1.000   1st Qu.:185.0  \n Median :2.00   Median :26.75   Median :1.000   Median :187.0  \n Mean   :1.76   Mean   :26.56   Mean   :1.235   Mean   :191.4  \n 3rd Qu.:2.00   3rd Qu.:28.93   3rd Qu.:1.000   3rd Qu.:212.0  \n Max.   :3.00   Max.   :35.30   Max.   :2.000   Max.   :284.0  \n                NA's   :9       NA's   :8       NA's   :10     \n\n\n\nImpute with PMM\nTo impute with PMM is straightforward: specify the method, method = pmm.\n\nimp_pmm &lt;- mice(nhanes, method = 'pmm', m=5, maxit=10)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  1   2  bmi  hyp  chl\n  1   3  bmi  hyp  chl\n  1   4  bmi  hyp  chl\n  1   5  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  2   2  bmi  hyp  chl\n  2   3  bmi  hyp  chl\n  2   4  bmi  hyp  chl\n  2   5  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  3   2  bmi  hyp  chl\n  3   3  bmi  hyp  chl\n  3   4  bmi  hyp  chl\n  3   5  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  4   2  bmi  hyp  chl\n  4   3  bmi  hyp  chl\n  4   4  bmi  hyp  chl\n  4   5  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n  5   2  bmi  hyp  chl\n  5   3  bmi  hyp  chl\n  5   4  bmi  hyp  chl\n  5   5  bmi  hyp  chl\n  6   1  bmi  hyp  chl\n  6   2  bmi  hyp  chl\n  6   3  bmi  hyp  chl\n  6   4  bmi  hyp  chl\n  6   5  bmi  hyp  chl\n  7   1  bmi  hyp  chl\n  7   2  bmi  hyp  chl\n  7   3  bmi  hyp  chl\n  7   4  bmi  hyp  chl\n  7   5  bmi  hyp  chl\n  8   1  bmi  hyp  chl\n  8   2  bmi  hyp  chl\n  8   3  bmi  hyp  chl\n  8   4  bmi  hyp  chl\n  8   5  bmi  hyp  chl\n  9   1  bmi  hyp  chl\n  9   2  bmi  hyp  chl\n  9   3  bmi  hyp  chl\n  9   4  bmi  hyp  chl\n  9   5  bmi  hyp  chl\n  10   1  bmi  hyp  chl\n  10   2  bmi  hyp  chl\n  10   3  bmi  hyp  chl\n  10   4  bmi  hyp  chl\n  10   5  bmi  hyp  chl\n\nimp_pmm\n\nClass: mids\nNumber of multiple imputations:  5 \nImputation methods:\n  age   bmi   hyp   chl \n   \"\" \"pmm\" \"pmm\" \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n# imputations for bmi\nimp_pmm$imp$bmi\n\n      1    2    3    4    5\n1  27.4 28.7 33.2 22.7 22.7\n3  29.6 33.2 22.0 30.1 22.0\n4  22.7 21.7 27.4 24.9 21.7\n6  27.4 22.5 20.4 22.5 24.9\n10 22.0 35.3 35.3 27.2 27.5\n11 27.2 30.1 30.1 28.7 22.7\n12 28.7 29.6 27.4 21.7 22.0\n16 30.1 35.3 33.2 33.2 29.6\n21 30.1 27.2 28.7 27.2 35.3\n\n\nAn alternative to the standard PMM is midastouch.\n\nimp_pmms &lt;- mice(nhanes, method = 'midastouch', m=5, maxit=10)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  1   2  bmi  hyp  chl\n  1   3  bmi  hyp  chl\n  1   4  bmi  hyp  chl\n  1   5  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  2   2  bmi  hyp  chl\n  2   3  bmi  hyp  chl\n  2   4  bmi  hyp  chl\n  2   5  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  3   2  bmi  hyp  chl\n  3   3  bmi  hyp  chl\n  3   4  bmi  hyp  chl\n  3   5  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  4   2  bmi  hyp  chl\n  4   3  bmi  hyp  chl\n  4   4  bmi  hyp  chl\n  4   5  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n  5   2  bmi  hyp  chl\n  5   3  bmi  hyp  chl\n  5   4  bmi  hyp  chl\n  5   5  bmi  hyp  chl\n  6   1  bmi  hyp  chl\n  6   2  bmi  hyp  chl\n  6   3  bmi  hyp  chl\n  6   4  bmi  hyp  chl\n  6   5  bmi  hyp  chl\n  7   1  bmi  hyp  chl\n  7   2  bmi  hyp  chl\n  7   3  bmi  hyp  chl\n  7   4  bmi  hyp  chl\n  7   5  bmi  hyp  chl\n  8   1  bmi  hyp  chl\n  8   2  bmi  hyp  chl\n  8   3  bmi  hyp  chl\n  8   4  bmi  hyp  chl\n  8   5  bmi  hyp  chl\n  9   1  bmi  hyp  chl\n  9   2  bmi  hyp  chl\n  9   3  bmi  hyp  chl\n  9   4  bmi  hyp  chl\n  9   5  bmi  hyp  chl\n  10   1  bmi  hyp  chl\n  10   2  bmi  hyp  chl\n  10   3  bmi  hyp  chl\n  10   4  bmi  hyp  chl\n  10   5  bmi  hyp  chl\n\nimp_pmm\n\nClass: mids\nNumber of multiple imputations:  5 \nImputation methods:\n  age   bmi   hyp   chl \n   \"\" \"pmm\" \"pmm\" \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\nimp_pmms$imp$bmi\n\n      1    2    3    4    5\n1  33.2 29.6 29.6 27.2 20.4\n3  29.6 30.1 29.6 29.6 30.1\n4  25.5 24.9 25.5 21.7 21.7\n6  28.7 24.9 25.5 25.5 24.9\n10 27.5 27.4 29.6 21.7 27.5\n11 29.6 29.6 35.3 22.0 20.4\n12 28.7 27.4 28.7 21.7 27.5\n16 25.5 29.6 22.5 20.4 35.3\n21 29.6 29.6 22.5 29.6 20.4"
  },
  {
    "objectID": "R/kruskal_wallis.html",
    "href": "R/kruskal_wallis.html",
    "title": "Kruskal Wallis R",
    "section": "",
    "text": "The Kruskal-Wallis test is a non-parametric equivalent to the one-way ANOVA. For this example, the data used is a subset of datasets::iris, testing for difference in sepal width between species of flower.\n\n\n      Species Sepal_Width\n1      setosa         3.4\n2      setosa         3.0\n3      setosa         3.4\n4      setosa         3.2\n5      setosa         3.5\n6      setosa         3.1\n7  versicolor         2.7\n8  versicolor         2.9\n9  versicolor         2.7\n10 versicolor         2.6\n11 versicolor         2.5\n12 versicolor         2.5\n13  virginica         3.0\n14  virginica         3.0\n15  virginica         3.1\n16  virginica         3.8\n17  virginica         2.7\n18  virginica         3.3"
  },
  {
    "objectID": "R/kruskal_wallis.html#introduction",
    "href": "R/kruskal_wallis.html#introduction",
    "title": "Kruskal Wallis R",
    "section": "",
    "text": "The Kruskal-Wallis test is a non-parametric equivalent to the one-way ANOVA. For this example, the data used is a subset of datasets::iris, testing for difference in sepal width between species of flower.\n\n\n      Species Sepal_Width\n1      setosa         3.4\n2      setosa         3.0\n3      setosa         3.4\n4      setosa         3.2\n5      setosa         3.5\n6      setosa         3.1\n7  versicolor         2.7\n8  versicolor         2.9\n9  versicolor         2.7\n10 versicolor         2.6\n11 versicolor         2.5\n12 versicolor         2.5\n13  virginica         3.0\n14  virginica         3.0\n15  virginica         3.1\n16  virginica         3.8\n17  virginica         2.7\n18  virginica         3.3"
  },
  {
    "objectID": "R/kruskal_wallis.html#implementing-kruskal-wallis-in-r",
    "href": "R/kruskal_wallis.html#implementing-kruskal-wallis-in-r",
    "title": "Kruskal Wallis R",
    "section": "Implementing Kruskal-Wallis in R",
    "text": "Implementing Kruskal-Wallis in R\nThe Kruskal-Wallis test can be implemented in R using stats::kruskal.test. Below, the test is defined using R’s formula interface (dependent ~ independent variable) and specifying the data set. The null hypothesis is that the samples are from identical populations.\n\nkruskal.test(Sepal_Width~Species, data=iris_sub)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Sepal_Width by Species\nKruskal-Wallis chi-squared = 10.922, df = 2, p-value = 0.004249"
  },
  {
    "objectID": "R/kruskal_wallis.html#results",
    "href": "R/kruskal_wallis.html#results",
    "title": "Kruskal Wallis R",
    "section": "Results",
    "text": "Results\nAs seen above, R outputs the Kruskal-Wallis rank sum statistic (10.922), the degrees of freedom (2), and the p-value of the test (0.004249). Therefore, the difference in population medians is statistically significant at the 5% level."
  },
  {
    "objectID": "R/correlation.html",
    "href": "R/correlation.html",
    "title": "Correlation Analysis Using R",
    "section": "",
    "text": "The most commonly used correlation analysis methods in clinical trials include:\n\nPearson correlation coefficient: product moment coefficient between two continuous variables, measuring linear associations.\n\n\\[\nr = \\frac{\\sum_{i=1}^n (x_i - m_x)(y_i - m_y)}{\\sqrt{\\sum_{i=1}^n (x_i - m_x)^2\\sum_{i=1}^n (y_i - m_y)^2}},\\]\nwhere \\(x\\) and \\(y\\) are observations from two continuous variables of length \\(n\\) and \\(m_x\\) and \\(m_y\\) are their respective means.\nSpearman correlation coefficient: rank correlation defined through the scaled sum of the squared values of the difference between ranks of two continuous variables.\n\\[\n\\rho = \\frac{\\sum_{i=1}^n (x'_i - m_{x'})(y'_i - m_{y'})}{\\sqrt{\\sum_{i=1}^n (x'_i - m_{x'})^2\\sum_{i=1}^n(y'_i - m_{y'})^2}},\n\\]\nwhere \\(x'\\) and \\(y'\\) are the ranks of \\(x\\) and \\(y\\) and \\(m_{x'}\\) and \\(m_{y'}\\) are the mean ranks of \\(x\\) and \\(y\\), respectively.\nKendall’s rank correlation: rank correlation based on the number of inversions in one ranking as compared with another.\n\\[\n\\tau = \\frac{n_c - n_d}{\\frac{1}{2}\\,n\\,(n-1)},\n\\]\nwhere \\(n_c\\) is the total number of concordant pairs, \\(n_d\\) is the total number of disconcordant pairs and \\(n\\) the total size of observations in \\(x\\) and \\(y\\).\n\nOther association measures are available for count data/contingency tables comparing observed frequencies with those expected under the assumption of independence\n\nFisher exact test\nChi-Square statistic\n\n\nExample: Lung Cancer Data\nData source: Loprinzi CL. Laurie JA. Wieand HS. Krook JE. Novotny PJ. Kugler JW. Bartel J. Law M. Bateman M. Klatt NE. et al. Prospective evaluation of prognostic variables from patient-completed questionnaires. North Central Cancer Treatment Group. Journal of Clinical Oncology. 12(3):601-7, 1994.\nSurvival in patients with advanced lung cancer from the North Central Cancer Treatment Group. Performance scores rate how well the patient can perform usual daily activities.\n\nlibrary(survival) \n\nglimpse(lung)\n\nRows: 228\nColumns: 10\n$ inst      &lt;dbl&gt; 3, 3, 3, 5, 1, 12, 7, 11, 1, 7, 6, 16, 11, 21, 12, 1, 22, 16…\n$ time      &lt;dbl&gt; 306, 455, 1010, 210, 883, 1022, 310, 361, 218, 166, 170, 654…\n$ status    &lt;dbl&gt; 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ age       &lt;dbl&gt; 74, 68, 56, 57, 60, 74, 68, 71, 53, 61, 57, 68, 68, 60, 57, …\n$ sex       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, …\n$ ph.ecog   &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 2, 2, 1, 2, 1, 2, 1, NA, 1, 1, 1, 2, 2, 1,…\n$ ph.karno  &lt;dbl&gt; 90, 90, 90, 90, 100, 50, 70, 60, 70, 70, 80, 70, 90, 60, 80,…\n$ pat.karno &lt;dbl&gt; 100, 90, 90, 60, 90, 80, 60, 80, 80, 70, 80, 70, 90, 70, 70,…\n$ meal.cal  &lt;dbl&gt; 1175, 1225, NA, 1150, NA, 513, 384, 538, 825, 271, 1025, NA,…\n$ wt.loss   &lt;dbl&gt; NA, 15, 15, 11, 0, 0, 10, 1, 16, 34, 27, 23, 5, 32, 60, 15, …\n\n\n\n\nOverview\ncor() computes the correlation coefficient between continuous variables x and y, where method chooses which correlation coefficient is to be computed (default: \"pearson\", \"kendall\", or \"spearman\").\ncor.test() calulates the test for association between paired samples, using one of Pearson’s product moment correlation coefficient, Kendall’s \\(\\tau\\) or Spearman’s \\(\\rho\\). Besides the correlation coefficient itself, it provides additional information.\nMissing values are assumed to be missing completely at random (MCAR). Different strategies are available, see ?cor for details.\n\n\nPearson Correlation\n\ncor.test(x = lung$age, y = lung$meal.cal, method = \"pearson\") \n\n\n    Pearson's product-moment correlation\n\ndata:  lung$age and lung$meal.cal\nt = -3.1824, df = 179, p-value = 0.001722\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.3649503 -0.0885415\nsample estimates:\n       cor \n-0.2314107 \n\n\n\n\nSpearman Correlation\n\ncor.test(x = lung$age, y = lung$meal.cal, method = \"spearman\")\n\nWarning in cor.test.default(x = lung$age, y = lung$meal.cal, method =\n\"spearman\"): Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  lung$age and lung$meal.cal\nS = 1193189, p-value = 0.005095\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.2073639 \n\n\nNote: Exact p-values require unanimous ranks.\n\n\nKendall’s rank correlation\n\ncor.test(x = lung$age, y = lung$meal.cal, method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  lung$age and lung$meal.cal\nz = -2.7919, p-value = 0.00524\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n       tau \n-0.1443877 \n\n\n\n\nInterpretation of correlation coefficients\nCorrelation coefficient is comprised between -1 and 1:\n\n\\(-1\\) indicates a strong negative correlation\n\\(0\\) means that there is no association between the two variables\n\\(1\\) indicates a strong positive correlation"
  },
  {
    "objectID": "R/summary_skew_kurt.html",
    "href": "R/summary_skew_kurt.html",
    "title": "Skewness/Kurtosis",
    "section": "",
    "text": "Skewness measures the the amount of asymmetry in a distribution, while Kurtosis describes the “tailedness” of the curve. These measures are frequently used to assess the normality of the data. There are several methods to calculate these measures. In R, there are at least four different packages that contain functions for Skewness and Kurtosis. This write-up will examine the the following packages: e1071, moments, procs, and sasLM.\n\n\nThe following data was used in this example.\n\n# Create sample data\ndat &lt;- tibble::tribble(\n  ~team, ~points, ~assists,\n  \"A\", 10, 2,\n  \"A\", 17, 5,\n  \"A\", 17, 6,\n  \"A\", 18, 3,\n  \"A\", 15, 0,\n  \"B\", 10, 2,\n  \"B\", 14, 5,\n  \"B\", 13, 4,\n  \"B\", 29, 0,\n  \"B\", 25, 2,\n  \"C\", 12, 1,\n  \"C\", 30, 1,\n  \"C\", 34, 3,\n  \"C\", 12, 4,\n  \"C\", 11, 7 \n)\n\n\n\n\nBase R and the stats package have no native functions for Skewness and Kurtosis. It is therefore necessary to use a packaged function to calculate these statistics. The packages examined use three different methods of calculating Skewness, and four different methods for calculating Kurtosis. Of the available packages, the functions in the e1071 package provide the most flexibility, and have options for three of the different methodologies.\n\n\nThe e1071 package contains miscellaneous statistical functions from the Probability Theory Group at the Vienna University of Technology. The package includes functions for both Skewness and Kurtosis, and each function has a “type” parameter to specify the method. There are three available methods for Skewness, and three methods for Kurtosis. A portion of the documentation for these functions is included below:\n\n\nThe documentation for the skewness() function describes three types of skewness calculations: Joanes and Gill (1998) discusses three methods for estimating skewness:\n\nType 1: This is the typical definition used in many older textbooks\n\n\\[g_1 = m_1/m_2^{3/2}\\]\n\nType 2: Used in SAS and SPSS\n\\[\nG_1 = g_1\\sqrt{n(n-1)}/(n-2)\n\\]\nType 3: Used in MINITAB and BMDP\n\\[\nb_1 = m_3/s^3 = g_1((n-1)/n)^{3/2}\n\\]\n\nAll three skewness measures are unbiased under normality\nThe three methods are illustrated in the following code:\n\ntype1 &lt;- e1071::skewness(dat$points, type = 1)\nstringr::str_glue(\"Skewness - Type 1: {type1}\")\n\nSkewness - Type 1: 0.905444204379853\n\ntype2 &lt;- e1071::skewness(dat$points, type = 2)\nstringr::str_glue(\"Skewness - Type 2: {type2}\")\n\nSkewness - Type 2: 1.00931792987094\n\ntype3 &lt;- e1071::skewness(dat$points, type = 3)\nstringr::str_glue(\"Skewness - Type 3: {type3}\")\n\nSkewness - Type 3: 0.816426058828937\n\n\nThe default for the e1071 skewness() function is Type 3.\n\n\n\nThe documentation for the kurtosis() function describes three types of kurtosis calculations: Joanes and Gill (1998) discuss three methods for estimating kurtosis:\n\nType 1: This is the typical definition used in many older textbooks\n\n\\[g_2 = m_4/m_2^{2}-3\\]\n\nType 2: Used in SAS and SPSS\n\\[G_2 = ((n+1)g_2+6)*\\frac{(n-1)}{(n-2)(n-3)}\\]\nType 3: Used in MINITAB and BMDP\n\\[b_2 = m_4/s^4-3 = (g_2 + 3)(1-1/n)^2-3\\]\n\nOnly \\(G_2\\) (corresponding to type 2) is unbiased under normality\nThe three methods are illustrated in the following code:\n\n  # Kurtosis - Type 1\ntype1 &lt;- e1071::kurtosis(dat$points, type = 1)\nstringr::str_glue(\"Kurtosis - Type 1: {type1}\")\n\nKurtosis - Type 1: -0.583341077124784\n\n# Kurtosis - Type 2\ntype2 &lt;- e1071::kurtosis(dat$points, type = 2)\nstringr::str_glue(\"Kurtosis - Type 2: {type2}\")\n\nKurtosis - Type 2: -0.299156418435587\n\n# Kurtosis - Type 3\ntype3 &lt;- e1071::kurtosis(dat$points, type = 3)\nstringr::str_glue(\"Kurtosis - Type 3: {type3}\")\n\nKurtosis - Type 3: -0.894821560517589\n\n\nThe default for the e1071 kurtosis() function is Type 3.\n\n\n\n\nThe moments package is a well-known package with a variety of statistical functions. The package contains functions for both Skewness and Kurtosis. But these functions provide no “type” option. The skewness() function in the moments package corresponds to Type 1 above. The kurtosis() function uses a Pearson’s measure of Kurtosis, which corresponds to none of the three types in the e1071 package.\n\n  library(moments)\n\n  # Skewness - Type 1\n  moments::skewness(dat$points)\n\n[1] 0.9054442\n\n  # [1] 0.9054442\n  \n  # Kurtosis - Pearson's measure\n  moments::kurtosis(dat$points)\n\n[1] 2.416659\n\n  # [1] 2.416659\n\nNote that neither of the functions from the moments package match SAS.\n\n\n\nThe procs package proc_means() function was written specifically to match SAS, and produces a Type 2 Skewness and Type 2 Kurtosis. This package also produces a data frame output, instead of a scalar value.\n\n  library(procs)\n\n  # Skewness and Kurtosis - Type 2 \n  proc_means(dat, var = points,\n             stats = v(skew, kurt))\n\n  TYPE FREQ    VAR     SKEW       KURT\n1    0   15 points 1.009318 -0.2991564\n\n\nViewer Output:\n\n\n\n\n\n\n\n\n\n\n\n\nThe sasLM package was also written specifically to match SAS. The Skewness() function produces a Type 2 Skewness, and the Kurtosis() function a Type 2 Kurtosis.\n\n  library(sasLM)\n\n  # Skewness - Type 2\n  Skewness(dat$points)\n\n[1] 1.009318\n\n  # [1] 1.009318\n  \n  # Kurtosis - Type 2\n  Kurtosis(dat$points)\n\n[1] -0.2991564\n\n  # [1] -0.2991564"
  },
  {
    "objectID": "R/summary_skew_kurt.html#data-used",
    "href": "R/summary_skew_kurt.html#data-used",
    "title": "Skewness/Kurtosis",
    "section": "",
    "text": "The following data was used in this example.\n\n# Create sample data\ndat &lt;- tibble::tribble(\n  ~team, ~points, ~assists,\n  \"A\", 10, 2,\n  \"A\", 17, 5,\n  \"A\", 17, 6,\n  \"A\", 18, 3,\n  \"A\", 15, 0,\n  \"B\", 10, 2,\n  \"B\", 14, 5,\n  \"B\", 13, 4,\n  \"B\", 29, 0,\n  \"B\", 25, 2,\n  \"C\", 12, 1,\n  \"C\", 30, 1,\n  \"C\", 34, 3,\n  \"C\", 12, 4,\n  \"C\", 11, 7 \n)"
  },
  {
    "objectID": "R/summary_skew_kurt.html#package-examination",
    "href": "R/summary_skew_kurt.html#package-examination",
    "title": "Skewness/Kurtosis",
    "section": "",
    "text": "Base R and the stats package have no native functions for Skewness and Kurtosis. It is therefore necessary to use a packaged function to calculate these statistics. The packages examined use three different methods of calculating Skewness, and four different methods for calculating Kurtosis. Of the available packages, the functions in the e1071 package provide the most flexibility, and have options for three of the different methodologies.\n\n\nThe e1071 package contains miscellaneous statistical functions from the Probability Theory Group at the Vienna University of Technology. The package includes functions for both Skewness and Kurtosis, and each function has a “type” parameter to specify the method. There are three available methods for Skewness, and three methods for Kurtosis. A portion of the documentation for these functions is included below:\n\n\nThe documentation for the skewness() function describes three types of skewness calculations: Joanes and Gill (1998) discusses three methods for estimating skewness:\n\nType 1: This is the typical definition used in many older textbooks\n\n\\[g_1 = m_1/m_2^{3/2}\\]\n\nType 2: Used in SAS and SPSS\n\\[\nG_1 = g_1\\sqrt{n(n-1)}/(n-2)\n\\]\nType 3: Used in MINITAB and BMDP\n\\[\nb_1 = m_3/s^3 = g_1((n-1)/n)^{3/2}\n\\]\n\nAll three skewness measures are unbiased under normality\nThe three methods are illustrated in the following code:\n\ntype1 &lt;- e1071::skewness(dat$points, type = 1)\nstringr::str_glue(\"Skewness - Type 1: {type1}\")\n\nSkewness - Type 1: 0.905444204379853\n\ntype2 &lt;- e1071::skewness(dat$points, type = 2)\nstringr::str_glue(\"Skewness - Type 2: {type2}\")\n\nSkewness - Type 2: 1.00931792987094\n\ntype3 &lt;- e1071::skewness(dat$points, type = 3)\nstringr::str_glue(\"Skewness - Type 3: {type3}\")\n\nSkewness - Type 3: 0.816426058828937\n\n\nThe default for the e1071 skewness() function is Type 3.\n\n\n\nThe documentation for the kurtosis() function describes three types of kurtosis calculations: Joanes and Gill (1998) discuss three methods for estimating kurtosis:\n\nType 1: This is the typical definition used in many older textbooks\n\n\\[g_2 = m_4/m_2^{2}-3\\]\n\nType 2: Used in SAS and SPSS\n\\[G_2 = ((n+1)g_2+6)*\\frac{(n-1)}{(n-2)(n-3)}\\]\nType 3: Used in MINITAB and BMDP\n\\[b_2 = m_4/s^4-3 = (g_2 + 3)(1-1/n)^2-3\\]\n\nOnly \\(G_2\\) (corresponding to type 2) is unbiased under normality\nThe three methods are illustrated in the following code:\n\n  # Kurtosis - Type 1\ntype1 &lt;- e1071::kurtosis(dat$points, type = 1)\nstringr::str_glue(\"Kurtosis - Type 1: {type1}\")\n\nKurtosis - Type 1: -0.583341077124784\n\n# Kurtosis - Type 2\ntype2 &lt;- e1071::kurtosis(dat$points, type = 2)\nstringr::str_glue(\"Kurtosis - Type 2: {type2}\")\n\nKurtosis - Type 2: -0.299156418435587\n\n# Kurtosis - Type 3\ntype3 &lt;- e1071::kurtosis(dat$points, type = 3)\nstringr::str_glue(\"Kurtosis - Type 3: {type3}\")\n\nKurtosis - Type 3: -0.894821560517589\n\n\nThe default for the e1071 kurtosis() function is Type 3.\n\n\n\n\nThe moments package is a well-known package with a variety of statistical functions. The package contains functions for both Skewness and Kurtosis. But these functions provide no “type” option. The skewness() function in the moments package corresponds to Type 1 above. The kurtosis() function uses a Pearson’s measure of Kurtosis, which corresponds to none of the three types in the e1071 package.\n\n  library(moments)\n\n  # Skewness - Type 1\n  moments::skewness(dat$points)\n\n[1] 0.9054442\n\n  # [1] 0.9054442\n  \n  # Kurtosis - Pearson's measure\n  moments::kurtosis(dat$points)\n\n[1] 2.416659\n\n  # [1] 2.416659\n\nNote that neither of the functions from the moments package match SAS.\n\n\n\nThe procs package proc_means() function was written specifically to match SAS, and produces a Type 2 Skewness and Type 2 Kurtosis. This package also produces a data frame output, instead of a scalar value.\n\n  library(procs)\n\n  # Skewness and Kurtosis - Type 2 \n  proc_means(dat, var = points,\n             stats = v(skew, kurt))\n\n  TYPE FREQ    VAR     SKEW       KURT\n1    0   15 points 1.009318 -0.2991564\n\n\nViewer Output:\n\n\n\n\n\n\n\n\n\n\n\n\nThe sasLM package was also written specifically to match SAS. The Skewness() function produces a Type 2 Skewness, and the Kurtosis() function a Type 2 Kurtosis.\n\n  library(sasLM)\n\n  # Skewness - Type 2\n  Skewness(dat$points)\n\n[1] 1.009318\n\n  # [1] 1.009318\n  \n  # Kurtosis - Type 2\n  Kurtosis(dat$points)\n\n[1] -0.2991564\n\n  # [1] -0.2991564"
  },
  {
    "objectID": "R/anova.html",
    "href": "R/anova.html",
    "title": "ANOVA",
    "section": "",
    "text": "Getting Started\nTo demonstrate the various types of sums of squares, we’ll create a data frame called df_disease taken from the SAS documentation.\n\n\nThe Model\nFor this example, we’re testing for a significant difference in stem_length using ANOVA. In R, we’re using lm() to run the ANOVA, and then using broom::glance() and broom::tidy() to view the results in a table format.\n\nlm_model &lt;- lm(y ~ drug + disease + drug*disease, df_disease)\n\nThe glance function gives us a summary of the model diagnostic values.\n\nlm_model %&gt;% \n  glance() %&gt;% \n  pivot_longer(everything())\n\n# A tibble: 12 × 2\n   name               value\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 r.squared        0.456  \n 2 adj.r.squared    0.326  \n 3 sigma           10.5    \n 4 statistic        3.51   \n 5 p.value          0.00130\n 6 df              11      \n 7 logLik        -212.     \n 8 AIC            450.     \n 9 BIC            477.     \n10 deviance      5081.     \n11 df.residual     46      \n12 nobs            58      \n\n\nThe tidy function gives a summary of the model results.\n\nlm_model %&gt;% tidy()\n\n# A tibble: 12 × 5\n   term           estimate std.error statistic      p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)      29.3        4.29    6.84   0.0000000160\n 2 drug2            -1.33       6.36   -0.210  0.835       \n 3 drug3           -13          7.43   -1.75   0.0869      \n 4 drug4           -15.7        6.36   -2.47   0.0172      \n 5 disease2         -1.08       6.78   -0.160  0.874       \n 6 disease3         -8.93       6.36   -1.40   0.167       \n 7 drug2:disease2    6.58       9.78    0.673  0.504       \n 8 drug3:disease2  -10.9       10.2    -1.06   0.295       \n 9 drug4:disease2    0.317      9.30    0.0340 0.973       \n10 drug2:disease3   -0.900      9.00   -0.100  0.921       \n11 drug3:disease3    1.10      10.2     0.107  0.915       \n12 drug4:disease3    9.53       9.20    1.04   0.306       \n\n\n\n\nThe Results\nYou’ll see that R print the individual results for each level of the drug and disease interaction. We can get the combined F table in R using the anova() function on the model object.\n\nlm_model %&gt;% \n  anova() %&gt;% \n  tidy() %&gt;% \n  kable()\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\ndrug\n3\n3133.2385\n1044.4128\n9.455761\n0.0000558\n\n\ndisease\n2\n418.8337\n209.4169\n1.895990\n0.1617201\n\n\ndrug:disease\n6\n707.2663\n117.8777\n1.067225\n0.3958458\n\n\nResiduals\n46\n5080.8167\n110.4525\nNA\nNA\n\n\n\n\n\nWe can add a Total row, by using add_row and calculating the sum of the degrees of freedom and sum of squares.\n\nlm_model %&gt;%\n  anova() %&gt;%\n  tidy() %&gt;%\n  add_row(term = \"Total\", df = sum(.$df), sumsq = sum(.$sumsq)) %&gt;% \n  kable()\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\ndrug\n3\n3133.2385\n1044.4128\n9.455761\n0.0000558\n\n\ndisease\n2\n418.8337\n209.4169\n1.895990\n0.1617201\n\n\ndrug:disease\n6\n707.2663\n117.8777\n1.067225\n0.3958458\n\n\nResiduals\n46\n5080.8167\n110.4525\nNA\nNA\n\n\nTotal\n57\n9340.1552\nNA\nNA\nNA\n\n\n\n\n\n\n\nSums of Squares Tables\nUnfortunately, it is not easy to get the various types of sums of squares calculations in using functions from base R. However, the rstatix package offers a solution to produce these various sums of squares tables. For each type, you supply the original dataset and model to the. anova_test function, then specify the ttype and se detailed = TRUE.\n\nType I\n\ndf_disease %&gt;% \n  rstatix::anova_test(\n    y ~ drug + disease + drug*disease, \n    type = 1, \n    detailed = TRUE) %&gt;% \n  rstatix::get_anova_table() %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect\nDFn\nDFd\nSSn\nSSd\nF\np\np&lt;.05\nges\n\n\n\n\ndrug\n3\n46\n3133.239\n5080.817\n9.456\n5.58e-05\n*\n0.381\n\n\ndisease\n2\n46\n418.834\n5080.817\n1.896\n1.62e-01\n\n0.076\n\n\ndrug:disease\n6\n46\n707.266\n5080.817\n1.067\n3.96e-01\n\n0.122\n\n\n\n\n\n\n\nType II\n\ndf_disease %&gt;% \n  rstatix::anova_test(\n    y ~ drug + disease + drug*disease, \n    type = 2, \n    detailed = TRUE) %&gt;% \n  rstatix::get_anova_table() %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect\nSSn\nSSd\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\n\ndrug\n3063.433\n5080.817\n3\n46\n9.245\n6.75e-05\n*\n0.376\n\n\ndisease\n418.834\n5080.817\n2\n46\n1.896\n1.62e-01\n\n0.076\n\n\ndrug:disease\n707.266\n5080.817\n6\n46\n1.067\n3.96e-01\n\n0.122\n\n\n\n\n\n\n\nType III\n\ndf_disease %&gt;% \n  rstatix::anova_test(\n    y ~ drug + disease + drug*disease, \n    type = 3, \n    detailed = TRUE) %&gt;% \n  rstatix::get_anova_table() %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect\nSSn\nSSd\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\n\n(Intercept)\n20037.613\n5080.817\n1\n46\n181.414\n0.00e+00\n*\n0.798\n\n\ndrug\n2997.472\n5080.817\n3\n46\n9.046\n8.09e-05\n*\n0.371\n\n\ndisease\n415.873\n5080.817\n2\n46\n1.883\n1.64e-01\n\n0.076\n\n\ndrug:disease\n707.266\n5080.817\n6\n46\n1.067\n3.96e-01\n\n0.122\n\n\n\n\n\n\n\nType IV\nIn R there is no equivalent operation to the Type IV sums of squares calculation in SAS."
  },
  {
    "objectID": "R/rounding.html",
    "href": "R/rounding.html",
    "title": "Rounding in R",
    "section": "",
    "text": "The round() function in Base R will round to the nearest whole number and ‘rounding to the even number’ when equidistant, meaning that exactly 12.5 rounds to the integer 12.\nNote that the janitor package in R contains a function round_half_up() that rounds away from zero. in this case it rounds to the nearest whole number and ‘away from zero’ or ‘rounding up’ when equidistant, meaning that exactly 12.5 rounds to the integer 13.\n\n#Example code\nmy_number &lt;-c(2.2,3.99,1.2345,7.876,13.8739)\n\nr_0_dec &lt;- round(my_number, digits=0);\nr_1_dec &lt;- round(my_number, digits=1);\nr_2_dec &lt;- round(my_number, digits=2);\nr_3_dec &lt;- round(my_number, digits=3);\n\nr_0_dec\nr_1_dec\nr_2_dec\nr_3_dec\n\n&gt; r_0_dec\n[1]  2  4  1  8 14\n&gt; r_1_dec\n[1]  2.2  4.0  1.2  7.9 13.9\n&gt; r_2_dec\n[1]  2.20  3.99  1.23  7.88 13.87\n&gt; r_3_dec\n[1]  2.200  3.990  1.234  7.876 13.874\n\nIf using the janitor package in R, and the function round_half_up(), the results would be the same with the exception of rounding 1.2345 to 3 decimal places where a result of 1.235 would be obtained instead of 1.234."
  }
]