[
  {
    "objectID": "R/linear-models.html",
    "href": "R/linear-models.html",
    "title": "linear-models",
    "section": "",
    "text": "Getting Started\nTo demonstrate the various types of sums of squares, we'll create a data frame called df_disease taken from the SAS documentation (reference). For more information/to investigate this data go here\n\n\nThe Model\nFor this example, we’re testing for a significant difference in stem_length using ANOVA. In R, we’re using lm() to run the ANOVA, and then using broom::glance() and broom::tidy() to view the results in a table format.\n\nlm_model <- lm(y ~ drug + disease + drug*disease, df_disease)\n\nThe glance function gives us a summary of the model diagnostic values.\n\nlm_model %>% \n  glance() %>% \n  pivot_longer(everything())\n\n# A tibble: 12 × 2\n   name               value\n   <chr>              <dbl>\n 1 r.squared        0.456  \n 2 adj.r.squared    0.326  \n 3 sigma           10.5    \n 4 statistic        3.51   \n 5 p.value          0.00130\n 6 df              11      \n 7 logLik        -212.     \n 8 AIC            450.     \n 9 BIC            477.     \n10 deviance      5081.     \n11 df.residual     46      \n12 nobs            58      \n\n\nThe tidy function gives a summary of the model results.\n\nlm_model %>% tidy()\n\n# A tibble: 12 × 5\n   term           estimate std.error statistic      p.value\n   <chr>             <dbl>     <dbl>     <dbl>        <dbl>\n 1 (Intercept)      29.3        4.29    6.84   0.0000000160\n 2 drug2            -1.33       6.36   -0.210  0.835       \n 3 drug3           -13.0        7.43   -1.75   0.0869      \n 4 drug4           -15.7        6.36   -2.47   0.0172      \n 5 disease2         -1.08       6.78   -0.160  0.874       \n 6 disease3         -8.93       6.36   -1.40   0.167       \n 7 drug2:disease2    6.58       9.78    0.673  0.504       \n 8 drug3:disease2  -10.9       10.2    -1.06   0.295       \n 9 drug4:disease2    0.317      9.30    0.0340 0.973       \n10 drug2:disease3   -0.900      9.00   -0.100  0.921       \n11 drug3:disease3    1.10      10.2     0.107  0.915       \n12 drug4:disease3    9.53       9.20    1.04   0.306       \n\n\n\n\nThe Results\nYou’ll see that R print the individual results for each level of the drug and disease interaction. We can get the combined F table in R using the anova() function on the model object.\n\nlm_model %>% \n  anova() %>% \n  tidy() %>% \n  kable()\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\ndrug\n3\n3133.2385\n1044.4128\n9.455761\n0.0000558\n\n\ndisease\n2\n418.8337\n209.4169\n1.895990\n0.1617201\n\n\ndrug:disease\n6\n707.2663\n117.8777\n1.067225\n0.3958458\n\n\nResiduals\n46\n5080.8167\n110.4525\nNA\nNA\n\n\n\n\n\nWe can add a Total row, by using add_row and calculating the sum of the degrees of freedom and sum of squares.\n\nlm_model %>%\n  anova() %>%\n  tidy() %>%\n  add_row(term = \"Total\", df = sum(.$df), sumsq = sum(.$sumsq)) %>% \n  kable()\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\ndrug\n3\n3133.2385\n1044.4128\n9.455761\n0.0000558\n\n\ndisease\n2\n418.8337\n209.4169\n1.895990\n0.1617201\n\n\ndrug:disease\n6\n707.2663\n117.8777\n1.067225\n0.3958458\n\n\nResiduals\n46\n5080.8167\n110.4525\nNA\nNA\n\n\nTotal\n57\n9340.1552\nNA\nNA\nNA\n\n\n\n\n\n\n\nSums of Squares Tables\nUnfortunately, it is not easy to get the various types of sums of squares calculations in using functions from base R. However, the rstatix package offers a solution to produce these various sums of squares tables. For each type, you supply the original dataset and model to the. anova_test function, then speccify the ttype and se detailed = TRUE.\n\nType I\n\ndf_disease %>% \n  rstatix::anova_test(\n    y ~ drug + disease + drug*disease, \n    type = 1, \n    detailed = TRUE) %>% \n  rstatix::get_anova_table() %>% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect\nDFn\nDFd\nSSn\nSSd\nF\np\np<.05\nges\n\n\n\n\ndrug\n3\n46\n3133.239\n5080.817\n9.456\n5.58e-05\n*\n0.381\n\n\ndisease\n2\n46\n418.834\n5080.817\n1.896\n1.62e-01\n\n0.076\n\n\ndrug:disease\n6\n46\n707.266\n5080.817\n1.067\n3.96e-01\n\n0.122\n\n\n\n\n\n\n\nType II\n\ndf_disease %>% \n  rstatix::anova_test(\n    y ~ drug + disease + drug*disease, \n    type = 2, \n    detailed = TRUE) %>% \n  rstatix::get_anova_table() %>% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect\nSSn\nSSd\nDFn\nDFd\nF\np\np<.05\nges\n\n\n\n\ndrug\n3063.433\n5080.817\n3\n46\n9.245\n6.75e-05\n*\n0.376\n\n\ndisease\n418.834\n5080.817\n2\n46\n1.896\n1.62e-01\n\n0.076\n\n\ndrug:disease\n707.266\n5080.817\n6\n46\n1.067\n3.96e-01\n\n0.122\n\n\n\n\n\n\n\nType III\n\ndf_disease %>% \n  rstatix::anova_test(\n    y ~ drug + disease + drug*disease, \n    type = 3, \n    detailed = TRUE) %>% \n  rstatix::get_anova_table() %>% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect\nSSn\nSSd\nDFn\nDFd\nF\np\np<.05\nges\n\n\n\n\n(Intercept)\n20037.613\n5080.817\n1\n46\n181.414\n0.00e+00\n*\n0.798\n\n\ndrug\n2997.472\n5080.817\n3\n46\n9.046\n8.09e-05\n*\n0.371\n\n\ndisease\n415.873\n5080.817\n2\n46\n1.883\n1.64e-01\n\n0.076\n\n\ndrug:disease\n707.266\n5080.817\n6\n46\n1.067\n3.96e-01\n\n0.122\n\n\n\n\n\n\n\nType IV\nIn R there is no equivalent operation to the Type IV sums of squares calculation in SAS."
  },
  {
    "objectID": "R/mmrm.html",
    "href": "R/mmrm.html",
    "title": "MMRM in R",
    "section": "",
    "text": "Fitting the MMRM in R\n\nUsing the nlme::gls function\nThe code below implements an MMRM fit in R with the nlme::gls function.\n\ngls(model = CHG ~ TRTP + AVISITN + TRTP:AVISITN + AVISITN + BASE,\n    data = data,\n    correlation = corSymm(form = ~1|SUBJID),\n    weights = varIdent(form = ~1|AVISITN),\n    control = glsControl(opt = \"optim\"),\n    method = \"REML\",\n    na.action = \"na.omit\")\n\nhere we can swap out corSymm for corCompSymm to give the compound symmetry structure or corCAR1 for autoregressive of first order (AR(1)).\n\n\nUsing the lme4::lmer function\nAn alternative way to fit an MMRM with unstructured covariance matrices is to use the lme4::lmer function as described by Daniel Sabanes Bove in his R in Pharma talk from 2020 see here. The relevance of this fit is apparent when we consider the availability of the Kenward-Roger’s degrees of freedom for the MMRM in R, which at the time of writing, were not yet available for the nlme::gls function via the pbkrtest package (see here).\n\nlmer(CHG ~ TRTA * VISIT + VISIT + BASE + (0 + VISIT|SUBJID),\n     data = data,\n     control = lmerControl(check.nobs.vs.nRE = \"ignore\"),\n     na.action = na.omit)\n\n\n\nExtracting effect estimates using emmeans\nIn order to extract relevant marginal means (LSmeans) and contrasts we can use the emmeans package. Below we start by constructing a ref_grid used to make explicit just how the predictions are generated across the levels of TRTP and AVISITN. The emmeans function permits various marginal means to be extracted depending on the formula provided and the following pairs() function call derives relevant contrasts. Note that more control can be obtained by calling the contrast() function.\n\nmod_grid <- ref_grid(model, data = data, mode = \"df.error\")\nmod_emm <- emmeans(mod_grid, ~TRTP * AVISITN, mode = \"df.error\") \npairs(mod_emm)"
  },
  {
    "objectID": "Comp/r-sas_linear-models.html",
    "href": "Comp/r-sas_linear-models.html",
    "title": "R vs SAS Linear Models",
    "section": "",
    "text": "Matching Contrasts: R and SAS\nIt is recommended to use the emmeans package when attempting to match contrasts between R and SAS. In SAS, all contrasts must be manually defined, whereas in R, we have many ways to use pre-existing contrast definitions. The emmeans package makes simplifies this process, and provides syntax that is similar to the syntax of SAS.\nThis is how we would define a contrast in SAS.\n\n# In SAS\nproc glm data=work.mycsv;\n   class drug;\n   model post = drug pre / solution;\n   estimate 'C vs A'  drug -1  1 0;\n   estimate 'E vs CA' drug -1 -1 2;\nrun;\n\nAnd this is how we would define the same contrast in R, using the emmeans package.\n\nlm(formula = post ~ pre + drug, data = df_trial) %>% \n  emmeans(\"drug\") %>% \n  contrast(method = list(\n    \"C vs A\"  = c(-1,  1, 0),\n    \"E vs CA\" = c(-1, -1, 2)\n  ))\n\nNote, however, that there are some cases where the scale of the parameter estimates between SAS and R is off, though the test statistics and p-values are identical. In these cases, we can adjust the SAS code to include a divisor. As far as we can tell, this difference only occurs when using the predefined Base R contrast methods like contr.helmert.\n\nproc glm data=work.mycsv;\n   class drug;\n   model post = drug pre / solution;\n   estimate 'C vs A'  drug -1  1 0 / divisor = 2;\n   estimate 'E vs CA' drug -1 -1 2 / divisor = 6;\nrun;"
  },
  {
    "objectID": "Comp/r-sas_mmrm.html",
    "href": "Comp/r-sas_mmrm.html",
    "title": "R vs SAS MMRM",
    "section": "",
    "text": "Data\nThe data used for this comparison was the lab ADaM dataset adlbh.xpt from the Phuse Pilot Study. Results were generated for each lab parameter and time point in the dataset using three different covariance structures, i.e. unstructured, compound symmetry and autoregressive of first order (AR(1)).\n\n\nComparison between SAS and R\nWith results available for SAS and R model fits, we turn our attention to generating some visual comparisons of the results. Note that here we adopt a Bland-Altman type plot which plots the difference on the y-axis and the average on the x-axis. This offers a way to inspect any bias or relationships with the size of effect and the associated bias.\nFor the extracted LS-means\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nand corresponding SEs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the derived contrasts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nand corresponding 95%CI widths\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of SAS and R Comparison\nUsing SAS PROC MIXED and R functions such as gls, lmer, mod_grid, and mod_emm, results were broadly aligned. Results not being exact can be attributed to many factors such as rounding precision, data handling, and many other internal processing nuances. However, Bland-Altman type plots showed small but randomly distributed differences across a broad range of parameters from the input data. Apart from a small subset of the parameters, there were no trends observed which would have suggested systemic differences between the languages. These analyses were based on a single set of data so more research must be done. However, based on comparing R documentation with SAS documentation, as well as the results displayed above in this paper, it is evident that the R and the SAS methods cover do produce similarly valid results for the options which were tested.\n\n\nFuture work\n\nRun SAS code by also removing assessments at avisitn=0 from the response variable, and using trtp (or trtpn) and avisit (or avisitn)\nInvestigating the differences\nImplement lmer equivalent to MMRM with compound symmetry\nComparisons for other models, i.e. only random, random and repeated, no repeated"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CAMIS",
    "section": "",
    "text": "Introduction\nSeveral discrepancies have been discovered in statistical analysis results between different programming languages, even in fully qualified statistical computing environments. Subtle differences exist between the fundamental approaches implemented by each language, yielding differences in results which are each correct in their own right. The fact that these differences exist causes unease on the behalf of sponsor companies when submitting to a regulatory agency, as it is uncertain if the agency will view these differences as problematic. In its Statistical Software Clarifying Statement, the US Food and Drug Administration (FDA) states that it “FDA does not require use of any specific software for statistical analyses” and that “the computer software used for data management and statistical analysis should be reliable.” Observing differences across languages can reduce the analyst’s confidence in reliability and, by understanding the source of any discrepancies, one can reinstate confidence in reliability.\n\nMotivation\nThe goal of this project is to demystify conflict when doing QC and to help ease the transitions to new languages and techniques with comprehensive explanations.\n\nTable of Contents\n\n\nMethod\nR\nSAS\nComparison\n\n\n\n\nSummary Statistics\n\n\n\n\n\nLinear Models\nR\nSAS\nR vs SAS\n\n\nMixed Models\nR\nSAS\nR vs SAS\n\n\nSurvival Models\n\n\n\n\n\nCochran-Mental-Haenszel Test"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "We are a cross-industry group formed of members from PHUSE, PSI, and Transcelerate."
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About",
    "section": "Background",
    "text": "Background\nAs clinical data analytics evolves within the pharmaceutical industry, a large and noteworthy contingent of people and organizations have explored the use of various computational technologies as an effort to reimagine how to tell the story about the data that is collected during the course of a clinical trial. These technologies, whether available commercially or as open source, offer new potential in the ability of a sponsor company to discover new medicines and demonstrate that they can be safely and effectively administered to patients for a given indication. We see applications of machine learning and artificial intelligence being built into exploratory analyses as well as automation of conventional reporting pipelines, both as expanded offerings of commercial products and through tools developed and available as open source. We are witnessing a desired transformation of how we deliver clinical insights from flat data files with rows/columns and compiled PDF reports into dynamic visualization platforms which facilitate a reviewer to explore the trial database in a three-dimensional way. And, most notably, because the tools that other industries most commonly used for these ‘new’ ways of data engineering, data analytics, and data reporting are often built on programming languages not historically used within the pharmaceutical industry, we are experiencing a dramatic shift away from dependence on a small set of commercially available solutions and toward embracing many languages to build and use the best-fit tools to extract the most knowledge from clinical data.\n\nWIP Note:\nAny value to discussing the changing demographic of analysts in the industry – R vs SAS as grad experience, for example\nWant to trim this back and make it more concise. I like the storytelling piece, but I think we should also focus in on access to new/advanced statistical methods and the rapid advancements of open-source data analysis tools over the past 10 years or so.\n\nThis last piece has brought to light an element of our data analytics that was previously overlooked due to an over dependence on a single solution from one programming language. Within the clinical reporting pipeline (transforming patient level clinical trial data from collection to submission), the industry has historically relied on comparing results to an independently generated second set of results as the primary form of quality control (QC). In the early years, comparisons were made on paper and thoroughly verified by a human that the number in the table matched the number independently derived by a second programmer. As technology progressed, electronic comparisons of the output data presented in a table reduced the risk of human error that the validator missed a discrepancy. The theory is that if two people put the same inputs through two independently developed processes (the code) and achieve the same outcome, then the outcome must be right. It’s not a perfect system and it can produce false positives, but efficiencies were gained and quality improved.\nHowever, up until recently, the QC process has nearly always been implemented with the same programming language being used both for the generation of results (‘on production’) and for independent QC. The shift in the industry to explore other languages has now raised questions such as “What if the numbers don’t match? Which is correct?”\nFor example, if one were to take a use case to compare rounding rules between SAS® and R, it is now becoming well understood that the default rounding rule (implemented in the respective language’s round() function) are different, but only when the number being rounded is equidistant between the two possible results. The round() function in SAS will round the number ‘away from zero’, meaning that 12.5 rounds to the integer 13. The round() function in Base R will round the number ‘to even’, meaning that 12.5 rounds to the integer 12. SAS also has the rounde() function which rounds to even and the janitor package in R contains a function that rounds away from zero. In this use case, SAS produces a correct result from its round() function, based on its documentation, as does R. Both are right based on what they say they do, but they produce different results.\n\nWIP Note: Can we make a table to illustrate this?\nI want to see how the rest of the paper pans out but I think we could move this into use cases for discussion. Referencing rounding I think it’s important to also note that the round to even is based on the IEC 60559 standard\n\nNow, the analyst has a choice to make if both R and SAS are in their toolbox – how do I round this result? To answer this question, the analyst needs to understand the rationale behind round-to-even rule and the round-away-from-zero rule, and even other rounding rules that may exist. To our knowledge, this ‘how do I round’ question has never been asked with respect to clinical trial reporting until the difference between R and SAS default rounding was discovered. The ‘correct’ answer is up to the analyst to determine and justify. It likely depends on such things as understanding the impact on the resulting data story about the safety and efficacy of the investigational product.\nWhy should the analyst care? Why does it matter? One answer is because they want to tell the most accurate story of their data. However, and perhaps more importantly in the highly regulated pharmaceutical industry, because a third-party reviewer will be assessing the integrity of the data. If the reviewer attempts to reproduce the same results and chooses a different language, the analyst needs to be able to explain why results may differ, else the integrity of the entire package may be questioned. By fully understanding the implications of choosing a statistical modeling implementation in Language A vs Language B, the analyst can communicate the rationale of the choice, based on sound statistical reasoning, and instill confidence in the regulatory body reviewing the submitted data.\nIt should be noted that in what follows, it is assumed that statistical packages and routines perform in a manner consistent with their documentation. The question at hand is not whether the procedures are accurate or reliable, but rather, in what ways to similar implementations across languages differ. Hence, we are not concerned with another major area of discussion within the industry – the so-called validation of packages and software."
  },
  {
    "objectID": "about.html#other-readings",
    "href": "about.html#other-readings",
    "title": "About",
    "section": "Other Readings",
    "text": "Other Readings\n\nPerhaps cite the TransCelerate MoA project\nPerhaps cite other working groups or published conference proceedings"
  },
  {
    "objectID": "data-info/sas_disease.html",
    "href": "data-info/sas_disease.html",
    "title": "SAS Disease",
    "section": "",
    "text": "To demonstrate the various types of sums of squares, we’ll create a data frame called `df_disease` taken from the SAS documentation (__reference__). The summary of the data is shown.\nThe general summary of the data is as follows\n\n\n drug   disease       y        \n 1:18   1:24    Min.   :-6.00  \n 2:18   2:24    1st Qu.: 9.00  \n 3:18   3:24    Median :21.00  \n 4:18           Mean   :18.88  \n                3rd Qu.:28.00  \n                Max.   :44.00  \n                NA's   :14     \n\n\n\nData summary\n\n\nName\ndf_disease\n\n\nNumber of rows\n72\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndrug\n0\n1\nFALSE\n4\n1: 18, 2: 18, 3: 18, 4: 18\n\n\ndisease\n0\n1\nFALSE\n3\n1: 24, 2: 24, 3: 24\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ny\n14\n0.81\n18.88\n12.8\n-6\n9\n21\n28\n44\n▅▆▆▇▂"
  },
  {
    "objectID": "SAS/linear-models.html",
    "href": "SAS/linear-models.html",
    "title": "linear-models",
    "section": "",
    "text": "Getting Started\nTo demonstrate the various types of sums of squares, we'll create a data frame called df_disease taken from the SAS documentation (reference). For more information/to investigate this data go here\n\n\nThe Model\nFor this example, we’re testing for a significant difference in stem_length using ANOVA.\n\nproc glm;\n   class drug disease;\n   model y=drug disease drug*disease;\nrun;\n\n\n\n\n\n\nSums of Squares Tables\nSAS has four types of sums of squares calculations. To get these calculations, the sum of squares option needs to be added (/ ss1 ss2 ss3 ss4) to the model statement.\n\nproc glm;\n   class drug disease;\n   model y=drug disease drug*disease / ss1 ss2 ss3 ss4;\nrun;\n\n\nType I\n\n\n\n\n\n\n\n\n\n\n\nType II\n\n\n\n\n\n\n\n\n\n\n\nType III\n\n\n\n\n\n\n\n\n\n\n\nType IV"
  },
  {
    "objectID": "SAS/mmrm.html",
    "href": "SAS/mmrm.html",
    "title": "MMRM in SAS",
    "section": "",
    "text": "Mixed Models\n\nFitting the MMRM in SAS\nIn SAS the following code was used (assessments at avisitn=0 should also be removed from the response variable):\n\nproc mixed data=adlbh;\n  where base ne . and avisitn not in (., 99);\n  class usubjid trtpn(ref=\"0\") avisitn;\n  by paramcd param;\n  model chg=base trtpn avisitn  trtpn*avisitn / solution cl alpha=0.05 ddfm=KR;\n  repeated avisitn/subject=usubjid type=&covar;\n  lsmeans trtpn * avisitn / diff cl slice=avisitn;\n  lsmeans trtpn / diff cl;\nrun;\n\nwhere the macro variable covar could be UN, CS or AR(1). The results were stored in .csv files that were post-processed in R and compared with the results from R."
  },
  {
    "objectID": "templates/R_template.html",
    "href": "templates/R_template.html",
    "title": "R Template",
    "section": "",
    "text": "Optional If there is only one available package this can be deleted. Otherwise please make a short list, paragraph or table. If there is a reason to use one package vs another please include it.\n\n\n\nJust a sentence or two about the data and a link to the data page\n\n\n\n\n\nOptional if there is more than one package"
  }
]