---
title: "A Brief Guide to Statistics and Machine Learning Methods"
---

# The crux of the problem

Whatever book about machine learning you take, it begins with more or less detailed classification of algorithms. The depths of classification varies from simple "supervised vs unsupervised" dichotomy to much more confusing partitioning consists of supervised, un-, semi- and self-supervised methods as well as reinforcement learning. Supervised learning commonly divides to classification and regression, and we still have enough stuff beyond it including learning to rank or specific computer vision tasks like image segmentation (actually, pixel-wise classification) and object detection (required solving classification and regression tasks simultaneously).

At the same time, all kinds of supervised learning are not so different as one would consider. Generalized linear models, random forest and various implementation of gradient boosting (xgboost/lightgbm/catboost/etc.) can be used for both classification and regression.

A more notable problem in studying of machine learning is its opposition to "traditional" statistics. Fundamental theoretical textbooks clearly point the origin of machine learning in approx. 100-years-old statistical algorithms but contain significant amount of math details making it barely suitable for non-STEM specialists. On the other hand, more practically-oriented manuals often start from programming perspective and describe machine learning as completely separated universe, so no p-values and confidence intervals here. And of course we will unlikely find any mentions about cross-validation in all but most recent statistics textbooks.

# Bridging the gap

We propose to use the intention-based models classification presented in  [Tidy Modeling with R](https://www.tmwr.org/) book by Max Kuhn and Julia Silge:

1.  **Descriptive models** - effectively any model when no predictions or statistical inference is made with it even if the model is capable to do both. Common examples are adding LOESS smoothing on ggplot2 scatterplots and plotting data in PCA coordinates or in coordinates of some embedding space like t-SNE. The key point is to make no assumptions and conclusions about how this model works for new, unseen samples.

2.  **Inferential models** - models that can produce p-values, confidence intervals and/or Bayesian estimates when we use it to answer pre-formulated questions via some kind of hypothesis testing. It is worth noting that machine learning approaches like cross-validation are fully applicable here for model selection because of overfitting risk. Even if researcher isn't interested in prediction quality for new samples, estimating model characteristics using out-of-fold samples using k-fold cross-validation is valuable and somewhat undervalued opportunity.

3. **Predictive models** - 

As was already mentioned, all models suffer from overfitting when repetitively modelling procedures with different settings are made on the same data. For inferential models it can lead to higher than expected type I and type II error rates or simply incorrect parameters estimations. Predictive models will report overestimated prediction quality if validation and test sets violate representativity or contain data leakage even if formal model selection procedure with (cross-)validation is done.

probability-like values in range [0, 1] (this values can be turned into "true probabilities" via calibration procedure)
