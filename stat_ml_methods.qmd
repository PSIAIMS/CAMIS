---
title: "A Brief Guide to Statistics and Machine Learning Methods"
---

# The crux of the problem

Whatever book about machine learning you take, it begins with more or less detailed classification of algorithms. The depths of classification varies from simple "supervised vs unsupervised" dichotomy to much more confusing partitioning consists of supervised, un-, semi- and self-supervised methods as well as reinforcement learning. Supervised learning commonly divides to classification and regression, and we still have enough stuff beyond it including learning to rank or specific computer vision tasks like image segmentation (actually, pixel-wise classification) and object detection (required solving classification and regression tasks simultaneously).

At the same time, all kinds of supervised learning are not so different as one would consider. Generalized linear models,  random forest and various implementation of gradient boosting (xgboost/lightgbm/catboost/etc.) can be used for both classification and regression.

A more notable  problem in studying of machine learning is its opposition to "traditional" statistics. Fundamental theoretical textbooks clearly point the origin of machine learning in approx. 100-years-old statistical algorithms but contain significant amount of math details making it barely suitable for non-STEM specialists. On the other hand, more practically-oriented manuals often start from programming perspective and describe machine learning as completely separated universe, so no p-values and confidence intervals here. And of course we are unlikely to find any mentions about cross-validation in all but most recent statistics textbooks.

# Bridging the gap